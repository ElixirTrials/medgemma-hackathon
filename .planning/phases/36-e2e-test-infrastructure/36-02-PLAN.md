---
phase: 36-e2e-test-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["36-01"]
files_modified:
  - tests/e2e/conftest.py
  - tests/e2e/test_infrastructure_smoke.py
autonomous: true

must_haves:
  truths:
    - "Test conftest provides a fixture that uploads a test PDF to the API and returns the protocol_id"
    - "After each E2E test function, all protocols and criteria created during the test are deleted from the database"
    - "A smoke test proves the infrastructure works: uploads a PDF, gets a protocol_id, and cleanup removes it"
  artifacts:
    - path: "tests/e2e/conftest.py"
      provides: "upload_test_pdf fixture and e2e_cleanup fixture"
      min_lines: 120
    - path: "tests/e2e/test_infrastructure_smoke.py"
      provides: "Smoke test validating upload fixture and cleanup"
      min_lines: 30
  key_links:
    - from: "tests/e2e/conftest.py"
      to: "POST /api/protocols/upload"
      via: "httpx POST to create protocol + upload PDF"
      pattern: "/api/protocols/upload"
    - from: "tests/e2e/conftest.py"
      to: "shared.models.Protocol"
      via: "SQLModel delete query for cleanup"
      pattern: "delete.*protocol|Protocol.*delete"
    - from: "tests/e2e/test_infrastructure_smoke.py"
      to: "tests/e2e/conftest.py"
      via: "pytest fixture injection (upload_test_pdf)"
      pattern: "upload_test_pdf"
---

<objective>
Add the test PDF upload fixture and database cleanup fixture to the E2E conftest, then create a smoke test that validates the entire infrastructure works.

Purpose: Provide reusable fixtures for Phase 37's actual E2E test cases, and prove the infrastructure works end-to-end with a minimal smoke test.

Output: Updated `tests/e2e/conftest.py` with upload + cleanup fixtures; `tests/e2e/test_infrastructure_smoke.py` with smoke test.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/36-e2e-test-infrastructure/36-01-SUMMARY.md
@services/api-service/src/api_service/protocols.py
@services/api-service/src/api_service/gcs.py
@services/api-service/src/api_service/main.py
@services/protocol-processor-service/src/protocol_processor/tools/pdf_parser.py
@libs/shared/src/shared/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add upload_test_pdf and e2e_cleanup fixtures to conftest</name>
  <files>tests/e2e/conftest.py</files>
  <action>
Add the following fixtures to the existing `tests/e2e/conftest.py` (created in Plan 01):

**1. `_created_protocol_ids` fixture (function-scoped):**
- Returns a `list[str]` that accumulates protocol IDs created during a test.
- Used by cleanup to know what to delete.

**2. `upload_test_pdf` fixture (function-scoped):**
- Depends on: `e2e_api_client`, `_created_protocol_ids`.
- Returns a callable (factory fixture pattern) so tests can upload multiple PDFs if needed.
- The callable signature: `upload_test_pdf(pdf_path: str | None = None) -> str` (returns protocol_id).
- Default `pdf_path`: `data/protocols/crc_protocols/isrctn/48616-d8fc1476.pdf` (smallest at 90K).

**Upload flow (mirrors what the frontend does):**
  a. Read the PDF file from disk as bytes.
  b. POST to `/api/protocols/upload` with JSON body:
     ```json
     {"filename": "test-e2e.pdf", "content_type": "application/pdf", "file_size_bytes": <size>}
     ```
  c. Response gives `protocol_id`, `upload_url`, `gcs_path`.
  d. PUT the PDF bytes to the `upload_url` (this is the local-upload endpoint in dev mode: `http://localhost:8000/local-upload/{blob_path}`).
  e. POST to `/api/protocols/{protocol_id}/confirm-upload` with JSON body:
     ```json
     {"pdf_bytes_base64": "<base64 encoded PDF>"}
     ```
     (Optional — for quality score. Include it since it's what the real flow does.)
  f. Append `protocol_id` to `_created_protocol_ids`.
  g. Return `protocol_id`.

- Assert that each step returns expected status codes (200/201) and fail the test immediately with clear errors if the API is not responding correctly.

**Important:** The API requires `USE_LOCAL_STORAGE=1` env var for local file storage. The Docker Compose stack should have this set. Document this in the fixture docstring. The E2E conftest should set `os.environ["USE_LOCAL_STORAGE"] = "1"` at the top of the file as a safety net (the API process has its own env, but this helps if tests import api-service code directly).

**3. `e2e_cleanup` fixture (function-scoped, autouse=True for e2e tests):**
- Depends on: `e2e_db_session`, `_created_protocol_ids`.
- Yields (runs test), then in teardown:
  a. For each protocol_id in `_created_protocol_ids`:
     - Delete all `Entity` rows where `criteria_id` is in criteria for this protocol's batches.
     - Delete all `Review` rows where `criteria_id` is in criteria for this protocol's batches.
     - Delete all `AuditLog` rows where `aggregate_id` matches the protocol_id or related criteria IDs.
     - Delete all `Criteria` rows for all `CriteriaBatch` rows for this protocol.
     - Delete all `CriteriaBatch` rows for this protocol.
     - Delete all `OutboxEvent` rows where payload contains this protocol_id.
     - Delete the `Protocol` row itself.
  b. Commit the session.
  c. Log what was cleaned up.

- Use SQLModel `select` + `delete` statements. Import models from `shared.models`.
- The cleanup must handle the case where the pipeline is still running (protocol stuck in "extracting") — just delete whatever exists.
- Use CASCADE-aware deletion order: entities/reviews first, then criteria, then batches, then protocol.

**Make `e2e_cleanup` autouse only for e2e-marked tests.** Use a conftest-level approach: since this conftest is in `tests/e2e/`, all tests in that directory will pick it up. Add `autouse=True` to the fixture definition. The fixture should only clean up if `_created_protocol_ids` has entries (no-op if no protocols were created).

**4. `wait_for_pipeline` helper function (not a fixture, a utility):**
- Signature: `wait_for_pipeline(api_client: httpx.Client, protocol_id: str, timeout: int = 180) -> dict`
- Polls `GET /api/protocols/{protocol_id}` every 5 seconds.
- Returns when protocol status is in a terminal state: `pending_review`, `complete`, `extraction_failed`, `grounding_failed`, `pipeline_failed`, `dead_letter`.
- Raises `TimeoutError` if timeout exceeded.
- Returns the final protocol response dict.
- This will be used by Phase 37 test cases, but define it here as a conftest utility.
  </action>
  <verify>
1. `uv run python -c "from tests.e2e.conftest import wait_for_pipeline; print('OK')"` should not error.
2. `uv run pytest --collect-only tests/e2e/` should show conftest loaded without import errors.
  </verify>
  <done>
- `upload_test_pdf` fixture uploads a real PDF via the API's upload flow (signed URL + local storage) and returns protocol_id.
- `e2e_cleanup` autouse fixture deletes all test-created data from the database after each test.
- `wait_for_pipeline` utility function polls protocol status until terminal state.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create infrastructure smoke test</name>
  <files>tests/e2e/test_infrastructure_smoke.py</files>
  <action>
Create `tests/e2e/test_infrastructure_smoke.py` with a minimal smoke test that validates the E2E infrastructure:

```python
"""Smoke test for E2E test infrastructure.

Validates that:
1. The upload fixture successfully creates a protocol via the API
2. The protocol appears in the database
3. The cleanup fixture removes it after the test
"""
import pytest

@pytest.mark.e2e
class TestInfrastructureSmoke:
    """Verify E2E test infrastructure works."""

    def test_upload_creates_protocol(self, upload_test_pdf, e2e_api_client):
        """Upload a test PDF and verify it appears in the API."""
        protocol_id = upload_test_pdf()

        # Verify protocol exists via API
        resp = e2e_api_client.get(f"/api/protocols/{protocol_id}")
        assert resp.status_code == 200
        data = resp.json()
        assert data["id"] == protocol_id
        assert data["status"] in ("uploaded", "extracting", "processing", "pending_review")

    def test_upload_with_custom_pdf(self, upload_test_pdf, e2e_api_client):
        """Upload a specific PDF and verify it works."""
        protocol_id = upload_test_pdf(
            pdf_path="data/protocols/crc_protocols/isrctn/48616-d8fc1476.pdf"
        )

        resp = e2e_api_client.get(f"/api/protocols/{protocol_id}")
        assert resp.status_code == 200

    def test_protocol_list_includes_upload(self, upload_test_pdf, e2e_api_client):
        """Uploaded protocol appears in the protocol list."""
        protocol_id = upload_test_pdf()

        resp = e2e_api_client.get("/api/protocols")
        assert resp.status_code == 200
        data = resp.json()
        protocol_ids = [p["id"] for p in data["items"]]
        assert protocol_id in protocol_ids
```

**Notes:**
- These tests do NOT wait for pipeline completion — that's Phase 37.
- They validate: (a) upload fixture works, (b) API is reachable, (c) cleanup happens.
- Cleanup is verified implicitly — if cleanup fails, subsequent test runs would accumulate data and eventually cause issues. We can add an explicit cleanup verification test if desired.
- All tests are marked `@pytest.mark.e2e` so they auto-skip when Docker Compose is down.
  </action>
  <verify>
1. With Docker Compose STOPPED: `uv run pytest tests/e2e/test_infrastructure_smoke.py -v` shows all tests SKIPPED with the Docker Compose message and exits 0.
2. With Docker Compose RUNNING: `uv run pytest tests/e2e/test_infrastructure_smoke.py -v` shows all tests PASSED.
3. After the test run with Docker Compose running, verify cleanup: query the database for protocols with title containing "test-e2e" — should find none.
  </verify>
  <done>
- Smoke test validates upload fixture creates a protocol via the real API.
- Smoke test validates protocol appears in API responses.
- All tests skip gracefully when Docker Compose is not running.
- All test data is cleaned up after each test function.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest -m e2e -v` with Docker Compose stopped: all E2E tests skipped, exit code 0
2. `uv run pytest -m e2e -v` with Docker Compose running: all E2E smoke tests pass
3. After running with Docker Compose: no test protocols remain in the database
4. `upload_test_pdf` fixture returns a valid protocol_id that the API recognizes
</verification>

<success_criteria>
- upload_test_pdf fixture uploads real PDF through API upload flow and returns protocol_id
- e2e_cleanup deletes all protocols, batches, criteria, entities created during tests
- Smoke tests pass against running Docker Compose stack
- Smoke tests skip gracefully against stopped stack (exit 0)
- wait_for_pipeline utility exists for Phase 37 to use
</success_criteria>

<output>
After completion, create `.planning/phases/36-e2e-test-infrastructure/36-02-SUMMARY.md`
</output>
