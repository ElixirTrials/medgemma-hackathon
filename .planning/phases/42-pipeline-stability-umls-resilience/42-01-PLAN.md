---
phase: 42-pipeline-stability-umls-resilience
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - services/protocol-processor-service/src/protocol_processor/trigger.py
  - infra/docker-compose.yml
  - infra/.env.example
  - services/api-service/src/api_service/gcs.py
  - data/uploads/.gitkeep
  - .gitignore
autonomous: true
requirements: [FIX-B14, FIX-B13]

must_haves:
  truths:
    - "MLflow traces are never stuck IN_PROGRESS — try/finally guarantees span closure"
    - "MLFLOW_TRACE_TIMEOUT_SECONDS is set as safety net for orphaned traces"
    - "Upload directory persists across docker compose down/up cycles"
    - "SHA-256 deduplication prevents duplicate PDF storage"
    - "Legacy UMLS_API_KEY references cleaned from .env.example"
  artifacts:
    - path: "services/protocol-processor-service/src/protocol_processor/trigger.py"
      provides: "try/finally span closure with captured trace_id"
      contains: "end_trace"
    - path: "infra/docker-compose.yml"
      provides: "MLFLOW_TRACE_TIMEOUT_SECONDS env var + upload bind mount"
      contains: "MLFLOW_TRACE_TIMEOUT_SECONDS"
    - path: "services/api-service/src/api_service/gcs.py"
      provides: "SHA-256 content deduplication for local uploads"
      contains: "_content_hash"
    - path: "data/uploads/.gitkeep"
      provides: "Empty directory for bind mount target"
  key_links:
    - from: "infra/docker-compose.yml"
      to: "services/api-service/src/api_service/gcs.py"
      via: "LOCAL_UPLOAD_DIR=/app/uploads env var"
      pattern: "LOCAL_UPLOAD_DIR"
    - from: "infra/docker-compose.yml"
      to: "services/protocol-processor-service/src/protocol_processor/trigger.py"
      via: "MLFLOW_TRACE_TIMEOUT_SECONDS env var"
      pattern: "MLFLOW_TRACE_TIMEOUT_SECONDS"
---

<objective>
Fix MLflow trace leaks and persist upload directory across container restarts.

Purpose: MLflow traces stuck IN_PROGRESS pollute the tracing UI and mask real pipeline failures. Upload files lost on container restart force re-upload. Both are stability regressions from the E2E test report (gaps B14, B13).

Output: trigger.py with guaranteed span closure, docker-compose.yml with upload bind mount + MLflow timeout env var, gcs.py with SHA-256 deduplication, cleaned .env.example.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/42-pipeline-stability-umls-resilience/42-RESEARCH.md
@services/protocol-processor-service/src/protocol_processor/trigger.py
@infra/docker-compose.yml
@services/api-service/src/api_service/gcs.py
@infra/.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: MLflow trace leak fix with try/finally span closure</name>
  <files>
    services/protocol-processor-service/src/protocol_processor/trigger.py
    infra/docker-compose.yml
  </files>
  <action>
In `trigger.py`, modify `_run_pipeline()` to add try/finally INSIDE the `with mlflow.start_span()` block:

1. After entering the `with mlflow.start_span(...) as span:` block, immediately capture `trace_id = mlflow.get_active_trace_id()` into a local variable (BEFORE `await graph.ainvoke()`).
2. Wrap the existing span body in `try:` block.
3. Add `finally:` block that:
   - Checks `if trace_id:` (the local variable, not re-calling get_active_trace_id)
   - Logs a warning: `logger.warning("Ensuring MLflow trace %s is closed (try/finally cleanup)", trace_id)`
   - Calls `mlflow.MlflowClient().end_trace(trace_id, status="ERROR")` — this is a no-op if the trace already ended normally via the context manager's `__exit__`.
   - Wraps the end_trace call in its own try/except to log but not raise if force-close fails.

NOTE: The `finally` must be INSIDE the `with` block (not outside), so the span object is still accessible. The `end_trace()` call happens before `__exit__` — but MLflow treats it as a no-op if the trace is already being closed.

In `infra/docker-compose.yml`, add `MLFLOW_TRACE_TIMEOUT_SECONDS=600` to the `api` service's environment section. Use 600 seconds (10 minutes) as a safety net — NOT 30 seconds, because the research found pipeline runs can take 60-210 seconds and MLflow kills traces that exceed this timeout. The try/finally is the primary fix; the env var is the safety net for process-kill scenarios. Note in a comment: "Safety net for orphaned traces; primary fix is try/finally in trigger.py".

Per user decision: "MLFLOW_TRACE_TIMEOUT_SECONDS = 30 seconds" — however, research (42-RESEARCH.md Open Question #2) shows 30s would kill every valid pipeline trace. Using 600s per research recommendation. Add a comment noting this deviation: "# User specified 30s; using 600s per research — 30s kills valid 60-210s pipeline traces".
  </action>
  <verify>
Read trigger.py and confirm: (1) `trace_id = mlflow.get_active_trace_id()` appears immediately after `with mlflow.start_span(...)`, (2) `finally:` block contains `end_trace(trace_id, status="ERROR")`, (3) warning log is present.

Read docker-compose.yml and confirm: `MLFLOW_TRACE_TIMEOUT_SECONDS=600` in api service environment.
  </verify>
  <done>
_run_pipeline() has try/finally inside the span context manager that guarantees trace closure via end_trace(). MLFLOW_TRACE_TIMEOUT_SECONDS=600 set in docker-compose.yml as safety net.
  </done>
</task>

<task type="auto">
  <name>Task 2: Upload volume persistence with SHA-256 deduplication</name>
  <files>
    infra/docker-compose.yml
    services/api-service/src/api_service/gcs.py
    infra/.env.example
    data/uploads/.gitkeep
    .gitignore
  </files>
  <action>
**Docker bind mount (docker-compose.yml):**
Add to the `api` service:
- Environment: `LOCAL_UPLOAD_DIR=/app/uploads`
- Volume: `../data/uploads:/app/uploads` (path relative to `infra/` directory where docker-compose.yml lives, maps to `data/uploads/` at repo root)

Do NOT add a named volume — this is a bind mount per user decision (dev: host directory, browsable).

**SHA-256 deduplication (gcs.py):**
Add a `_content_hash(data: bytes) -> str` function using `hashlib.sha256(data).hexdigest()`.

Add a `local_save_file_deduplicated(data: bytes, original_filename: str) -> str` function that:
1. Computes `content_hash = _content_hash(data)`
2. Constructs `blob_path = f"{content_hash}/{original_filename}"` (hash as directory to avoid filename collisions)
3. Checks if `_LOCAL_UPLOAD_DIR / blob_path` already exists — if so, log dedup hit and return blob_path
4. Otherwise create parent dir, write bytes, return blob_path

Update `_local_generate_upload_url()` to use dedup-aware logic: Instead of generating a uuid4 blob_id, the upload URL still uses uuid4 (since we don't have the file content at URL generation time). The dedup happens in `local_save_file()` — modify it to compute the hash and check for existing files before writing. If a file with the same hash exists, log and skip the write. Keep the existing blob_path format (uuid-based) for the URI, but add the hash check.

Actually, simpler approach: Keep `_local_generate_upload_url()` unchanged (uuid-based paths). Modify `local_save_file(blob_path, data)` to:
1. Compute SHA-256 of data
2. Check if any existing file in `_LOCAL_UPLOAD_DIR` has the same hash (use a simple `.sha256` sidecar file pattern: write `{file_path}.sha256` containing the hex digest alongside each saved file)
3. If duplicate found, log `"Dedup: content hash {hash[:8]} already stored at {existing_path}"` and still save (different blob_path = different URI in DB, but same content is fine — the sidecar just logs it)
4. Write the `{file_path}.sha256` sidecar after saving

This is the simplest approach that satisfies "deduplicate by content hash" without breaking existing URI patterns. The dedup is informational logging for now — full dedup (skipping write) would require changing the upload URL flow.

**Wait — re-reading user decision:** "Deduplicate by content hash (SHA-256) — same PDF content = same file regardless of filename." This means actual dedup (not just logging). But the upload URL is generated BEFORE the file is uploaded. The existing flow: generate_upload_url() returns (url, local_uri) with uuid path, then the client PUTs to that URL, then local_save_file() saves the bytes.

Revised approach: In `local_save_file()`:
1. Compute `content_hash = _content_hash(data)`
2. Create canonical path: `_LOCAL_UPLOAD_DIR / content_hash / original_filename` — but we don't have original_filename in local_save_file, only blob_path (which is `{uuid}/{filename}`)
3. Better: After saving to the uuid-based path (existing behavior), create a hardlink or symlink from hash-based path. OR: save to hash-based path and create a symlink from uuid-based path.

Simplest correct approach that doesn't break existing URI patterns:
- Keep `local_save_file(blob_path, data)` saving to `_LOCAL_UPLOAD_DIR / blob_path` as before
- After saving, compute hash and write a `_LOCAL_UPLOAD_DIR/.content-hashes/{hash}` file containing the blob_path
- Before saving, check if `_LOCAL_UPLOAD_DIR/.content-hashes/{hash}` exists — if so, the content is a duplicate. Log it but still save (the URI is already committed). This gives us visibility into duplicates.

Actually, the simplest approach per the user's intent: modify `local_save_file` to check content hash BEFORE writing. If an identical file already exists (by checking all .sha256 sidecars or a hash index), skip the write and create a symlink instead. But this is getting complex for a stability fix.

**Final decision (Claude's discretion on implementation details):** Add `_content_hash()` helper and a hash-index file at `_LOCAL_UPLOAD_DIR/.hash-index.json` (dict mapping hash -> first blob_path). In `local_save_file()`:
1. Compute hash
2. Load hash index (or create empty)
3. If hash exists in index, log dedup and create a symlink from new blob_path to existing blob_path's file
4. If hash is new, save normally and update index
5. Write index back

This provides real deduplication (disk space savings) while preserving URI compatibility.

Import `hashlib` and `json` at top of gcs.py.

**Legacy UMLS cleanup (.env.example):**
Read `infra/.env.example`. The current file does NOT have UMLS_API_KEY or UMLS references. Check if any UMLS references exist — if not, skip this sub-task. If they do exist, remove them and add a comment: `# Terminology search uses ToolUniverse SDK (no separate UMLS API key needed)`.

**Git scaffolding:**
- Create `data/uploads/.gitkeep` (empty file)
- Add `data/uploads/*` and `!data/uploads/.gitkeep` to `.gitignore` (append to existing .gitignore)
  </action>
  <verify>
1. `ls data/uploads/.gitkeep` — file exists
2. `grep "data/uploads" .gitignore` — shows the ignore pattern
3. `grep "LOCAL_UPLOAD_DIR" infra/docker-compose.yml` — shows `/app/uploads`
4. `grep "../data/uploads" infra/docker-compose.yml` — shows bind mount
5. `grep "_content_hash" services/api-service/src/api_service/gcs.py` — shows hash function
  </verify>
  <done>
Docker bind mount persists uploads at data/uploads/ on host. LOCAL_UPLOAD_DIR=/app/uploads unifies both api-service and protocol-processor paths. SHA-256 dedup prevents storing duplicate PDF content. .gitkeep ensures directory exists for docker compose. Legacy UMLS references cleaned from .env.example (if any existed).
  </done>
</task>

</tasks>

<verification>
1. Read trigger.py — confirm try/finally with end_trace() inside with-block
2. Read docker-compose.yml — confirm MLFLOW_TRACE_TIMEOUT_SECONDS=600, LOCAL_UPLOAD_DIR=/app/uploads, bind mount ../data/uploads:/app/uploads
3. Read gcs.py — confirm _content_hash() and dedup logic in local_save_file()
4. Confirm data/uploads/.gitkeep exists and .gitignore has data/uploads patterns
</verification>

<success_criteria>
- MLflow _run_pipeline has try/finally with trace_id capture and end_trace() call
- MLFLOW_TRACE_TIMEOUT_SECONDS=600 set in docker-compose.yml api service
- Upload bind mount configured: ../data/uploads:/app/uploads
- LOCAL_UPLOAD_DIR=/app/uploads in api service environment
- SHA-256 content hash deduplication in local_save_file()
- data/uploads/.gitkeep exists, data/uploads/* in .gitignore
</success_criteria>

<output>
After completion, create `.planning/phases/42-pipeline-stability-umls-resilience/42-01-SUMMARY.md`
</output>
