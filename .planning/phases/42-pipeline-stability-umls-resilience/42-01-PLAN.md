---
phase: 42-pipeline-stability-umls-resilience
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - services/protocol-processor-service/src/protocol_processor/trigger.py
  - infra/docker-compose.yml
  - infra/.env.example
  - services/api-service/src/api_service/gcs.py
autonomous: true
requirements: [FIX-B14, FIX-B13]

must_haves:
  truths:
    - "MLflow traces are never stuck IN_PROGRESS — try/finally guarantees span closure"
    - "MLFLOW_TRACE_TIMEOUT_SECONDS is set as safety net for orphaned traces"
    - "Upload directory persists across docker compose down/up cycles"
    - "SHA-256 deduplication prevents duplicate PDF storage"
  artifacts:
    - path: "services/protocol-processor-service/src/protocol_processor/trigger.py"
      provides: "try/finally span closure with captured trace_id"
      contains: "end_trace"
    - path: "infra/docker-compose.yml"
      provides: "MLFLOW_TRACE_TIMEOUT_SECONDS env var + upload bind mount"
      contains: "MLFLOW_TRACE_TIMEOUT_SECONDS"
    - path: "services/api-service/src/api_service/gcs.py"
      provides: "SHA-256 content deduplication for local uploads"
      contains: "_content_hash"
  key_links:
    - from: "infra/docker-compose.yml"
      to: "services/api-service/src/api_service/gcs.py"
      via: "LOCAL_UPLOAD_DIR=/app/uploads env var"
      pattern: "LOCAL_UPLOAD_DIR"
    - from: "infra/docker-compose.yml"
      to: "services/protocol-processor-service/src/protocol_processor/trigger.py"
      via: "MLFLOW_TRACE_TIMEOUT_SECONDS env var"
      pattern: "MLFLOW_TRACE_TIMEOUT_SECONDS"
---

<objective>
Fix MLflow trace leaks and persist upload directory across container restarts.

Purpose: MLflow traces stuck IN_PROGRESS pollute the tracing UI and mask real pipeline failures. Upload files lost on container restart force re-upload. Both are stability regressions from the E2E test report (gaps B14, B13).

Output: trigger.py with guaranteed span closure, docker-compose.yml with upload bind mount + MLflow timeout env var, gcs.py with SHA-256 deduplication, cleaned .env.example.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/42-pipeline-stability-umls-resilience/42-RESEARCH.md
@services/protocol-processor-service/src/protocol_processor/trigger.py
@infra/docker-compose.yml
@services/api-service/src/api_service/gcs.py
@infra/.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: MLflow trace leak fix with try/finally span closure</name>
  <files>
    services/protocol-processor-service/src/protocol_processor/trigger.py
    infra/docker-compose.yml
  </files>
  <action>
In `trigger.py`, modify `_run_pipeline()` to add try/finally INSIDE the `with mlflow.start_span()` block:

1. After entering the `with mlflow.start_span(...) as span:` block, immediately capture `trace_id = mlflow.get_active_trace_id()` into a local variable (BEFORE `await graph.ainvoke()`).
2. Wrap the existing span body in `try:` block.
3. Add `finally:` block that:
   - Checks `if trace_id:` (the local variable, not re-calling get_active_trace_id)
   - Logs a warning: `logger.warning("Ensuring MLflow trace %s is closed (try/finally cleanup)", trace_id)`
   - Calls `mlflow.MlflowClient().end_trace(trace_id, status="ERROR")` — this is a no-op if the trace already ended normally via the context manager's `__exit__`.
   - Wraps the end_trace call in its own try/except to log but not raise if force-close fails.

NOTE: The `finally` must be INSIDE the `with` block (not outside), so the span object is still accessible. The `end_trace()` call happens before `__exit__` — but MLflow treats it as a no-op if the trace is already being closed.

In `infra/docker-compose.yml`, add `MLFLOW_TRACE_TIMEOUT_SECONDS=300` to the `api` service's environment section. Per user's revised decision: 300 seconds (5 minutes) as a safety net for orphaned traces. The try/finally is the primary fix; the env var is the safety net for process-kill scenarios.
  </action>
  <verify>
Read trigger.py and confirm: (1) `trace_id = mlflow.get_active_trace_id()` appears immediately after `with mlflow.start_span(...)`, (2) `finally:` block contains `end_trace(trace_id, status="ERROR")`, (3) warning log is present.

Read docker-compose.yml and confirm: `MLFLOW_TRACE_TIMEOUT_SECONDS=300` in api service environment.
  </verify>
  <done>
_run_pipeline() has try/finally inside the span context manager that guarantees trace closure via end_trace(). MLFLOW_TRACE_TIMEOUT_SECONDS=300 set in docker-compose.yml as safety net per user decision.
  </done>
</task>

<task type="auto">
  <name>Task 2: Upload volume persistence with SHA-256 deduplication</name>
  <files>
    infra/docker-compose.yml
    services/api-service/src/api_service/gcs.py
    infra/.env.example
  </files>
  <action>
**Docker bind mount (docker-compose.yml):**
Add to the `api` service:
- Environment: `LOCAL_UPLOAD_DIR=/app/uploads`
- Volume: `../data/uploads:/app/uploads` (path relative to `infra/` directory where docker-compose.yml lives, maps to `data/uploads/` at repo root)

Do NOT add a named volume — this is a bind mount per user decision (dev: host directory, browsable). Docker auto-creates the host directory on `docker compose up` — no .gitkeep or .gitignore changes needed. The existing `data/` rule in .gitignore already covers `data/uploads/`.

**SHA-256 deduplication (gcs.py):**

Import `hashlib` and `json` at top of gcs.py.

Add a `_content_hash(data: bytes) -> str` function using `hashlib.sha256(data).hexdigest()`.

Modify `local_save_file(blob_path, data)` to use a hash-index file at `_LOCAL_UPLOAD_DIR/.hash-index.json` (dict mapping hash -> first blob_path). Logic:
1. Compute `content_hash = _content_hash(data)`
2. Load hash index from `_LOCAL_UPLOAD_DIR/.hash-index.json` (or create empty dict if missing)
3. If hash exists in index, log `"Dedup: content hash {hash[:8]} already stored at {existing_path}"` and create a symlink from new blob_path to the existing blob_path's file
4. If hash is new, save file normally and add entry to index
5. Write index back to `.hash-index.json`

This provides real deduplication (disk space savings) while preserving URI compatibility.

**Legacy UMLS cleanup (.env.example):**
Read `infra/.env.example`. If any UMLS_API_KEY references exist, remove them and add a comment: `# Terminology search uses ToolUniverse SDK (no separate UMLS API key needed)`. If no UMLS references exist, skip this sub-task.
  </action>
  <verify>
1. `grep "LOCAL_UPLOAD_DIR" infra/docker-compose.yml` — shows `/app/uploads`
2. `grep "../data/uploads" infra/docker-compose.yml` — shows bind mount
3. `grep "_content_hash" services/api-service/src/api_service/gcs.py` — shows hash function
  </verify>
  <done>
Docker bind mount persists uploads at data/uploads/ on host (auto-created by Docker). LOCAL_UPLOAD_DIR=/app/uploads unifies both api-service and protocol-processor paths. SHA-256 dedup via hash-index.json prevents storing duplicate PDF content. Upload files remain gitignored via existing data/ rule.
  </done>
</task>

</tasks>

<verification>
1. Read trigger.py — confirm try/finally with end_trace() inside with-block
2. Read docker-compose.yml — confirm MLFLOW_TRACE_TIMEOUT_SECONDS=300, LOCAL_UPLOAD_DIR=/app/uploads, bind mount ../data/uploads:/app/uploads
3. Read gcs.py — confirm _content_hash() and dedup logic in local_save_file()
</verification>

<success_criteria>
- MLflow _run_pipeline has try/finally with trace_id capture and end_trace() call
- MLFLOW_TRACE_TIMEOUT_SECONDS=300 set in docker-compose.yml api service
- Upload bind mount configured: ../data/uploads:/app/uploads
- LOCAL_UPLOAD_DIR=/app/uploads in api service environment
- SHA-256 content hash deduplication in local_save_file()
- Upload files remain gitignored via existing data/ rule
</success_criteria>

<output>
After completion, create `.planning/phases/42-pipeline-stability-umls-resilience/42-01-SUMMARY.md`
</output>
