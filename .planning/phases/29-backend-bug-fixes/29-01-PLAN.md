---
phase: 29-backend-bug-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - services/grounding-service/src/grounding_service/nodes/medgemma_ground.py
  - services/grounding-service/src/grounding_service/prompts/agentic_extract.jinja2
  - services/grounding-service/src/grounding_service/prompts/agentic_evaluate.jinja2
  - services/grounding-service/src/grounding_service/schemas/agentic_actions.py
autonomous: true

must_haves:
  truths:
    - "Grounding pipeline produces real UMLS CUI and SNOMED codes for common medical terms (acetaminophen, osteoarthritis, Heparin)"
    - "At least 80% of extracted entities get non-zero confidence scores after grounding"
    - "Grounding failures are logged with diagnostic detail (MedGemma response, UMLS search results, parse errors) instead of silently falling back to expert_review"
  artifacts:
    - path: "services/grounding-service/src/grounding_service/nodes/medgemma_ground.py"
      provides: "Agentic grounding node with diagnostic logging and fixed parse/search logic"
      contains: "GROUNDING DEBUG"
    - path: "services/grounding-service/src/grounding_service/prompts/agentic_extract.jinja2"
      provides: "Extract prompt that generates precise UMLS search terms"
    - path: "services/grounding-service/src/grounding_service/prompts/agentic_evaluate.jinja2"
      provides: "Evaluate prompt that selects best CUI/SNOMED match from candidates"
  key_links:
    - from: "services/grounding-service/src/grounding_service/nodes/medgemma_ground.py"
      to: "UMLS MCP concept_search"
      via: "MCP tool invocation with search terms from MedGemma extract response"
      pattern: "concept_search.*ainvoke"
    - from: "services/grounding-service/src/grounding_service/nodes/medgemma_ground.py"
      to: "services/grounding-service/src/grounding_service/nodes/validate_confidence.py"
      via: "grounded_entities in GroundingState with non-zero confidence"
      pattern: "grounded_entities"
---

<objective>
Fix grounding confidence stuck at 0% for all entities (BUGF-01). Diagnose root cause by adding comprehensive debug logging to the MedGemma agentic grounding loop, then fix the identified failures in prompt generation, JSON response parsing, and/or UMLS search term quality.

Purpose: Grounding confidence is the foundation for regulatory compliance (21 CFR Part 11). All entities currently fall back to expert_review with 0% confidence, making the UMLS/SNOMED grounding pipeline non-functional.

Output: Working grounding pipeline where 80%+ of entities get real UMLS CUI and SNOMED codes with non-zero confidence.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-backend-bug-fixes/29-RESEARCH.md
@.planning/phases/20-medgemma-agentic-grounding/20-02-SUMMARY.md
@.planning/phases/18-grounding-pipeline-debug-fix/18-01-SUMMARY.md
@services/grounding-service/src/grounding_service/nodes/medgemma_ground.py
@services/grounding-service/src/grounding_service/prompts/agentic_extract.jinja2
@services/grounding-service/src/grounding_service/prompts/agentic_evaluate.jinja2
@services/grounding-service/src/grounding_service/schemas/agentic_actions.py
@services/grounding-service/src/grounding_service/state.py
@services/grounding-service/src/grounding_service/graph.py
@services/grounding-service/src/grounding_service/nodes/validate_confidence.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add diagnostic logging and trace grounding failure root cause</name>
  <files>services/grounding-service/src/grounding_service/nodes/medgemma_ground.py</files>
  <action>
Add comprehensive debug logging to medgemma_ground_node to trace the exact failure point causing 0% confidence for all entities. Follow the logging pattern from 29-RESEARCH.md Pattern 3.

Add logging at these critical points:
1. **Entry**: Log batch_id and criteria count at start of medgemma_ground_node
2. **Extract phase**: Log MedGemma extract response (first 300 chars), parsed entity count, each entity's text/type/search_term
3. **UMLS search phase**: Log candidate count per entity, top candidate CUI/display when found, WARNING for zero-candidate entities with their search_term
4. **Evaluate phase**: Log MedGemma evaluate response (first 300 chars), action_type, selections count
5. **Final results**: Log each grounded entity's text, CUI, SNOMED, confidence, and method. Log summary: total entities, zero-confidence count and percentage
6. **Error paths**: At every except block, log the full raw response that failed parsing (not just the exception message) using logger.error with exc_info=True

Also log at WARNING level when hitting any of the 3 fallback paths:
- _fallback_entities_for_criteria (extract phase failure)
- _fallback_from_entities (evaluate loop exhaustion)
- Empty grounded_entities list

After adding logging, run the grounding pipeline on test data (or read existing logs) to identify which of these root causes is active:
1. MedGemma extract prompt produces malformed JSON (JSONDecodeError at parse)
2. MedGemma extract prompt produces valid JSON but poor search terms (zero UMLS candidates)
3. UMLS MCP subprocess fails to start or crashes (ConnectionError/OSError)
4. MedGemma evaluate prompt returns "refine" for all 3 iterations without "evaluate" action
5. _normalize_search_results drops valid candidates due to format mismatch

Per user decision: fix the agentic loop itself, do NOT bypass to direct Gemini.
  </action>
  <verify>
Run `uv run ruff check services/grounding-service/` passes clean.
Run `uv run mypy services/grounding-service/src/grounding_service/nodes/medgemma_ground.py` passes or only shows pre-existing errors.
Grep for "GROUNDING DEBUG" in medgemma_ground.py confirms logging is present at all 5 critical points.
  </verify>
  <done>
medgemma_ground.py has comprehensive diagnostic logging at all 5 critical points (entry, extract, UMLS search, evaluate, final results) and all 3 fallback paths. Root cause of 0% confidence is identified from log output or code analysis.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix grounding root cause and verify 80%+ entity success rate</name>
  <files>
    services/grounding-service/src/grounding_service/nodes/medgemma_ground.py
    services/grounding-service/src/grounding_service/prompts/agentic_extract.jinja2
    services/grounding-service/src/grounding_service/prompts/agentic_evaluate.jinja2
    services/grounding-service/src/grounding_service/schemas/agentic_actions.py
  </files>
  <action>
Based on root cause identified in Task 1, apply the appropriate fix. The research identifies 5 candidate root causes. Apply the fix for whichever is active (likely multiple):

**If JSON parse failure (root cause 1):**
- Improve _parse_json_response to handle more LLM output formats (markdown fences with language tags like ```json, leading/trailing text around JSON, multiple JSON blocks)
- Add a secondary regex-based extraction as fallback before giving up
- Log the exact character position of parse failure

**If poor search terms (root cause 2):**
- Update agentic_extract.jinja2 to add explicit instructions: "For search_term, use the canonical medical term (e.g., 'diabetes mellitus type 2' not 'patients with diabetes'). Use generic drug names (e.g., 'acetaminophen' not 'Tylenol 500mg'). Keep search terms to 1-3 words when possible."
- Add few-shot examples showing good vs bad search terms in the extract prompt

**If MCP subprocess failure (root cause 3):**
- Add MCP connection health check before batch processing
- Improve error handling around StdioConnection to capture stderr output
- Consider adding a retry for MCP connection initialization (separate from per-entity retry)

**If evaluate loop exhaustion (root cause 4):**
- Update agentic_evaluate.jinja2 to be more directive: "You MUST select the best match from the candidates provided. Use action_type='evaluate' with your selections. Only use action_type='refine' if ZERO candidates are relevant."
- Add logging to track iteration count and action_type at each iteration

**If _normalize_search_results format mismatch (root cause 5):**
- Trace the actual format returned by concept_search MCP tool (list of content blocks per Phase 18 fix)
- Update _normalize_search_results to handle any new format variants
- Add defensive parsing with logging for unexpected formats

Apply the fix, then verify by running tests:
- `uv run pytest services/grounding-service/tests/` should pass
- `uv run ruff check services/grounding-service/` should pass

For the 80%+ quality bar (user decision): this will be verified via manual spot check on 1-2 protocols during E2E testing (no automated re-grounding script per user decision). The code fix should address the systemic issue causing 0% confidence for ALL entities.
  </action>
  <verify>
`uv run pytest services/grounding-service/tests/` passes all tests.
`uv run ruff check services/grounding-service/` passes clean.
`uv run mypy services/grounding-service/src/` passes or shows only pre-existing errors.
Diagnostic logging confirms the fix addresses the root cause (grounding no longer hits the 0%-confidence fallback paths for common medical terms).
  </verify>
  <done>
Root cause of 0% confidence is fixed. The grounding pipeline's extract/evaluate loop successfully processes entities through UMLS search and produces real CUI/SNOMED codes with non-zero confidence for common medical terms. All existing tests pass. Diagnostic logging remains for ongoing observability.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest services/grounding-service/tests/` — all tests pass
2. `uv run ruff check services/grounding-service/` — clean
3. Grep medgemma_ground.py for "GROUNDING DEBUG" — diagnostic logging present
4. Code review: no new fallback paths that silently produce 0% confidence
5. Code review: prompts guide MedGemma toward precise search terms and decisive evaluation
</verification>

<success_criteria>
- Grounding pipeline no longer produces 0% confidence for all entities
- Common medical terms (acetaminophen, osteoarthritis, Heparin) can be grounded to UMLS CUI via the agentic loop
- Diagnostic logging is present at all critical points for future debugging
- All existing tests pass, no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/29-backend-bug-fixes/29-01-SUMMARY.md`
</output>
