---
phase: 40-legacy-cleanup-tooluniverse-grounding
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - infra/docker-compose.yml
  - services/api-service/pyproject.toml
  - services/api-service/src/api_service/umls_search.py
  - services/api-service/src/api_service/terminology_search.py
  - services/api-service/src/api_service/main.py
  - services/api-service/src/api_service/README.md
  - services/api-service/tests/test_schemas.py
  - services/api-service/tests/test_umls_clients.py
  - services/api-service/tests/test_umls_search.py
  - services/protocol-processor-service/pyproject.toml
  - services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py
  - services/protocol-processor-service/src/protocol_processor/tools/terminology_router.py
  - services/protocol-processor-service/src/protocol_processor/tools/medgemma_decider.py
  - services/protocol-processor-service/src/protocol_processor/nodes/ground.py
  - services/protocol-processor-service/src/protocol_processor/config/routing.yaml
  - docs/onboarding.md
  - docs/components/index.md
  - docs/jinja2-prompts.md
  - docs/architecture/system-architecture.md
  - docs/architecture/data-models.md
autonomous: true
requirements:
  - CLEAN-01
  - CLEAN-02
  - CLEAN-03
  - CLEAN-05

must_haves:
  truths:
    - "services/grounding-service/, services/extraction-service/, and services/umls-mcp-server/ directories no longer exist"
    - "No Python file in the repo imports from grounding_service, extraction_service, or umls_mcp_server"
    - "TerminologyRouter dispatches all 6 systems (umls, snomed, icd10, loinc, rxnorm, hpo) through ToolUniverse SDK"
    - "/api/umls/search and /api/terminology/{system}/search endpoints use ToolUniverse backend"
    - "uv sync succeeds without errors after workspace member removal"
    - "uv run pytest services/api-service and services/protocol-processor-service pass with no import errors"
    - "MedGemma agentic reasoning loop asks 3 questions before each retry attempt (valid criterion? derived entity? rephrase?)"
    - "Max 3 grounding attempts per entity, then route to expert_review with warning badge"
    - "No docs/ files reference grounding-service, extraction-service, or umls-mcp-server"
  artifacts:
    - path: "services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py"
      provides: "Singleton ToolUniverse wrapper with TTLCache"
      contains: "search_terminology"
    - path: "services/protocol-processor-service/src/protocol_processor/tools/terminology_router.py"
      provides: "Rewritten TerminologyRouter using ToolUniverse"
      contains: "_query_tooluniverse"
    - path: "services/protocol-processor-service/src/protocol_processor/config/routing.yaml"
      provides: "All api_configs pointing to tooluniverse source"
      contains: "tooluniverse"
    - path: "services/protocol-processor-service/src/protocol_processor/tools/medgemma_decider.py"
      provides: "MedGemma 3-question agentic reasoning loop with query reformulation"
      contains: "agentic_reasoning_loop"
    - path: "services/protocol-processor-service/src/protocol_processor/nodes/ground.py"
      provides: "Max 3-attempt retry loop with expert_review routing"
      contains: "expert_review"
  key_links:
    - from: "services/protocol-processor-service/src/protocol_processor/tools/terminology_router.py"
      to: "services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py"
      via: "import search_terminology"
      pattern: "from protocol_processor\\.tools\\.tooluniverse_client import search_terminology"
    - from: "services/api-service/src/api_service/terminology_search.py"
      to: "services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py"
      via: "import search_terminology for autocomplete"
      pattern: "from protocol_processor\\.tools\\.tooluniverse_client import search_terminology"
    - from: "services/api-service/src/api_service/umls_search.py"
      to: "services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py"
      via: "import search_terminology replacing UmlsClient"
      pattern: "from protocol_processor\\.tools\\.tooluniverse_client import search_terminology"
    - from: "services/protocol-processor-service/src/protocol_processor/nodes/ground.py"
      to: "services/protocol-processor-service/src/protocol_processor/tools/medgemma_decider.py"
      via: "ground_node calls agentic_reasoning_loop for retry logic"
      pattern: "agentic_reasoning_loop|medgemma_decide"
---

<objective>
Delete all three legacy services (grounding-service, extraction-service, umls-mcp-server), create the ToolUniverse client wrapper, rewrite TerminologyRouter and API search endpoints to use ToolUniverse, clean up all workspace references, and rewrite broken tests.

Purpose: Eliminate legacy v1 code that is broken (0% CUI rate due to missing `concept_search` method on UmlsClient) and replace it with a working ToolUniverse SDK integration. This is the single-pass "delete + integrate" plan per user decision.

Output: Clean codebase with zero legacy imports, working ToolUniverse integration in protocol-processor-service, and API endpoints backed by ToolUniverse.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/40-legacy-cleanup-tooluniverse-grounding/40-RESEARCH.md
@services/protocol-processor-service/src/protocol_processor/tools/terminology_router.py
@services/protocol-processor-service/src/protocol_processor/config/routing.yaml
@services/api-service/src/api_service/umls_search.py
@services/api-service/src/api_service/terminology_search.py
@services/api-service/src/api_service/main.py
@pyproject.toml
@infra/docker-compose.yml
@services/api-service/pyproject.toml
@services/protocol-processor-service/pyproject.toml
@services/api-service/tests/test_schemas.py
@services/api-service/tests/test_umls_clients.py
@services/api-service/tests/test_umls_search.py
@services/protocol-processor-service/src/protocol_processor/tools/medgemma_decider.py
@services/protocol-processor-service/src/protocol_processor/nodes/ground.py
@services/api-service/README.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Delete legacy services and clean all workspace references</name>
  <files>
    services/grounding-service/ (DELETE entire directory)
    services/extraction-service/ (DELETE entire directory)
    services/umls-mcp-server/ (DELETE entire directory)
    pyproject.toml
    infra/docker-compose.yml
    services/api-service/pyproject.toml
    services/protocol-processor-service/pyproject.toml
  </files>
  <action>
    **Step 1: Delete legacy service directories**
    ```bash
    rm -rf services/grounding-service services/extraction-service services/umls-mcp-server
    ```

    **Step 2: Clean root pyproject.toml**
    Remove these entries from `[tool.uv.workspace] members`:
    - `"services/extraction-service"`
    - `"services/grounding-service"`
    - `"services/umls-mcp-server"`

    Remove these entries from `[tool.pytest.ini_options] pythonpath`:
    - `"services/extraction-service/src"`
    - `"services/grounding-service/src"`
    - `"services/umls-mcp-server/src"`

    Remove these entries from `[tool.coverage.run] source`:
    - `"services/extraction-service/src"`
    - `"services/grounding-service/src"`
    - `"services/umls-mcp-server/src"`

    Check `[tool.mypy.overrides]` for any ignore entries referencing these services and remove them.

    **Step 3: Clean services/api-service/pyproject.toml**
    Remove ALL three legacy services from `[project] dependencies`:
    - `"extraction-service"`
    - `"grounding-service"`
    - (Note: `umls-mcp-server` is NOT in dependencies, only in the deleted service's own deps)
    Remove ALL three legacy services from `[tool.uv.sources]`:
    - `extraction-service = { workspace = true }`
    - `grounding-service = { workspace = true }`
    Remove legacy service paths from `[tool.pytest.ini_options] pythonpath`:
    - `"../extraction-service/src"`
    - `"../grounding-service/src"`
    Keep `protocol-processor-service`, `shared`, `inference`, `events-py` — these are still valid.

    **Step 4: Clean services/protocol-processor-service/pyproject.toml**
    Remove any references to `grounding-service`, `extraction-service`, or `umls-mcp-server` from dependencies, `[tool.uv.sources]`, `[tool.mypy] mypy_path`, and `[tool.pytest.ini_options] pythonpath`.
    ADD `tooluniverse` and `cachetools` as dependencies:
    ```
    "tooluniverse>=1.0.18",
    "cachetools>=5.0",
    ```

    **Step 5: Clean infra/docker-compose.yml**
    Remove the `extraction-service` and `grounding-service` service blocks entirely (they are under `profiles: [agents]`). Keep everything else.

    **Step 6: Clean docs and READMEs of legacy architecture references**
    Per locked decision "Aggressive cleanup: all docs referencing legacy architecture":

    `docs/onboarding.md`: Remove/rewrite the sentence referencing `services/extraction-service` and `services/grounding-service` (line ~44). Replace with reference to `protocol-processor-service` as the unified pipeline.

    `docs/components/index.md`: Remove the `extraction-service` and `grounding-service` rows from the components table (lines ~9-10). Replace with a single `protocol-processor-service` row describing the unified pipeline.

    `docs/jinja2-prompts.md`: Remove ALL sections referencing `services/extraction-service/src/extraction_service/prompts/` and `services/grounding-service/src/grounding_service/prompts/` paths. These template files no longer exist. Update to reference `services/protocol-processor-service/src/protocol_processor/prompts/` if equivalent templates exist there, otherwise delete the sections entirely.

    `docs/architecture/system-architecture.md`: Update lines referencing `extraction-service` and `grounding-service` in event handlers table and service descriptions. Replace with `protocol-processor-service` as the handler for both `ProtocolUploaded` and `CriteriaExtracted` events.

    `docs/architecture/data-models.md`: Remove/rewrite sections about `extraction-service` `ExtractionState` and `grounding-service` `GroundingState` TypedDicts. Replace with reference to `protocol_processor.state.PipelineState`.

    `services/api-service/README.md`: Remove line 40 (`from extraction_service.graph import create_graph`) — this is a legacy Python import in documentation that will confuse readers.

    **Step 7: Regenerate lock file**
    ```bash
    cd /Users/noahdolevelixir/Code/medgemma-hackathon && uv sync
    ```
    This will regenerate `uv.lock` without the deleted workspace members. Commit the updated `uv.lock`.

    **IMPORTANT PITFALL:** Do NOT use `uv sync --frozen` — it will fail because the lock file still references deleted packages. Use plain `uv sync` to regenerate.
  </action>
  <verify>
    ```bash
    # Directories gone
    test ! -d services/grounding-service && test ! -d services/extraction-service && test ! -d services/umls-mcp-server && echo "PASS: dirs deleted"

    # No references in workspace config
    grep -r "grounding-service\|extraction-service\|umls-mcp-server" pyproject.toml services/api-service/pyproject.toml services/protocol-processor-service/pyproject.toml infra/docker-compose.yml && echo "FAIL: refs remain" || echo "PASS: refs cleaned"

    # No references in docs
    grep -r "grounding-service\|extraction-service\|umls-mcp-server" docs/ services/api-service/README.md && echo "FAIL: doc refs remain" || echo "PASS: doc refs cleaned"

    # uv sync succeeds
    uv sync && echo "PASS: uv sync ok"
    ```
  </verify>
  <done>
    Three legacy service directories deleted. All references removed from root pyproject.toml (workspace members, pythonpath, coverage, mypy), api-service pyproject.toml, protocol-processor-service pyproject.toml, and docker-compose.yml. `uv sync` succeeds and `uv.lock` is updated.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ToolUniverse client wrapper and rewrite TerminologyRouter + API endpoints + tests</name>
  <files>
    services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py (NEW)
    services/protocol-processor-service/src/protocol_processor/tools/terminology_router.py
    services/protocol-processor-service/src/protocol_processor/config/routing.yaml
    services/api-service/src/api_service/umls_search.py
    services/api-service/src/api_service/terminology_search.py
    services/api-service/src/api_service/main.py
    services/api-service/tests/test_schemas.py
    services/api-service/tests/test_umls_clients.py
    services/api-service/tests/test_umls_search.py
  </files>
  <action>
    **Part A: Create tooluniverse_client.py (NEW FILE)**

    Create `services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py` using the complete code example from 40-RESEARCH.md (Pattern 1 + the `search_terminology` function + `_call_tool` + `_parse_result`). Key details:
    - Module-level singleton via `@lru_cache(maxsize=1)` on `_get_tu()`
    - `_TOOL_CATEGORIES = ["umls", "icd", "loinc", "rxnorm", "hpo"]` for selective loading
    - `_SYSTEM_TOOL_MAP` mapping all 6 systems to their ToolUniverse tool names (verified in research):
      - umls -> umls_search_concepts
      - snomed -> snomed_search_concepts
      - icd10 -> ICD10_search_codes
      - loinc -> LOINC_search_tests
      - rxnorm -> RxNorm_get_drug_names
      - hpo -> HPO_search_terms
    - `TTLCache(maxsize=1000, ttl=300)` for autocomplete caching (5-min TTL)
    - Each system's response parsing handles its unique format (see Pattern 3 in research):
      - UMLS/SNOMED: `data.result.results[].{ui, name, rootSource}`
      - ICD10: `data.results[].{code, name}`
      - LOINC: `results[].{code/LOINC_NUM, LONG_COMMON_NAME}`
      - RxNorm: single dict `{rxcui, drug_name}` (NOT a list)
      - HPO: `data[].{id, name}`
    - Check for `"error"` key in raw result (UMLS_API_KEY missing case)
    - Returns `list[GroundingCandidate]` using existing schema from `protocol_processor.schemas.grounding`

    **Part B: Rewrite terminology_router.py**

    Rewrite `services/protocol-processor-service/src/protocol_processor/tools/terminology_router.py`:
    1. Remove ALL imports of `httpx` (ToolUniverse handles HTTP internally)
    2. Remove `_RXNORM_APPROXIMATE_URL`, `_ICD10_SEARCH_URL`, `_LOINC_SEARCH_URL`, `_HPO_SEARCH_URL` constants
    3. Remove `_HTTP_TIMEOUT` constant
    4. Delete methods: `_query_umls`, `_query_snomed`, `_query_direct_api` (and any `_fetch_*` methods)
    5. Add single method `_query_tooluniverse(self, api_name: str, entity_text: str) -> list[GroundingCandidate]`:
       - Import `search_terminology` from `protocol_processor.tools.tooluniverse_client`
       - Call `search_terminology(api_name, entity_text, max_results=10)`
       - Wrap with tenacity retry decorator for `TransientAPIError` (keep existing retry config: `stop_after_attempt(3)`, `wait_random_exponential(multiplier=1, min=2, max=10)`)
    6. Update `route_entity()` to call `_query_tooluniverse()` for ALL api_names (replacing the old dispatch logic that checked `api_configs[api_name]["source"]`)
    7. Keep: YAML config loading, entity type routing logic, error accumulation, `get_apis_for_entity()`, diskcache, tenacity imports, logging
    8. Update docstring to reflect ToolUniverse backend

    **Part C: Update routing.yaml**

    Rewrite `services/protocol-processor-service/src/protocol_processor/config/routing.yaml`:
    - Keep `routing_rules` section as-is EXCEPT: change `Demographic` from `skip: true` to routing to `["umls", "snomed"]` (per user decision: demographics NOT blanket-skipped; age/gender attempt grounding)
    - Add a new entity type `Consent` with `skip: true` (consent/participation criteria should be skipped per user decision)
    - Rewrite `api_configs` section: ALL entries get `source: tooluniverse` and a `tool_name` field:
      ```yaml
      rxnorm:
        source: tooluniverse
        tool_name: RxNorm_get_drug_names
      icd10:
        source: tooluniverse
        tool_name: ICD10_search_codes
      loinc:
        source: tooluniverse
        tool_name: LOINC_search_tests
      hpo:
        source: tooluniverse
        tool_name: HPO_search_terms
      umls:
        source: tooluniverse
        tool_name: umls_search_concepts
      snomed:
        source: tooluniverse
        tool_name: snomed_search_concepts
      ```

    **Part D: Rewrite api-service search endpoints**

    Rewrite `services/api-service/src/api_service/umls_search.py`:
    - Remove import of `umls_mcp_server.umls_api` (deleted service)
    - Import `search_terminology` from `protocol_processor.tools.tooluniverse_client`
    - Keep the same endpoint URL (`/api/umls/search`) and response models
    - Rewrite the handler to call `search_terminology("umls", query, max_results=max_results)` and map `GroundingCandidate` results to `UmlsConceptResponse` (cui=code, name=preferred_term, etc.)

    Rewrite `services/api-service/src/api_service/terminology_search.py`:
    - Remove all `httpx` imports and NLM API URL constants
    - Import `search_terminology` from `protocol_processor.tools.tooluniverse_client`
    - Keep the same endpoint URL (`/api/terminology/{system}/search`) and response models
    - Rewrite the handler to call `search_terminology(system, query, max_results=max_results)` and map results to existing response models

    Check `services/api-service/src/api_service/main.py` — ensure router imports for `umls_search` and `terminology_search` still work (they should since the files still exist, just rewritten). No changes expected unless there are imports from deleted services.

    **Part E: Delete and rewrite broken tests**

    `services/api-service/tests/test_schemas.py`: Delete the ENTIRE file content and rewrite. The old file imports from `extraction_service.schemas.criteria` and `grounding_service.schemas.entities` which no longer exist. Write new tests that import from `protocol_processor.schemas.extraction` and `protocol_processor.schemas.grounding` and test basic Pydantic model validation (field presence, defaults, serialization).

    `services/api-service/tests/test_umls_clients.py`: Delete the ENTIRE file content and rewrite. Write new tests that:
    - Mock `protocol_processor.tools.tooluniverse_client.search_terminology`
    - Test the `/api/umls/search` endpoint returns correct response format
    - Test the `/api/terminology/{system}/search` endpoint for at least 2 systems
    - Use `unittest.mock.patch` to mock the ToolUniverse calls (see test pattern in research)

    `services/api-service/tests/test_umls_search.py`: Delete the ENTIRE file content and rewrite. The existing file imports `from umls_mcp_server.umls_api import SnomedCandidate, UmlsApiError` which will cause ImportError after umls-mcp-server deletion. Rewrite to:
    - Mock `protocol_processor.tools.tooluniverse_client.search_terminology` using `unittest.mock.patch("api_service.umls_search.search_terminology")`
    - Test `/api/umls/search?q=diabetes` returns 200 with ToolUniverse-backed response format (list of `{cui, name, ...}`)
    - Test query validation (too short query returns 422)
    - Test empty results return 200 with empty list
    - Test error handling (mock raising exception returns appropriate error status)
    - Test `max_results` parameter is respected
    - Keep test structure similar to existing but replace all `SnomedCandidate`/`UmlsApiError`/`get_umls_client` references with ToolUniverse-based mocks returning `GroundingCandidate` objects

    **CRITICAL:** Do NOT add `tooluniverse` as a dependency to `services/api-service/pyproject.toml`. api-service imports the wrapper from protocol_processor which is already a workspace dependency.
  </action>
  <verify>
    ```bash
    # Check tooluniverse_client.py exists and has key functions
    grep -q "search_terminology" services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py && echo "PASS: client exists"
    grep -q "_get_tu" services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py && echo "PASS: singleton"

    # Check terminology_router.py has no legacy imports
    grep -q "httpx\|_query_umls\|_query_snomed\|_query_direct_api" services/protocol-processor-service/src/protocol_processor/tools/terminology_router.py && echo "FAIL: legacy code remains" || echo "PASS: router cleaned"

    # Check routing.yaml uses tooluniverse
    grep -q "tooluniverse" services/protocol-processor-service/src/protocol_processor/config/routing.yaml && echo "PASS: routing updated"

    # Check no imports from deleted services anywhere
    grep -r "from umls_mcp_server\|from grounding_service\|from extraction_service" services/ && echo "FAIL: legacy imports remain" || echo "PASS: zero legacy imports"

    # Run tests
    cd /Users/noahdolevelixir/Code/medgemma-hackathon && uv run pytest services/api-service/tests/ -x -q
    cd /Users/noahdolevelixir/Code/medgemma-hackathon && uv run pytest services/protocol-processor-service/tests/ -x -q
    ```
  </verify>
  <done>
    ToolUniverse client wrapper created with singleton pattern and TTLCache. TerminologyRouter rewritten to dispatch all 6 systems through ToolUniverse. routing.yaml updated with all tooluniverse sources and Demographics routed (not skipped). API search endpoints rewritten with ToolUniverse backend. Test files deleted and rewritten against new code. Zero imports from deleted services remain. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement MedGemma agentic reasoning loop with max 3-attempt retry and expert_review routing</name>
  <files>
    services/protocol-processor-service/src/protocol_processor/tools/medgemma_decider.py
    services/protocol-processor-service/src/protocol_processor/nodes/ground.py
    services/protocol-processor-service/src/protocol_processor/prompts/grounding_reasoning.jinja2 (NEW)
  </files>
  <action>
    Per locked decision: "MedGemma agentic reasoning loop before each retry attempt" with "Max 3 attempts total, then route to expert_review queue."

    **Part A: Add agentic reasoning loop to medgemma_decider.py**

    Add a new async function `agentic_reasoning_loop` to `medgemma_decider.py`. This function is called by `ground_node` when the initial grounding attempt returns zero candidates or all candidates have confidence < 0.5. The function:

    1. Takes: `entity` dict, `criterion_context` str, `search_fn` callable (the `search_terminology` function), `router` (TerminologyRouter instance)
    2. Asks MedGemma 3 reasoning questions (in a single prompt, not 3 separate calls — minimize token usage per user decision):
       - **Q1: Valid medical criterion?** "Is '{entity_text}' actually a valid medical criterion that should be grounded to a terminology code? Examples of non-medical: consent, participation, willingness. If not valid, return skip=true."
       - **Q2: Derived entity?** "Is '{entity_text}' a derived entity that maps to a standard concept? Example: 'age >= 18' maps to birthDate concept. If derived, return the standard concept term."
       - **Q3: Rephrase?** "Can '{entity_text}' be rephrased to proper medical terminology for better search results? Example: 'high blood pressure' → 'hypertension'. If so, return the rephrased query."
    3. MedGemma returns reasoning via the existing two-model pattern (MedGemma generates free text, Gemini structures it). Create a new Pydantic model `AgenticReasoningResult`:
       ```python
       class AgenticReasoningResult(BaseModel):
           should_skip: bool = Field(default=False, description="True if entity is not a valid medical criterion (e.g., consent)")
           is_derived: bool = Field(default=False, description="True if entity maps to a standard concept")
           derived_term: str | None = Field(default=None, description="Standard concept term if derived (e.g., 'birthDate')")
           rephrased_query: str | None = Field(default=None, description="Rephrased medical terminology query")
           reasoning: str = Field(default="", description="Brief explanation")
       ```
    4. If `should_skip=True`, return immediately with confidence=0.0, reasoning="Non-medical criterion (e.g., consent) — skipped by MedGemma reasoning"
    5. If `is_derived=True` and `derived_term` is set, use `derived_term` as the new search query
    6. If `rephrased_query` is set, use it as the new search query
    7. Call `search_fn(system, new_query)` for each system the entity_type routes to (get from router.get_apis_for_entity)
    8. If new candidates found, call existing `medgemma_decide()` to select best match
    9. Return the `EntityGroundingResult`

    Create a Jinja2 prompt template `grounding_reasoning.jinja2` in the prompts directory with the 3-question prompt. The template takes `entity_text`, `entity_type`, `criterion_context`, and `previous_query` as variables.

    **IMPORTANT:** The orchestrating agent (Gemini, via `_structure_decision_with_gemini`) and MedGemma should collaborate — Gemini structures the reasoning output and can add its own reformulation suggestions in the structuring step. Add a field `gemini_suggestion: str | None` to `AgenticReasoningResult` for this.

    **Part B: Update ground_node with max 3-attempt retry loop**

    Modify `ground_node` in `ground.py` to implement the retry loop:

    1. After the existing Step 1 (route_entity) and Step 2 (medgemma_decide), check if the result needs retry:
       - Retry condition: `result.confidence < 0.5 AND result.selected_code is None AND entity_type != "Consent"`
    2. If retry needed, enter a loop (max 2 more attempts, for 3 total):
       ```python
       attempt = 1
       while result.selected_code is None and result.confidence < 0.5 and attempt < 3:
           attempt += 1
           reasoning_result = await agentic_reasoning_loop(
               entity, criterion_text, search_terminology, router
           )
           if reasoning_result.should_skip:
               # Non-medical criterion — break with skip result
               result = EntityGroundingResult(
                   entity_text=entity_text,
                   entity_type=entity_type,
                   selected_code=None,
                   selected_system=None,
                   preferred_term=None,
                   confidence=0.0,
                   candidates=[],
                   reasoning=f"Skipped: {reasoning_result.reasoning}",
               )
               break
           # Use reformulated query from reasoning
           new_query = reasoning_result.rephrased_query or reasoning_result.derived_term or entity_text
           new_candidates = await router.route_entity(new_query, entity_type)
           if new_candidates:
               result = await medgemma_decide(entity, new_candidates, criterion_text)
               if result.selected_code and result.confidence >= 0.5:
                   break
       ```
    3. After the loop, if `result.selected_code is None` and all 3 attempts exhausted:
       - Set `result.reasoning` to include "Routed to expert_review after 3 failed grounding attempts"
       - Add a flag to the result: set `result.needs_expert_review = True` (add this field to `EntityGroundingResult` if not present — check the schema first; if not possible, encode in reasoning string)
       - Log at WARNING level: "Entity '{entity_text}' routed to expert_review after 3 attempts"

    4. **Demographic handling:** Do NOT skip Demographics blanket-style. Remove the existing check in ground_node that logs "Entity skipped by TerminologyRouter — no APIs configured for this type" for `entity_type == "Demographic"`. Demographics should now go through the normal grounding flow (routing.yaml already updated in Task 2 to route Demographic to umls/snomed). The agentic reasoning loop handles whether age/gender need derived mapping.

    5. **Consent handling:** Add a check BEFORE the grounding loop: if `entity_type == "Consent"`, skip grounding entirely (return confidence=0.0 with reasoning="Consent/participation criterion — not groundable"). This replaces the old Demographic skip.
  </action>
  <verify>
    ```bash
    # Check agentic_reasoning_loop exists
    grep -q "agentic_reasoning_loop" services/protocol-processor-service/src/protocol_processor/tools/medgemma_decider.py && echo "PASS: agentic loop exists"

    # Check AgenticReasoningResult model exists
    grep -q "AgenticReasoningResult" services/protocol-processor-service/src/protocol_processor/tools/medgemma_decider.py && echo "PASS: reasoning model exists"

    # Check ground_node has retry loop
    grep -q "attempt < 3\|expert_review" services/protocol-processor-service/src/protocol_processor/nodes/ground.py && echo "PASS: retry loop exists"

    # Check Consent skip logic
    grep -q "Consent" services/protocol-processor-service/src/protocol_processor/nodes/ground.py && echo "PASS: consent skip exists"

    # Check prompt template exists
    test -f services/protocol-processor-service/src/protocol_processor/prompts/grounding_reasoning.jinja2 && echo "PASS: prompt template exists"

    # Run tests
    cd /Users/noahdolevelixir/Code/medgemma-hackathon && uv run pytest services/protocol-processor-service/tests/ -x -q
    ```
  </verify>
  <done>
    MedGemma agentic reasoning loop implemented with 3 questions (valid criterion? derived entity? rephrase?). ground_node has max 3-attempt retry loop. Failed entities after 3 attempts route to expert_review with warning in reasoning. Consent entities skip grounding. Demographic entities attempt grounding (age/gender routed to umls/snomed). Gemini collaborates on structuring reasoning output.
  </done>
</task>

</tasks>

<verification>
1. `test ! -d services/grounding-service && test ! -d services/extraction-service && test ! -d services/umls-mcp-server` -- all three deleted
2. `grep -r "from umls_mcp_server\|from grounding_service\|from extraction_service" services/` -- returns nothing (zero legacy imports)
3. `grep -r "grounding-service\|extraction-service\|umls-mcp-server" pyproject.toml infra/docker-compose.yml` -- returns nothing
4. `grep -r "grounding-service\|extraction-service\|umls-mcp-server" docs/ services/api-service/README.md` -- returns nothing (doc refs cleaned)
5. `uv sync` succeeds without errors
6. `uv run pytest services/api-service/tests/ -x` passes
7. `uv run pytest services/protocol-processor-service/tests/ -x` passes
8. `grep "tooluniverse" services/protocol-processor-service/src/protocol_processor/config/routing.yaml` -- all 6 systems use tooluniverse
9. `grep "search_terminology" services/protocol-processor-service/src/protocol_processor/tools/tooluniverse_client.py` -- function exists
10. `grep "agentic_reasoning_loop" services/protocol-processor-service/src/protocol_processor/tools/medgemma_decider.py` -- MedGemma loop exists
11. `grep "expert_review" services/protocol-processor-service/src/protocol_processor/nodes/ground.py` -- expert_review routing exists
12. `grep "Consent" services/protocol-processor-service/src/protocol_processor/nodes/ground.py` -- consent skip exists
</verification>

<success_criteria>
- Three legacy service directories deleted (CLEAN-01)
- ToolUniverse client wrapper with singleton + TTLCache exists (CLEAN-02)
- TerminologyRouter dispatches through ToolUniverse for all 6 systems (CLEAN-02)
- API search endpoints use ToolUniverse backend (CLEAN-02)
- Zero imports from deleted services (CLEAN-03)
- All docs cleaned of legacy architecture references (CLEAN-01)
- MedGemma agentic reasoning loop with 3 questions implemented (CLEAN-05)
- Max 3-attempt retry with expert_review routing in ground_node (CLEAN-05)
- Consent entities skip grounding; Demographics attempt grounding (CLEAN-05)
- All tests pass with no import errors
- `uv sync` and `uv.lock` updated cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/40-legacy-cleanup-tooluniverse-grounding/40-01-SUMMARY.md`
</output>
