---
phase: 07-production-hardening
plan: 03
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - services/api-service/src/api_service/main.py
  - services/api-service/src/api_service/middleware.py
  - services/api-service/src/api_service/reviews.py
  - services/api-service/src/api_service/entities.py
  - libs/shared/src/shared/resilience.py
autonomous: true

must_haves:
  truths:
    - "LangGraph autolog captures extraction and grounding graph execution as nested MLflow traces"
    - "FastAPI middleware logs every API request to MLflow with method, path, status, latency"
    - "Circuit breaker state changes are logged to MLflow with service name and failure count"
    - "HITL review and entity approval actions are traced in MLflow with reviewer, action, and target"
    - "MLflow experiment 'protocol-processing' is created at startup"
  artifacts:
    - path: "services/api-service/src/api_service/middleware.py"
      provides: "MLflow request tracing middleware for FastAPI"
      contains: "mlflow_trace_middleware"
    - path: "services/api-service/src/api_service/main.py"
      provides: "MLflow autolog and experiment setup in lifespan"
      contains: "mlflow.langchain.autolog"
    - path: "libs/shared/src/shared/resilience.py"
      provides: "Circuit breaker MLflow listener"
      contains: "MLflowCircuitBreakerListener"
  key_links:
    - from: "services/api-service/src/api_service/main.py"
      to: "services/api-service/src/api_service/middleware.py"
      via: "middleware registration"
      pattern: "mlflow_trace_middleware"
    - from: "libs/shared/src/shared/resilience.py"
      to: "mlflow"
      via: "CircuitBreakerListener logging state changes"
      pattern: "mlflow.log_param"
---

<objective>
Instrument the entire system with MLflow as the single source of truth for observability: LangGraph autolog for pipeline traces, FastAPI middleware for request tracing, circuit breaker event logging, and HITL action tracing.

Purpose: Per CONTEXT.md user decision, "ALL tracing goes through MLflow, not just AI agent traces." This provides the visibility needed to monitor >95% success rate and <5min targets from the MLflow UI without a separate dashboard.

Output: MLflow middleware, autolog setup, circuit breaker MLflow listener, HITL trace decorators. MLflow experiment "protocol-processing" created at startup.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-production-hardening/07-RESEARCH.md
@.planning/phases/07-production-hardening/07-01-SUMMARY.md
@.planning/phases/07-production-hardening/07-02-SUMMARY.md
@services/api-service/src/api_service/main.py
@services/api-service/src/api_service/reviews.py
@services/api-service/src/api_service/entities.py
@libs/shared/src/shared/resilience.py
@infra/docker-compose.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add MLflow autolog, experiment setup, and FastAPI request tracing middleware</name>
  <files>
    services/api-service/src/api_service/main.py
    services/api-service/src/api_service/middleware.py
  </files>
  <action>
  **1. Create `services/api-service/src/api_service/middleware.py`:**

  FastAPI middleware that traces every HTTP request to MLflow (per CONTEXT.md: "API request traces: every endpoint call with latency, status, user, protocol ID").

  ```python
  """MLflow request tracing middleware for FastAPI.

  Traces every API request with method, path, status code, latency,
  and user info. Skips health/ready endpoints to avoid noise.
  """

  import logging
  import os
  import time

  from fastapi import Request, Response
  from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint

  logger = logging.getLogger(__name__)

  # Skip tracing for health/readiness probes
  _SKIP_PATHS = {"/health", "/ready", "/"}


  class MLflowRequestMiddleware(BaseHTTPMiddleware):
      """Middleware that creates MLflow traces for API requests."""

      async def dispatch(
          self, request: Request, call_next: RequestResponseEndpoint
      ) -> Response:
          if request.url.path in _SKIP_PATHS:
              return await call_next(request)

          start = time.perf_counter()

          try:
              import mlflow

              tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
              if not tracking_uri:
                  # MLflow not configured, skip tracing
                  return await call_next(request)

              with mlflow.start_span(
                  name=f"{request.method} {request.url.path}",
                  span_type="HTTP",
              ) as span:
                  span.set_inputs({
                      "method": request.method,
                      "path": request.url.path,
                      "query": str(request.query_params),
                  })

                  response = await call_next(request)
                  latency_ms = (time.perf_counter() - start) * 1000

                  span.set_outputs({
                      "status_code": response.status_code,
                      "latency_ms": round(latency_ms, 2),
                  })

                  return response

          except ImportError:
              logger.debug("mlflow not installed, skipping request tracing")
              return await call_next(request)
          except Exception:
              # Never let tracing failure break the request
              logger.debug("MLflow tracing failed, continuing without trace", exc_info=True)
              response = await call_next(request)
              return response
  ```

  Use `mlflow.start_span` instead of `mlflow.start_run` to create lightweight spans that nest under parent runs. This avoids creating a full MLflow run per request. Wrap all mlflow operations in try/except so tracing failures never break the API.

  **2. Update `services/api-service/src/api_service/main.py`:**

  a. In the `lifespan` function, BEFORE starting the outbox processor, add MLflow setup:
  ```python
  # Initialize MLflow
  try:
      import mlflow

      tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
      if tracking_uri:
          mlflow.set_tracking_uri(tracking_uri)
          mlflow.set_experiment("protocol-processing")
          mlflow.langchain.autolog(log_models=False)
          logger.info("MLflow initialized: tracking_uri=%s, experiment=protocol-processing", tracking_uri)
      else:
          logger.info("MLFLOW_TRACKING_URI not set, skipping MLflow initialization")
  except ImportError:
      logger.info("mlflow not installed, skipping initialization")
  except Exception:
      logger.warning("MLflow initialization failed, continuing without tracing", exc_info=True)
  ```

  Key decisions:
  - `log_models=False` to avoid large model artifacts in MLflow storage
  - Wrapped in try/except so the API starts even if MLflow is unavailable
  - `mlflow.langchain.autolog()` captures ALL LangGraph graph invocations automatically

  b. Add the MLflow middleware AFTER CORS middleware (before routers):
  ```python
  from api_service.middleware import MLflowRequestMiddleware
  app.add_middleware(MLflowRequestMiddleware)
  ```

  Place it after the CORSMiddleware line.
  </action>
  <verify>
  `uv run ruff check services/api-service/src/api_service/main.py services/api-service/src/api_service/middleware.py` passes.
  `uv run mypy services/api-service/src/api_service/middleware.py` passes.
  `uv run pytest services/api-service/tests/ -x` passes (middleware is safely no-op without MLflow URI).
  </verify>
  <done>
  MLflow experiment "protocol-processing" created at API startup. LangGraph autolog enabled for automatic pipeline tracing. FastAPI middleware traces every non-health request with method, path, status, latency. All MLflow operations wrapped in try/except so API functions without MLflow.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add circuit breaker MLflow listener and HITL action tracing</name>
  <files>
    libs/shared/src/shared/resilience.py
    services/api-service/src/api_service/reviews.py
    services/api-service/src/api_service/entities.py
  </files>
  <action>
  **1. Update `libs/shared/src/shared/resilience.py`:**

  Add MLflow circuit breaker listener (per CONTEXT.md: "Circuit breaker events: when breakers trip/recover, which service, failure count"):

  ```python
  class MLflowCircuitBreakerListener:
      """Log circuit breaker state changes to MLflow.

      Records when breakers trip (open), recover (half_open -> closed),
      or probe (half_open). Safe no-op if MLflow unavailable.
      """

      def state_change(self, cb, old_state, new_state):
          try:
              import mlflow

              tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
              if not tracking_uri:
                  return

              with mlflow.start_span(
                  name=f"circuit_breaker_{cb.name}",
                  span_type="TOOL",
              ) as span:
                  span.set_inputs({
                      "service": cb.name,
                      "old_state": str(old_state),
                      "new_state": str(new_state),
                      "fail_counter": cb.fail_counter,
                  })
          except Exception:
              logger.debug("MLflow circuit breaker logging failed", exc_info=True)
  ```

  Add `import os` at top if not already present.

  Register the listener on all 4 circuit breakers:
  ```python
  _mlflow_listener = MLflowCircuitBreakerListener()

  gemini_breaker = CircuitBreaker(
      fail_max=_FAIL_MAX,
      timeout_duration=_RECOVERY_TIMEOUT,
      name="gemini",
      listeners=[_mlflow_listener],
  )
  # ... same for umls_breaker, gcs_breaker, vertex_ai_breaker
  ```

  **2. Add HITL tracing to reviews.py:**

  Per CONTEXT.md: "HITL events: accept, reject, modify actions by reviewers".

  In the `submit_review_action` endpoint, after creating the Review and AuditLog records but before commit, add MLflow tracing:

  ```python
  # Trace HITL action in MLflow
  try:
      import mlflow

      if os.getenv("MLFLOW_TRACKING_URI"):
          with mlflow.start_span(
              name=f"hitl_review_{body.action}",
              span_type="TOOL",
          ) as span:
              span.set_inputs({
                  "action": body.action,
                  "reviewer_id": body.reviewer_id,
                  "criteria_id": criteria_id,
                  "batch_id": criterion.batch_id,
              })
  except Exception:
      logger.debug("MLflow HITL tracing failed", exc_info=True)
  ```

  Add `import os` at top of reviews.py if not already present.

  **3. Add HITL tracing to entities.py:**

  Find the entity approval action endpoint (similar to criteria review). Add the same MLflow span pattern:

  ```python
  # Trace entity review action in MLflow
  try:
      import mlflow

      if os.getenv("MLFLOW_TRACKING_URI"):
          with mlflow.start_span(
              name=f"hitl_entity_{body.action}",
              span_type="TOOL",
          ) as span:
              span.set_inputs({
                  "action": body.action,
                  "reviewer_id": body.reviewer_id,
                  "entity_id": entity_id,
              })
  except Exception:
      logger.debug("MLflow entity review tracing failed", exc_info=True)
  ```

  Add `import os` at top of entities.py if not already present.

  **Important patterns:**
  - Always wrap MLflow calls in try/except â€” tracing must NEVER break functionality
  - Check `os.getenv("MLFLOW_TRACKING_URI")` before trying MLflow operations
  - Use `start_span` (lightweight) not `start_run` (heavyweight) for inline tracing
  - Use `span_type="TOOL"` for service calls and HITL actions per MLflow conventions
  </action>
  <verify>
  `uv run ruff check libs/shared/src/shared/resilience.py services/api-service/src/api_service/reviews.py services/api-service/src/api_service/entities.py` passes.
  `uv run mypy libs/shared/src/shared/resilience.py` passes.
  `uv run pytest -x` passes (all traces are safely no-op without MLflow).
  </verify>
  <done>
  Circuit breaker state changes (trip, recover) are logged to MLflow with service name, old/new state, and failure count. HITL criteria review actions traced in MLflow with action, reviewer, and target. Entity approval actions traced in MLflow. All tracing wrapped in try/except for safe no-op.
  </done>
</task>

</tasks>

<verification>
1. `uv run ruff check .` passes
2. `uv run mypy .` passes (allowing existing exclusions)
3. `uv run pytest` passes
4. MLflow middleware file exists and is registered in main.py
5. `mlflow.langchain.autolog()` is called in lifespan
6. `mlflow.set_experiment("protocol-processing")` is called in lifespan
7. Circuit breakers have MLflowCircuitBreakerListener attached
8. Reviews endpoint has HITL span tracing
9. Entities endpoint has HITL span tracing
10. All MLflow operations have try/except safety wrappers
</verification>

<success_criteria>
- MLflow experiment "protocol-processing" created at startup (graceful skip if MLflow unavailable)
- LangGraph autolog enabled with log_models=False
- FastAPI middleware traces all non-health requests with method, path, status, latency
- Circuit breaker state changes logged to MLflow via listener
- HITL review actions (criteria + entity) traced in MLflow
- All tracing is safely no-op without MLFLOW_TRACKING_URI
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-production-hardening/07-03-SUMMARY.md`
</output>
