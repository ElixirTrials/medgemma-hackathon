---
phase: 32-entity-model-ground-node-multi-code-display
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - services/protocol-processor-service/src/protocol_processor/graph.py
  - services/protocol-processor-service/src/protocol_processor/trigger.py
  - services/protocol-processor-service/pyproject.toml
  - services/api-service/src/api_service/protocols.py
autonomous: true

must_haves:
  truths:
    - "Pipeline graph uses PostgreSQL checkpointer to save state after each node"
    - "Retry endpoint resumes from last successful checkpoint instead of re-running entire pipeline"
    - "Protocol status correctly tracks pipeline failures with recoverable state"
  artifacts:
    - path: "services/protocol-processor-service/src/protocol_processor/graph.py"
      provides: "Graph compiled with PostgresSaver checkpointer"
      contains: "PostgresSaver"
    - path: "services/protocol-processor-service/src/protocol_processor/trigger.py"
      provides: "Thread-based config for checkpointing using protocol_id"
      contains: "thread_id"
    - path: "services/api-service/src/api_service/protocols.py"
      provides: "Retry endpoint that resumes from checkpoint"
      contains: "ainvoke.*None"
  key_links:
    - from: "services/protocol-processor-service/src/protocol_processor/trigger.py"
      to: "services/protocol-processor-service/src/protocol_processor/graph.py"
      via: "graph.ainvoke with thread_id config"
      pattern: "thread_id.*protocol_id"
    - from: "services/api-service/src/api_service/protocols.py"
      to: "services/protocol-processor-service/src/protocol_processor/graph.py"
      via: "graph.ainvoke(None, config) for checkpoint resume"
      pattern: "ainvoke.*None"
---

<objective>
Add LangGraph PostgreSQL checkpointing to the 5-node pipeline and update the retry endpoint to resume from the last checkpoint instead of restarting.

Purpose: When grounding fails (API timeout, rate limit), the pipeline currently has no recovery path except full restart. Checkpointing saves state after each node, so retry skips successful nodes (ingest, extract, parse) and resumes from the failed node (ground).

Output: Graph with PostgresSaver, trigger with thread_id config, retry endpoint that passes None to ainvoke for checkpoint resume.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@services/protocol-processor-service/src/protocol_processor/graph.py
@services/protocol-processor-service/src/protocol_processor/trigger.py
@services/protocol-processor-service/src/protocol_processor/state.py
@services/api-service/src/api_service/protocols.py
@.planning/phases/32-entity-model-ground-node-multi-code-display/32-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add PostgreSQL checkpointer to graph and trigger</name>
  <files>
    services/protocol-processor-service/src/protocol_processor/graph.py
    services/protocol-processor-service/src/protocol_processor/trigger.py
    services/protocol-processor-service/pyproject.toml
  </files>
  <action>
**pyproject.toml:** Add `langgraph-checkpoint-postgres>=2.0` and `psycopg[binary]>=3.1` to dependencies.

**graph.py:** Update `create_graph()` to accept a checkpointer and compile with it:

```python
from langgraph.checkpoint.postgres import PostgresSaver

def create_graph(checkpointer=None):
    workflow = StateGraph(PipelineState)
    # ... existing node/edge setup unchanged ...
    return workflow.compile(checkpointer=checkpointer)
```

Update `get_graph()` to create a PostgresSaver singleton using DATABASE_URL from environment:

```python
_checkpointer = None

def _get_checkpointer():
    global _checkpointer
    if _checkpointer is None:
        import os
        db_url = os.environ["DATABASE_URL"]
        # Convert sqlalchemy URL format to psycopg format if needed
        # sqlalchemy: postgresql://... -> psycopg needs postgresql://...
        _checkpointer = PostgresSaver.from_conn_string(db_url)
        _checkpointer.setup()  # Create checkpoint tables
    return _checkpointer

def get_graph():
    global _graph
    if _graph is None:
        _graph = create_graph(checkpointer=_get_checkpointer())
    return _graph
```

Per Pitfall 1 from research: checkpointer is singleton, created once, reused across all invocations. Do NOT create per-invocation.

**trigger.py:** Update `handle_protocol_uploaded` to pass `thread_id` config to `graph.ainvoke`:

```python
config = {"configurable": {"thread_id": protocol_id}}
result = asyncio.run(graph.ainvoke(initial_state, config))
```

The protocol_id is used as thread_id so retry can find the checkpoint later. Update both the MLflow and non-MLflow code paths.

Also add a new function `retry_from_checkpoint` that the API endpoint will call:

```python
async def retry_from_checkpoint(protocol_id: str) -> dict:
    """Resume pipeline from last checkpoint for a failed protocol."""
    from protocol_processor.graph import get_graph
    graph = get_graph()
    config = {"configurable": {"thread_id": protocol_id}}
    result = await graph.ainvoke(None, config)
    return result
```

**Important:** The PipelineState has `pdf_bytes: bytes | None` which may not serialize well to PostgreSQL checkpoints. If PostgresSaver has issues with bytes fields, convert pdf_bytes to base64 string before checkpointing or handle via a custom serializer. Test this during implementation.
  </action>
  <verify>
`uv run python -c "from protocol_processor.graph import create_graph; print('graph import ok')"` succeeds.

`uv run python -c "from protocol_processor.trigger import retry_from_checkpoint; print('retry import ok')"` succeeds.

Check that `langgraph-checkpoint-postgres` is in the resolved dependencies: `uv run pip list | grep langgraph`
  </verify>
  <done>
Graph compiled with PostgresSaver checkpointer. Trigger passes thread_id=protocol_id config. retry_from_checkpoint function available for API endpoint.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update retry endpoint to resume from checkpoint</name>
  <files>
    services/api-service/src/api_service/protocols.py
  </files>
  <action>
Update the existing `retry_protocol` endpoint (`POST /protocols/{protocol_id}/retry`) to resume from the LangGraph checkpoint instead of resetting status and creating a new outbox event.

**CRITICAL: Make the endpoint `async def` — FastAPI runs within an active event loop so `asyncio.run()` will raise RuntimeError. Use `await` instead.**

Current behavior (remove):
- Resets protocol.status to "uploaded"
- Creates a new ProtocolUploaded outbox event
- Pipeline starts from scratch

New behavior (implement):

```python
@router.post("/{protocol_id}/retry", response_model=RetryResponse)
async def retry_protocol(
    protocol_id: str,
    db: Session = Depends(get_db),
) -> RetryResponse:
    protocol = db.get(Protocol, protocol_id)
    if not protocol:
        raise HTTPException(status_code=404, detail=f"Protocol {protocol_id} not found")

    retryable_states = ["extraction_failed", "grounding_failed", "dead_letter"]
    if protocol.status not in retryable_states:
        raise HTTPException(
            status_code=400,
            detail=f"Protocol is not in a retryable state (current: {protocol.status})",
        )

    # Update status to processing
    protocol.status = "processing"
    protocol.error_reason = None
    db.add(protocol)
    db.commit()

    # Resume from checkpoint — await directly (do NOT use asyncio.run inside async endpoint)
    try:
        from protocol_processor.trigger import retry_from_checkpoint
        await retry_from_checkpoint(protocol_id)
    except Exception as e:
        protocol.error_reason = str(e)[:500]
        db.add(protocol)
        db.commit()
        logger.error("Retry failed for protocol %s: %s", protocol_id, e)

    return RetryResponse(status="retry_started", protocol_id=protocol_id)
```

Remove the outbox event creation code from the retry endpoint. The retry goes directly through the graph, not through the outbox.

**Important:** Keep the old ProtocolUploaded outbox pattern for INITIAL runs (via trigger.py handle_protocol_uploaded). Only retry uses checkpoint resume.
  </action>
  <verify>
`uv run ruff check services/api-service/src/api_service/protocols.py` passes.

`uv run python -c "from api_service.protocols import router; print('protocols import ok')"` succeeds.

Confirm no `asyncio.run(` remains in the retry endpoint: `grep -n "asyncio.run" services/api-service/src/api_service/protocols.py` returns no match (or only matches outside the retry endpoint).
  </verify>
  <done>
Retry endpoint is `async def`, uses `await retry_from_checkpoint(protocol_id)` (not asyncio.run). Resumes from last checkpoint (passes None to graph.ainvoke). No new outbox event created on retry. Protocol status transitions: failed -> processing -> (checkpoint resume) -> completed/failed.
  </done>
</task>

</tasks>

<verification>
- `uv run ruff check services/protocol-processor-service/src/ services/api-service/src/api_service/protocols.py` passes
- PostgresSaver is used in graph compilation
- Trigger passes thread_id config to graph.ainvoke
- Retry endpoint is `async def` and uses `await retry_from_checkpoint(...)` (no asyncio.run inside FastAPI endpoint)
- No changes to node logic (ingest, extract, parse, ground, persist unchanged)
</verification>

<success_criteria>
- Graph compiled with PostgresSaver singleton checkpointer
- Pipeline invocation uses protocol_id as thread_id for deterministic checkpoint lookup
- Retry endpoint is async def and awaits retry_from_checkpoint directly (no asyncio.run)
- Initial pipeline runs still use outbox trigger pattern unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/32-entity-model-ground-node-multi-code-display/32-02-SUMMARY.md`
</output>
