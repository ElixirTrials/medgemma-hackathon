---
phase: 32-entity-model-ground-node-multi-code-display
plan: 02
type: execute
wave: 2
depends_on: ["32-01"]
files_modified:
  - services/grounding-service/src/grounding_service/terminology/router.py
  - services/grounding-service/src/grounding_service/nodes/ground.py
  - services/grounding-service/src/grounding_service/prompts/ground_extract.jinja2
  - services/grounding-service/src/grounding_service/schemas/extraction.py
  - services/grounding-service/src/grounding_service/graph.py
  - services/grounding-service/src/grounding_service/state.py
  - services/api-service/src/api_service/main.py
  - services/api-service/src/api_service/protocols.py
autonomous: true

must_haves:
  truths:
    - "Ground node extracts entities using Gemini structured output and routes them to terminology APIs via TerminologyRouter"
    - "Entity extraction produces structured results with entity type, text, and span positions"
    - "Pipeline failure at any stage leaves protocol in recoverable state with error reason"
    - "Failed protocols can be retried via API endpoint that resumes from LangGraph checkpoint"
  artifacts:
    - path: "services/grounding-service/src/grounding_service/terminology/router.py"
      provides: "TerminologyRouter dispatching entities to system-specific clients"
      contains: "class TerminologyRouter"
    - path: "services/grounding-service/src/grounding_service/nodes/ground.py"
      provides: "Ground node with Gemini structured entity extraction + terminology routing"
      contains: "async def ground_node"
    - path: "services/grounding-service/src/grounding_service/schemas/extraction.py"
      provides: "Pydantic schemas for Gemini structured output"
      contains: "class BatchEntityExtractionResult"
    - path: "services/grounding-service/src/grounding_service/graph.py"
      provides: "LangGraph graph with PostgreSQL checkpointer"
      contains: "PostgresSaver"
  key_links:
    - from: "services/grounding-service/src/grounding_service/nodes/ground.py"
      to: "services/grounding-service/src/grounding_service/terminology/router.py"
      via: "TerminologyRouter.ground_entity()"
      pattern: "router\\.ground_entity"
    - from: "services/grounding-service/src/grounding_service/nodes/ground.py"
      to: "langchain_google_genai"
      via: "ChatGoogleGenerativeAI with structured output"
      pattern: "with_structured_output"
    - from: "services/api-service/src/api_service/protocols.py"
      to: "services/grounding-service/src/grounding_service/graph.py"
      via: "retry endpoint invoking graph with None input"
      pattern: "ainvoke.*None"
---

<objective>
Implement the ground node with Gemini structured entity extraction and TerminologyRouter integration, add LangGraph PostgreSQL checkpointing for fault tolerance, and create a retry API endpoint for failed protocols.

Purpose: This is the core grounding intelligence — Gemini extracts entities with guaranteed valid JSON, TerminologyRouter maps them to multi-system codes, and checkpointing enables retry-from-failure instead of full pipeline restart.
Output: Working ground node, terminology router, checkpointed graph, and retry endpoint.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/32-entity-model-ground-node-multi-code-display/32-CONTEXT.md
@.planning/phases/32-entity-model-ground-node-multi-code-display/32-RESEARCH.md
@.planning/phases/32-entity-model-ground-node-multi-code-display/32-01-SUMMARY.md
@services/grounding-service/src/grounding_service/graph.py
@services/grounding-service/src/grounding_service/state.py
@services/grounding-service/src/grounding_service/nodes/medgemma_ground.py
@services/grounding-service/src/grounding_service/nodes/validate_confidence.py
@libs/shared/src/shared/models.py
@services/api-service/src/api_service/protocols.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TerminologyRouter and Gemini extraction schemas</name>
  <files>
    services/grounding-service/src/grounding_service/terminology/router.py
    services/grounding-service/src/grounding_service/schemas/extraction.py
    services/grounding-service/src/grounding_service/prompts/ground_extract.jinja2
  </files>
  <action>
**TerminologyRouter (terminology/router.py):**
Create a class that routes entities to appropriate terminology APIs based on entity type:
- `ROUTING_TABLE` mapping entity types to terminology system lists (in priority order):
  - Medication → ["rxnorm", "umls_snomed"]
  - Condition → ["umls_snomed", "icd10"]
  - Procedure → ["umls_snomed"]
  - Lab_Value → ["loinc", "umls_snomed"]
  - Biomarker → ["umls_snomed", "hpo"]
  - Demographic → [] (explicitly skip with logging)
- `async def ground_entity(self, entity_text: str, entity_type: str, context: str = "") -> dict[str, str | None]`
  - Calls each relevant terminology client in sequence
  - Returns partial results on partial failure (if RxNorm succeeds but ICD-10 fails, still return RxNorm code)
  - Returns dict with keys: rxnorm_code, icd10_code, loinc_code, hpo_code, umls_cui, snomed_code, grounding_system, grounding_confidence, grounding_error
  - grounding_confidence = highest confidence from any successful lookup
  - grounding_system = first system that returned a result (based on routing priority)
  - grounding_error = None if any system succeeded; specific error message if all failed
- Initialize with lazy singleton terminology clients from Plan 01
- Use existing UmlsClient for umls_snomed lookups (import from umls-mcp-server or shared)

**Pydantic extraction schemas (schemas/extraction.py):**
Create schemas for Gemini structured output:
- `ExtractedEntity(BaseModel)`: text, entity_type (enum: Condition/Medication/Procedure/Lab_Value/Biomarker/Demographic), span_start (default=0), span_end (default=0), context_window (default="")
  - All optional fields use `Field(default=...)` to avoid Gemini omission issues (Pitfall 3 from RESEARCH.md)
- `CriterionEntities(BaseModel)`: criterion_id (str), entities (list[ExtractedEntity])
- `BatchEntityExtractionResult(BaseModel)`: results (list[CriterionEntities])

**Extraction prompt (prompts/ground_extract.jinja2):**
Create a Jinja2 prompt template for Gemini entity extraction:
- System instructions: "You are a medical entity extraction expert. Extract all medical entities from clinical trial eligibility criteria."
- Entity type definitions with examples for each type
- Instructions to produce one definitive entity per mention (not synonyms)
- Few-shot examples showing expected JSON structure matching the Pydantic schema
- User template accepting criteria list with IDs
  </action>
  <verify>
`uv run ruff check services/grounding-service/` passes.
`uv run python -c "from grounding_service.terminology.router import TerminologyRouter; from grounding_service.schemas.extraction import BatchEntityExtractionResult"` succeeds.
  </verify>
  <done>TerminologyRouter routes entities to 5 terminology systems based on entity type with partial failure handling. Pydantic schemas guarantee valid Gemini structured output. Extraction prompt ready for use.</done>
</task>

<task type="auto">
  <name>Task 2: Implement ground node with Gemini structured extraction and terminology routing</name>
  <files>
    services/grounding-service/src/grounding_service/nodes/ground.py
    services/grounding-service/src/grounding_service/nodes/__init__.py
  </files>
  <action>
**Ground node (nodes/ground.py):**
Create the new ground node that replaces the medgemma_ground node for entity extraction:

`async def ground_node(state: GroundingState) -> dict`:
1. **Extract entities with Gemini structured output:**
   - Use `ChatGoogleGenerativeAI(model=os.getenv("GEMINI_MODEL_NAME", "gemini-2.5-flash"), temperature=0)` with `with_structured_output(BatchEntityExtractionResult)`
   - Render ground_extract.jinja2 prompt with criteria texts from state
   - Invoke structured LLM — result is guaranteed-valid BatchEntityExtractionResult
   - Per user decision: "Gemini produces fully structured results using native structured output (response_schema with Pydantic models)"

2. **Route entities to terminology systems:**
   - Initialize TerminologyRouter (singleton)
   - For each extracted entity, call `router.ground_entity(entity.text, entity.entity_type, entity.context_window)`
   - Collect all grounding results

3. **Build entity records:**
   - For each grounded entity, create Entity-compatible dict with all multi-code fields populated from TerminologyRouter result
   - Map criterion_id from extraction result to actual criteria IDs in state

4. **Handle errors:**
   - Wrap entire extraction in try/except
   - On Gemini API failure: set state["error"] with specific message, return early
   - On individual entity grounding failure: log warning, continue with partial results, set grounding_error on that entity
   - Per CONTEXT.md: "Retry from failed node only (resume from last LangGraph checkpoint)"

5. **Return updated state:**
   - Set state["entities"] with grounded entity dicts
   - Set state["error"] = None on success

**Update __init__.py exports** to include the new ground_node alongside existing nodes.

NOTE: The existing medgemma_ground_node is preserved for now. The new ground_node is the replacement that uses Gemini for extraction (per user decision) while MedGemma can still serve as medical expert in future iterations.
  </action>
  <verify>
`uv run ruff check services/grounding-service/` passes.
`uv run mypy services/grounding-service/src/grounding_service/nodes/ground.py` passes (or has only pre-existing issues).
  </verify>
  <done>Ground node extracts entities via Gemini structured output, routes through TerminologyRouter, and populates multi-code fields. Error handling preserves partial results and sets specific error messages.</done>
</task>

<task type="auto">
  <name>Task 3: Add LangGraph checkpointing, rewire graph, and create retry endpoint</name>
  <files>
    services/grounding-service/src/grounding_service/graph.py
    services/grounding-service/src/grounding_service/state.py
    services/api-service/src/api_service/protocols.py
    services/api-service/src/api_service/main.py
  </files>
  <action>
**Graph with checkpointing (graph.py):**
- Add PostgreSQL checkpointer using `langgraph-checkpoint-postgres`:
  ```python
  from langgraph.checkpoint.postgres import PostgresSaver
  ```
- Create checkpointer from DATABASE_URL env var (same connection string as main DB)
- Call `checkpointer.setup()` to create checkpoint tables if needed
- Replace old 2-node graph (medgemma_ground → validate_confidence) with new graph:
  - START → ground_node → validate_confidence → END
  - Keep conditional error routing after ground_node (same should_continue pattern)
- Compile with `workflow.compile(checkpointer=checkpointer)`
- Create graph instance as singleton (avoid Pitfall 1 — connection pool exhaustion)
- Install dependency: `uv add langgraph-checkpoint-postgres psycopg` in grounding-service pyproject.toml

**State update (state.py):**
- Ensure GroundingState TypedDict includes `error: str | None` field (may already exist)
- Add `thread_id: str | None` field for checkpoint tracking

**Retry endpoint (api_service/protocols.py):**
- Add `POST /protocols/{protocol_id}/retry` endpoint:
  - Look up protocol by ID, verify status is in a failed state (extraction_failed, grounding_failed)
  - Resume from checkpoint: `graph.ainvoke(None, config={"configurable": {"thread_id": protocol_id}})`
  - Do NOT pass initial state — this resumes from last successful node (Pitfall 5 from RESEARCH.md)
  - Update protocol status to "grounding" before retry
  - On success: protocol transitions to pending_review
  - On failure: protocol stays in failed state with updated error_reason
  - Return 200 with {"status": "retrying"} or 400 if protocol not in failed state
- Import grounding graph from grounding_service

**Register retry route in main.py** if not already part of the protocols router.
  </action>
  <verify>
`uv run ruff check services/grounding-service/ services/api-service/` passes.
Grounding graph compiles: `uv run python -c "from grounding_service.graph import create_graph"` succeeds (may need DATABASE_URL set).
`uv run pytest services/grounding-service/tests/ -x -q` — graph compilation tests pass (update if needed for new graph structure).
  </verify>
  <done>Graph compiled with PostgreSQL checkpointer. Retry endpoint at POST /protocols/{id}/retry resumes from last checkpoint. Failed protocols recoverable without full pipeline restart.</done>
</task>

</tasks>

<verification>
- `uv run ruff check .` passes
- TerminologyRouter routes 6 entity types to correct terminology systems
- Ground node uses Gemini structured output with BatchEntityExtractionResult schema
- Graph compiled with PostgresSaver checkpointer
- Retry endpoint returns 200 for failed protocols and 400 for non-failed protocols
- Grounding service tests pass with updated graph structure
</verification>

<success_criteria>
Complete grounding pipeline: Gemini extracts entities with structured output, TerminologyRouter maps to multi-system codes, checkpointing enables fault-tolerant retry. Failed protocols recoverable via single API call.
</success_criteria>

<output>
After completion, create `.planning/phases/32-entity-model-ground-node-multi-code-display/32-02-SUMMARY.md`
</output>
