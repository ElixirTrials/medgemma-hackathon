---
phase: 20-medgemma-agentic-grounding
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - libs/inference/src/inference/config.py
  - libs/inference/src/inference/model_garden.py
  - libs/inference/src/inference/__init__.py
  - libs/inference/pyproject.toml
  - .env.example
autonomous: true

must_haves:
  truths:
    - "AgentConfig.from_env() reads MODEL_BACKEND, VERTEX_ENDPOINT_ID, GCP_PROJECT_ID, GCP_REGION from environment and returns a frozen dataclass"
    - "ModelGardenChatModel wraps Vertex AI Model Garden endpoints via google-cloud-aiplatform SDK and returns AIMessage from _generate()"
    - "MedGemma prompt formatting uses Gemma chat template with <start_of_turn>user/model delimiters, folding system messages into user turn"
    - "Vertex endpoint.predict calls have retry with exponential backoff for transient errors (ServiceUnavailable, InternalServerError, ResourceExhausted)"
    - "create_model_loader(config) returns a lazy callable that produces ModelGardenChatModel for vertex backend or falls back to local HuggingFace loader"
  artifacts:
    - path: "libs/inference/src/inference/config.py"
      provides: "AgentConfig dataclass with from_env() and supports_tools property"
      contains: "class AgentConfig"
    - path: "libs/inference/src/inference/model_garden.py"
      provides: "ModelGardenChatModel, _build_gemma_prompt, _predict_with_retry, create_model_loader"
      contains: "class ModelGardenChatModel"
    - path: "libs/inference/src/inference/__init__.py"
      provides: "Public exports for AgentConfig, create_model_loader"
      contains: "AgentConfig"
  key_links:
    - from: "libs/inference/src/inference/model_garden.py"
      to: "libs/inference/src/inference/config.py"
      via: "import AgentConfig"
      pattern: "from inference\\.config import AgentConfig"
    - from: "libs/inference/src/inference/model_garden.py"
      to: "google.cloud.aiplatform.Endpoint"
      via: "Vertex AI SDK predict call"
      pattern: "endpoint\\.predict"
---

<objective>
Port ModelGardenChatModel and AgentConfig from gemma-hackathon to libs/inference/, enabling MedGemma Vertex AI Model Garden endpoint invocation.

Purpose: The agentic grounding node (Plan 02) needs a LangChain-compatible chat model that wraps MedGemma's Vertex AI endpoint. The reference implementation at gemma-hackathon has a proven ModelGardenChatModel + AgentConfig pattern that handles Gemma-specific prompt formatting, retry logic, and endpoint configuration.

Output: libs/inference/ gains config.py (AgentConfig) and model_garden.py (ModelGardenChatModel + create_model_loader), ready for import by grounding-service.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-medgemma-agentic-grounding/20-RESEARCH.md

Reference implementation (port from):
@/Users/noahdolevelixir/Code/gemma-hackathon/components/inference/src/inference/config.py
@/Users/noahdolevelixir/Code/gemma-hackathon/components/inference/src/inference/model_factory.py

Current inference library:
@libs/inference/src/inference/__init__.py
@libs/inference/src/inference/factory.py
@libs/inference/src/inference/loaders.py
@libs/inference/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Port AgentConfig dataclass to libs/inference/config.py</name>
  <files>libs/inference/src/inference/config.py</files>
  <action>
Create `libs/inference/src/inference/config.py` by porting the `AgentConfig` dataclass from `/Users/noahdolevelixir/Code/gemma-hackathon/components/inference/src/inference/config.py`.

The file is a near-exact copy with these adaptations:
1. Copy the entire `AgentConfig` frozen dataclass with all fields: backend, model_path, quantization, max_new_tokens, gcp_project_id, gcp_region, vertex_endpoint_id, vertex_model_name, vertex_endpoint_url
2. Copy `from_env()` classmethod that reads MODEL_BACKEND (or MEDGEMMA_BACKEND), MEDGEMMA_MODEL_PATH, MEDGEMMA_QUANTIZATION, MEDGEMMA_MAX_TOKENS, GCP_PROJECT_ID, GCP_REGION, VERTEX_ENDPOINT_ID, VERTEX_MODEL_NAME, VERTEX_ENDPOINT_URL
3. Copy `supports_tools` property (returns True only if backend=="vertex" AND vertex_model_name is set; returns False for endpoint-only configs since Model Garden endpoints don't support native tool calling)
4. Keep the `__future__ annotations` import and `os` + `dataclasses` imports
5. Keep docstrings and type annotations from the reference

Do NOT add any new fields or modify the behavior from the reference. This is a direct port.
  </action>
  <verify>
Run `cd /Users/noahdolevelixir/Code/medgemma-hackathon && uv run python -c "from inference.config import AgentConfig; c = AgentConfig.from_env(); print(f'backend={c.backend}, endpoint={c.vertex_endpoint_id[:20]}..., supports_tools={c.supports_tools}')"` — should print backend=vertex, endpoint ID prefix, supports_tools=False (because we use endpoint, not model_name).
  </verify>
  <done>AgentConfig.from_env() correctly reads environment variables and supports_tools returns False for Model Garden endpoint config.</done>
</task>

<task type="auto">
  <name>Task 2: Port ModelGardenChatModel and create_model_loader to model_garden.py</name>
  <files>
libs/inference/src/inference/model_garden.py
libs/inference/src/inference/__init__.py
libs/inference/pyproject.toml
.env.example
  </files>
  <action>
Create `libs/inference/src/inference/model_garden.py` by porting from `/Users/noahdolevelixir/Code/gemma-hackathon/components/inference/src/inference/model_factory.py`. This file contains the core Vertex AI Model Garden integration. Port these components:

1. **`_build_gemma_prompt(messages)`** — formats LangChain messages into Gemma chat template with `<start_of_turn>user/model` delimiters. System messages are folded into user turns with `### Instruction:\n` prefix. Ends with `<start_of_turn>model\n` to prime response generation. Port exactly as-is from reference.

2. **`_is_retryable_error(exception)`** — checks if exception is transient (retries on ServiceUnavailable, InternalServerError, DeadlineExceeded, ResourceExhausted, ConnectionError, Timeout, ChunkedEncodingError; does NOT retry on PermissionDenied, Unauthenticated, InvalidArgument, NotFound). Port exactly from reference.

3. **`_predict_with_retry(endpoint, instances, parameters)`** — tenacity @retry decorator with stop_after_attempt(3), wait_exponential(multiplier=1, min=1, max=10), reraise=True. Port exactly from reference.

4. **`ModelGardenChatModel(BaseChatModel)`** — Define as a top-level class (NOT nested inside a function like in the reference). Fields: endpoint_resource_name (str), project (str), location (str), max_output_tokens (int, default=512). The `_generate()` method:
   - Initializes `aiplatform.Endpoint(self.endpoint_resource_name)`
   - Builds prompt via `_build_gemma_prompt(messages)`
   - Creates instance dict with prompt, max_tokens, temperature, top_p=0.95, top_k=40
   - Creates parameters dict with max_output_tokens and temperature
   - Calls `_predict_with_retry(endpoint, [instance], parameters)`
   - Strips echoed prompt from response if present
   - Returns ChatResult with AIMessage
   - `_llm_type` property returns "vertex_model_garden"

5. **`_validate_vertex_config(cfg)`** — validates project_id, region, and either endpoint_id or vertex_model_name are set. Port from reference.

6. **`create_model_loader(config=None)`** — factory that returns a lazy callable. If config.backend == "vertex", creates a loader that:
   - Calls `vertexai.init(project=project_id, location=region)`
   - If vertex_model_name is set, uses ChatGoogleGenerativeAI (for Gemini models)
   - If only endpoint_id, builds endpoint_resource_name and returns ModelGardenChatModel
   - Uses `@lazy_singleton` from shared.lazy_cache for caching
   - If backend is "local", raise NotImplementedError("Local MedGemma loading not yet ported") — we don't need local loading for this phase

Key differences from reference implementation:
- ModelGardenChatModel is a top-level class, not nested inside `_build_vertex_endpoint_model()`. This is cleaner for imports and type checking.
- Remove `_ensure_hf_auth_env()`, `_import_torch()`, `_import_langchain_hf()`, `_is_model_cached()`, `_build_model_kwargs()`, `_load_hf_pipeline()`, `_create_local_model_loader()` — these are local/HuggingFace-specific and not needed for this phase.
- Remove `create_gemini_model_loader()` — we already have Gemini loading elsewhere.
- Remove `_get_dedicated_endpoint_info()` — unnecessary complexity for our deployment.
- Import `requests` for retryable error checking.

Update **`libs/inference/src/inference/__init__.py`** to export:
```python
from inference.config import AgentConfig
from inference.model_garden import ModelGardenChatModel, create_model_loader
```

Update **`libs/inference/pyproject.toml`** to add required dependencies:
- Add `"google-cloud-aiplatform>=1.38.0"` to dependencies
- Add `"requests>=2.28.0"` to dependencies (for retryable error type checking)
- Keep all existing dependencies

Update **`.env.example`** to add the Vertex AI configuration variables that are now used:
```
# Vertex AI Model Garden (required for MedGemma agentic grounding)
MODEL_BACKEND=vertex
GCP_PROJECT_ID=
GCP_REGION=europe-west4
VERTEX_ENDPOINT_ID=
```
Add these AFTER the existing UMLS section, BEFORE the GCloud section. Do NOT remove any existing content.
  </action>
  <verify>
1. Run `cd /Users/noahdolevelixir/Code/medgemma-hackathon && uv run python -c "from inference.config import AgentConfig; from inference.model_garden import ModelGardenChatModel, create_model_loader; print('imports OK')"` — should print "imports OK"
2. Run `cd /Users/noahdolevelixir/Code/medgemma-hackathon && uv run python -c "from inference import AgentConfig, create_model_loader; print('public API OK')"` — should print "public API OK"
3. Run `cd /Users/noahdolevelixir/Code/medgemma-hackathon && uv run ruff check libs/inference/src/inference/config.py libs/inference/src/inference/model_garden.py` — should pass clean
  </verify>
  <done>ModelGardenChatModel is importable from inference.model_garden, create_model_loader returns a lazy loader for Vertex endpoints, .env.example documents required variables, ruff passes clean.</done>
</task>

</tasks>

<verification>
1. `uv run python -c "from inference.config import AgentConfig; c = AgentConfig.from_env(); print(c)"` succeeds
2. `uv run python -c "from inference.model_garden import ModelGardenChatModel, create_model_loader, _build_gemma_prompt; print('all exports available')"` succeeds
3. `uv run python -c "from inference import AgentConfig, ModelGardenChatModel, create_model_loader; print('public API complete')"` succeeds
4. `uv run ruff check libs/inference/` passes clean
5. AgentConfig.from_env() reads VERTEX_ENDPOINT_ID from .env correctly
6. AgentConfig.supports_tools returns False for endpoint-based config (no vertex_model_name)
</verification>

<success_criteria>
- AgentConfig dataclass with from_env() and supports_tools property exists in libs/inference/src/inference/config.py
- ModelGardenChatModel wraps Vertex AI Model Garden endpoints with Gemma prompt formatting and retry logic in libs/inference/src/inference/model_garden.py
- create_model_loader() returns lazy loader that produces ModelGardenChatModel for vertex backend
- All code passes ruff check
- .env.example documents MODEL_BACKEND, GCP_PROJECT_ID, GCP_REGION, VERTEX_ENDPOINT_ID
</success_criteria>

<output>
After completion, create `.planning/phases/20-medgemma-agentic-grounding/20-01-SUMMARY.md`
</output>
