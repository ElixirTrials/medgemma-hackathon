---
phase: 31-terminologyrouter-pipeline-consolidation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - services/protocol-processor-service/src/protocol_processor/nodes/ingest.py
  - services/protocol-processor-service/src/protocol_processor/nodes/extract.py
  - services/protocol-processor-service/src/protocol_processor/nodes/parse.py
  - services/protocol-processor-service/src/protocol_processor/schemas/extraction.py
  - services/protocol-processor-service/src/protocol_processor/tools/pdf_parser.py
  - services/protocol-processor-service/src/protocol_processor/tools/gemini_extractor.py
  - services/protocol-processor-service/src/protocol_processor/prompts/system.jinja2
  - services/protocol-processor-service/src/protocol_processor/prompts/user.jinja2
  - services/protocol-processor-service/src/protocol_processor/prompts/__init__.py
autonomous: true

must_haves:
  truths:
    - "Ingest node fetches PDF bytes from GCS/local and populates state"
    - "Extract node calls Gemini with structured output and returns JSON string"
    - "Parse node creates CriteriaBatch and Criteria DB records from extraction JSON"
    - "Independent composite criteria are split into separate criteria during extraction per user decision"
    - "All three nodes use PipelineState (not ExtractionState) and return minimal state updates"
  artifacts:
    - path: "services/protocol-processor-service/src/protocol_processor/nodes/ingest.py"
      provides: "PDF fetching and markdown conversion node"
      contains: "async def ingest_node"
    - path: "services/protocol-processor-service/src/protocol_processor/nodes/extract.py"
      provides: "Gemini structured extraction node"
      contains: "async def extract_node"
    - path: "services/protocol-processor-service/src/protocol_processor/nodes/parse.py"
      provides: "DB persistence and entity preparation node"
      contains: "async def parse_node"
    - path: "services/protocol-processor-service/src/protocol_processor/schemas/extraction.py"
      provides: "Pydantic schemas for Gemini structured output"
      contains: "class ExtractionResult"
    - path: "services/protocol-processor-service/src/protocol_processor/tools/gemini_extractor.py"
      provides: "Gemini API call with structured output"
      contains: "async def extract_criteria_structured"
  key_links:
    - from: "services/protocol-processor-service/src/protocol_processor/nodes/extract.py"
      to: "services/protocol-processor-service/src/protocol_processor/tools/gemini_extractor.py"
      via: "function call"
      pattern: "extract_criteria_structured"
    - from: "services/protocol-processor-service/src/protocol_processor/nodes/parse.py"
      to: "shared.models"
      via: "SQLModel import"
      pattern: "from shared\\.models import"
---

<objective>
Create the first three pipeline nodes (ingest, extract, parse) adapted from existing extraction-service code to work with the new PipelineState TypedDict and tools-based architecture.

Purpose: These nodes handle PDF ingestion, Gemini-based criteria extraction, and DB persistence — the extraction half of the consolidated pipeline. They are adapted from extraction-service but refactored to use the new minimal PipelineState and tools-based pattern (nodes orchestrate, tools contain business logic).
Output: Three working pipeline nodes, extraction Pydantic schemas, PDF parser tool, Gemini extractor tool, and extraction prompts.
</objective>

<execution_context>
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/workflows/execute-plan.md
@/Users/noahdolevelixir/.claude-elixirtrials/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/31-terminologyrouter-pipeline-consolidation/31-CONTEXT.md
@.planning/phases/31-terminologyrouter-pipeline-consolidation/31-RESEARCH.md
@.planning/phases/31-terminologyrouter-pipeline-consolidation/31-01-SUMMARY.md

# Existing code to adapt from:
@services/extraction-service/src/extraction_service/nodes/ingest.py
@services/extraction-service/src/extraction_service/nodes/extract.py
@services/extraction-service/src/extraction_service/nodes/parse.py
@services/extraction-service/src/extraction_service/nodes/queue.py
@services/extraction-service/src/extraction_service/pdf_fetcher.py
@services/extraction-service/src/extraction_service/pdf_parser.py
@services/extraction-service/src/extraction_service/schemas/criteria.py
@services/extraction-service/src/extraction_service/prompts/system.jinja2
@services/extraction-service/src/extraction_service/prompts/user.jinja2
@services/extraction-service/src/extraction_service/state.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create extraction tools (pdf_parser, gemini_extractor) and schemas</name>
  <files>
    services/protocol-processor-service/src/protocol_processor/tools/pdf_parser.py
    services/protocol-processor-service/src/protocol_processor/tools/gemini_extractor.py
    services/protocol-processor-service/src/protocol_processor/schemas/extraction.py
    services/protocol-processor-service/src/protocol_processor/prompts/__init__.py
    services/protocol-processor-service/src/protocol_processor/prompts/system.jinja2
    services/protocol-processor-service/src/protocol_processor/prompts/user.jinja2
  </files>
  <action>
    **Tools-based architecture (Pattern 2 from research):** Nodes orchestrate workflow; tools contain business logic.

    1. Create `tools/pdf_parser.py`:
       - Adapt from `extraction_service/pdf_parser.py` and `extraction_service/pdf_fetcher.py`
       - `async def fetch_pdf_bytes(file_uri: str) -> bytes`: Fetch PDF from GCS or local storage. Reuse existing pdf_fetcher logic (GCS signed URL download or local file read).
       - Keep it a pure function — no state dependency.

    2. Create `schemas/extraction.py` with Pydantic models for Gemini structured output:
       - Adapt from `extraction_service/schemas/criteria.py`
       - `ExtractedCriterion(BaseModel)`: text, criteria_type (inclusion/exclusion), category, temporal_constraint, conditions, numeric_thresholds, assertion_status, confidence, source_section, page_number
       - `ExtractionResult(BaseModel)`: criteria (list[ExtractedCriterion]), protocol_summary (str)
       - These models serve as `response_schema` for Gemini structured output (per 31-RESEARCH Pattern: Gemini Structured Output with Pydantic).

    3. Create `tools/gemini_extractor.py`:
       - Adapt from `extraction_service/nodes/extract.py`
       - `async def extract_criteria_structured(pdf_bytes: bytes, protocol_id: str, title: str) -> str`: Upload PDF to Gemini File API, call Gemini with structured output (`response_schema=ExtractionResult`), return JSON string (not dict — per minimal state design).
       - Use existing system.jinja2 and user.jinja2 prompts (copy from extraction-service with enhancement).
       - Per user decision: "Independent composite criteria split into separate criteria during extraction" — add instruction to system prompt: "If a criterion contains independent conditions (e.g., 'Age 18-65 AND BMI <30'), split into separate criteria entries. Dependent conditions (one implies the other) remain as a single criterion."

    4. Copy and adapt prompts from extraction-service:
       - `prompts/system.jinja2`: Copy existing, add composite criteria splitting instruction and AutoCriteria decomposition guidance (Entity, Operator, Value, Unit, Time per CONTEXT.md specifics).
       - `prompts/user.jinja2`: Copy existing.
       - `prompts/__init__.py`: Prompt loading utility (Jinja2 Environment pointing to prompts dir).
  </action>
  <verify>
    - `uv run python -c "from protocol_processor.schemas.extraction import ExtractionResult; print(ExtractionResult.model_json_schema())"` — prints valid JSON schema
    - `uv run python -c "from protocol_processor.tools.gemini_extractor import extract_criteria_structured; print('OK')"` — imports successfully
    - `uv run ruff check services/protocol-processor-service/` passes
  </verify>
  <done>
    Extraction tools and schemas created. PDF parser handles GCS/local fetch. Gemini extractor returns JSON string via structured output. Prompts include composite criteria splitting instruction.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ingest, extract, and parse pipeline nodes using PipelineState</name>
  <files>
    services/protocol-processor-service/src/protocol_processor/nodes/ingest.py
    services/protocol-processor-service/src/protocol_processor/nodes/extract.py
    services/protocol-processor-service/src/protocol_processor/nodes/parse.py
  </files>
  <action>
    Adapt the 3 extraction-service nodes to use PipelineState and delegate to tools. Each node is thin orchestration — business logic lives in tools.

    1. Create `nodes/ingest.py`:
       - `async def ingest_node(state: PipelineState) -> dict`: Call `fetch_pdf_bytes(state["file_uri"])` to get PDF. Return `{"pdf_bytes": pdf_bytes, "status": "processing"}`. On error, return `{"error": "...", "status": "failed"}`.
       - Node does NOT convert to markdown (the Gemini multimodal approach sends PDF bytes directly per Phase 16 architecture). Only fetch bytes.

    2. Create `nodes/extract.py`:
       - `async def extract_node(state: PipelineState) -> dict`: Call `extract_criteria_structured(state["pdf_bytes"], state["protocol_id"], state["title"])`. Return `{"extraction_json": result_json}`. On error, return `{"error": "..."}`.
       - This node orchestrates the tool call. No LLM logic in the node itself.

    3. Create `nodes/parse.py`:
       - `async def parse_node(state: PipelineState) -> dict`:
         - Parse `state["extraction_json"]` into `ExtractionResult` Pydantic model
         - Create CriteriaBatch and Criteria DB records (adapt from extraction-service queue.py)
         - Collect criteria IDs and entity-relevant data
         - Build `entities_json` — a JSON string with list of `{"criterion_id": str, "text": str, "criteria_type": str, "category": str | None}` for the ground node to process
         - Return `{"batch_id": batch.id, "entities_json": entities_json, "pdf_bytes": None}` (clear pdf_bytes from state after use to reduce state size per research Pattern 1)
         - Update protocol status to "processing" (not "extracted" — since grounding happens in same pipeline now)
       - **IMPORTANT:** This node does NOT publish CriteriaExtracted outbox event (per PIPE-03: criteria_extracted outbox removed). The pipeline continues directly to ground node.
       - Import from `api_service.storage`, `shared.models`, `events_py` as needed (same cross-service import pattern as existing queue.py).
  </action>
  <verify>
    - `uv run python -c "from protocol_processor.nodes.ingest import ingest_node; print('OK')"` succeeds
    - `uv run python -c "from protocol_processor.nodes.extract import extract_node; print('OK')"` succeeds
    - `uv run python -c "from protocol_processor.nodes.parse import parse_node; print('OK')"` succeeds
    - `uv run ruff check services/protocol-processor-service/` passes
    - Verify parse_node does NOT import `DomainEventKind.CRITERIA_EXTRACTED` or call `persist_with_outbox` with criteria_extracted
  </verify>
  <done>
    Three pipeline nodes created using PipelineState. Nodes delegate to tools for business logic. Parse node creates DB records without publishing CriteriaExtracted outbox event. State size minimized by clearing pdf_bytes after extraction.
  </done>
</task>

</tasks>

<verification>
1. All three nodes import successfully and use PipelineState (not ExtractionState)
2. `uv run ruff check services/protocol-processor-service/` — clean
3. Parse node does NOT publish CriteriaExtracted outbox event
4. Extraction prompts include composite criteria splitting instruction
5. Tools are pure functions with clear inputs/outputs (no state dependency)
</verification>

<success_criteria>
- Ingest, extract, and parse nodes exist and are importable
- Nodes use PipelineState TypedDict (not old ExtractionState)
- Gemini extractor returns JSON string (not dict) for minimal state
- Parse node persists to DB without CriteriaExtracted outbox event
- PDF bytes cleared from state after extraction (state size optimization)
</success_criteria>

<output>
After completion, create `.planning/phases/31-terminologyrouter-pipeline-consolidation/31-02-SUMMARY.md`
</output>
