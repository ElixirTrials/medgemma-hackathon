{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ElixirTrials \u00b6 AI-powered extraction and structuring of clinical trial eligibility criteria from protocol PDFs. What is ElixirTrials? \u00b6 ElixirTrials takes a clinical trial protocol PDF and produces structured, coded eligibility criteria ready for cohort identification. The system combines: Gemini-powered extraction \u2014 LLM reads the PDF and extracts inclusion/exclusion criteria Multi-terminology grounding \u2014 entities are linked to SNOMED, LOINC, RxNorm, ICD-10 via UMLS + OMOP Expression tree structuring \u2014 criteria are decomposed into atomic conditions with AND/OR/NOT logic Human-in-the-loop review \u2014 clinicians review, approve, or modify AI-generated criteria Standard exports \u2014 output in OHDSI CIRCE, FHIR R4 Group, and OMOP CDM evaluation SQL Quick Start \u00b6 # 1. Clone and configure cp .env.example .env # Fill in GOOGLE_API_KEY, UMLS_API_KEY, etc. # 2. Start everything make run-dev # Starts DB + MLflow + API + UI # 3. Open the UI open http://localhost:3000 Architecture at a Glance \u00b6 graph LR UI[hitl-ui<br/>React + Vite] -->|HTTP| API[api-service<br/>FastAPI] API -->|outbox event| PP[protocol-processor-service<br/>LangGraph] API --> DB[(PostgreSQL)] PP --> DB PP -->|Gemini API| LLM[Gemini 2.5 Flash] PP -->|UMLS + OMOP| VOCAB[Terminology Services] API --> MLflow[MLflow] Documentation Map \u00b6 Section What you'll find Onboarding First-time setup, prerequisites, environment configuration Architecture System design, data models, service boundaries User Journeys End-to-end workflows with sequence diagrams Components Per-service deep dives with file paths Diagrams Visual reference for HITL, agent, and pipeline flows Code Tour Slide-style walkthrough of key code paths Status What's production-ready vs experimental Guides Testing, auth setup, development guides","title":"Home"},{"location":"#elixirtrials","text":"AI-powered extraction and structuring of clinical trial eligibility criteria from protocol PDFs.","title":"ElixirTrials"},{"location":"#what-is-elixirtrials","text":"ElixirTrials takes a clinical trial protocol PDF and produces structured, coded eligibility criteria ready for cohort identification. The system combines: Gemini-powered extraction \u2014 LLM reads the PDF and extracts inclusion/exclusion criteria Multi-terminology grounding \u2014 entities are linked to SNOMED, LOINC, RxNorm, ICD-10 via UMLS + OMOP Expression tree structuring \u2014 criteria are decomposed into atomic conditions with AND/OR/NOT logic Human-in-the-loop review \u2014 clinicians review, approve, or modify AI-generated criteria Standard exports \u2014 output in OHDSI CIRCE, FHIR R4 Group, and OMOP CDM evaluation SQL","title":"What is ElixirTrials?"},{"location":"#quick-start","text":"# 1. Clone and configure cp .env.example .env # Fill in GOOGLE_API_KEY, UMLS_API_KEY, etc. # 2. Start everything make run-dev # Starts DB + MLflow + API + UI # 3. Open the UI open http://localhost:3000","title":"Quick Start"},{"location":"#architecture-at-a-glance","text":"graph LR UI[hitl-ui<br/>React + Vite] -->|HTTP| API[api-service<br/>FastAPI] API -->|outbox event| PP[protocol-processor-service<br/>LangGraph] API --> DB[(PostgreSQL)] PP --> DB PP -->|Gemini API| LLM[Gemini 2.5 Flash] PP -->|UMLS + OMOP| VOCAB[Terminology Services] API --> MLflow[MLflow]","title":"Architecture at a Glance"},{"location":"#documentation-map","text":"Section What you'll find Onboarding First-time setup, prerequisites, environment configuration Architecture System design, data models, service boundaries User Journeys End-to-end workflows with sequence diagrams Components Per-service deep dives with file paths Diagrams Visual reference for HITL, agent, and pipeline flows Code Tour Slide-style walkthrough of key code paths Status What's production-ready vs experimental Guides Testing, auth setup, development guides","title":"Documentation Map"},{"location":"jinja2-prompts/","text":"Jinja2 Prompts Reference \u00b6 This document lists the current Jinja2 templates in the repository and where they are used. Last verified against code and template files: 2026-02-20. Current Template Inventory \u00b6 Area Template Purpose Protocol Processor system.jinja2 System prompt for structured criteria extraction from protocol PDFs Protocol Processor user.jinja2 User prompt for extraction request, including protocol title context Protocol Processor entity_decompose.jinja2 Prompt to split a criterion sentence into discrete groundable entities Protocol Processor grounding_system.jinja2 System prompt for MedGemma terminology selection Protocol Processor grounding_evaluate.jinja2 User prompt with terminology candidates for entity-level grounding Protocol Processor grounding_reasoning.jinja2 User prompt for retry-time agentic reasoning (skip/derive/rephrase) Shared ( libs/shared ) placeholder_system.j2 Placeholder template; currently not used by runtime services Protocol Processor Prompts \u00b6 All protocol-processor templates live in services/protocol-processor-service/src/protocol_processor/prompts/ . system.jinja2 \u00b6 Used by: protocol_processor.tools.gemini_extractor.extract_criteria_structured() Render path: inference.factory.render_prompts(...) with system_template=\"system.jinja2\" Variables: none Role: Defines extraction rules (criteria splitting, numeric thresholds, assertion status, page number guidance) user.jinja2 \u00b6 Used by: protocol_processor.tools.gemini_extractor.extract_criteria_structured() Render path: inference.factory.render_prompts(...) with user_template=\"user.jinja2\" Variables: title Role: Provides protocol-specific extraction instructions for the attached PDF entity_decompose.jinja2 \u00b6 Used by: protocol_processor.tools.entity_decomposer.decompose_entities_from_criterion() Render path: protocol_processor.tools.entity_decomposer._render_decompose_prompt() Variables: criterion_text , category Role: Extracts one or more discrete medical entities from a criterion for downstream terminology routing grounding_system.jinja2 \u00b6 Used by: protocol_processor.tools.medgemma_decider.medgemma_decide() , protocol_processor.tools.medgemma_decider.agentic_reasoning_loop() Render path: protocol_processor.tools.medgemma_decider._render_template() Variables: none Role: Sets MedGemma behavior for terminology grounding decisions grounding_evaluate.jinja2 \u00b6 Used by: protocol_processor.tools.medgemma_decider.medgemma_decide() Render path: protocol_processor.tools.medgemma_decider._render_template() Variables: entity_text , entity_type , criterion_context , candidates Role: Presents candidate concepts and asks MedGemma to select best match with confidence/reasoning grounding_reasoning.jinja2 \u00b6 Used by: protocol_processor.tools.medgemma_decider.agentic_reasoning_loop() Render path: protocol_processor.tools.medgemma_decider._render_template() Variables: entity_text , entity_type , criterion_context , previous_query , attempt Role: Runs 3-question retry reasoning (valid criterion, derived concept, better query phrase) Shared Template \u00b6 placeholder_system.j2 \u00b6 Path: libs/shared/src/shared/templates/placeholder_system.j2 Used by: currently unused (placeholder only) Variables: accepts prompt vars but template body does not consume them","title":"Jinja2 Prompts Reference"},{"location":"jinja2-prompts/#jinja2-prompts-reference","text":"This document lists the current Jinja2 templates in the repository and where they are used. Last verified against code and template files: 2026-02-20.","title":"Jinja2 Prompts Reference"},{"location":"jinja2-prompts/#current-template-inventory","text":"Area Template Purpose Protocol Processor system.jinja2 System prompt for structured criteria extraction from protocol PDFs Protocol Processor user.jinja2 User prompt for extraction request, including protocol title context Protocol Processor entity_decompose.jinja2 Prompt to split a criterion sentence into discrete groundable entities Protocol Processor grounding_system.jinja2 System prompt for MedGemma terminology selection Protocol Processor grounding_evaluate.jinja2 User prompt with terminology candidates for entity-level grounding Protocol Processor grounding_reasoning.jinja2 User prompt for retry-time agentic reasoning (skip/derive/rephrase) Shared ( libs/shared ) placeholder_system.j2 Placeholder template; currently not used by runtime services","title":"Current Template Inventory"},{"location":"jinja2-prompts/#protocol-processor-prompts","text":"All protocol-processor templates live in services/protocol-processor-service/src/protocol_processor/prompts/ .","title":"Protocol Processor Prompts"},{"location":"jinja2-prompts/#systemjinja2","text":"Used by: protocol_processor.tools.gemini_extractor.extract_criteria_structured() Render path: inference.factory.render_prompts(...) with system_template=\"system.jinja2\" Variables: none Role: Defines extraction rules (criteria splitting, numeric thresholds, assertion status, page number guidance)","title":"system.jinja2"},{"location":"jinja2-prompts/#userjinja2","text":"Used by: protocol_processor.tools.gemini_extractor.extract_criteria_structured() Render path: inference.factory.render_prompts(...) with user_template=\"user.jinja2\" Variables: title Role: Provides protocol-specific extraction instructions for the attached PDF","title":"user.jinja2"},{"location":"jinja2-prompts/#entity_decomposejinja2","text":"Used by: protocol_processor.tools.entity_decomposer.decompose_entities_from_criterion() Render path: protocol_processor.tools.entity_decomposer._render_decompose_prompt() Variables: criterion_text , category Role: Extracts one or more discrete medical entities from a criterion for downstream terminology routing","title":"entity_decompose.jinja2"},{"location":"jinja2-prompts/#grounding_systemjinja2","text":"Used by: protocol_processor.tools.medgemma_decider.medgemma_decide() , protocol_processor.tools.medgemma_decider.agentic_reasoning_loop() Render path: protocol_processor.tools.medgemma_decider._render_template() Variables: none Role: Sets MedGemma behavior for terminology grounding decisions","title":"grounding_system.jinja2"},{"location":"jinja2-prompts/#grounding_evaluatejinja2","text":"Used by: protocol_processor.tools.medgemma_decider.medgemma_decide() Render path: protocol_processor.tools.medgemma_decider._render_template() Variables: entity_text , entity_type , criterion_context , candidates Role: Presents candidate concepts and asks MedGemma to select best match with confidence/reasoning","title":"grounding_evaluate.jinja2"},{"location":"jinja2-prompts/#grounding_reasoningjinja2","text":"Used by: protocol_processor.tools.medgemma_decider.agentic_reasoning_loop() Render path: protocol_processor.tools.medgemma_decider._render_template() Variables: entity_text , entity_type , criterion_context , previous_query , attempt Role: Runs 3-question retry reasoning (valid criterion, derived concept, better query phrase)","title":"grounding_reasoning.jinja2"},{"location":"jinja2-prompts/#shared-template","text":"","title":"Shared Template"},{"location":"jinja2-prompts/#placeholder_systemj2","text":"Path: libs/shared/src/shared/templates/placeholder_system.j2 Used by: currently unused (placeholder only) Variables: accepts prompt vars but template body does not consume them","title":"placeholder_system.j2"},{"location":"onboarding/","text":"Onboarding \u00b6 Get ElixirTrials running locally in under 10 minutes. Prerequisites \u00b6 Tool Version Purpose Python 3.12+ Backend services uv latest Python package manager (replaces pip/venv) Node.js 18+ Frontend UI Docker latest PostgreSQL database gcloud CLI latest GCP authentication (optional for local dev) 1. Clone and Install \u00b6 git clone https://github.com/noahdolevelixir/medgemma-hackathon.git cd medgemma-hackathon uv sync # Install all Python dependencies cd apps/hitl-ui && npm install && cd ../.. 2. Environment Configuration \u00b6 cp .env.example .env Edit .env with your credentials: Variable Required Source GOOGLE_API_KEY Yes Google AI Studio UMLS_API_KEY Yes UMLS Sign Up GEMINI_MODEL_NAME No Defaults to gemini-2.5-flash MODEL_BACKEND No vertex for Vertex AI, omit for Gemini Developer API GCP_PROJECT_ID If Vertex Your GCP project ID GCP_REGION If Vertex e.g., europe-west4 VERTEX_ENDPOINT_ID If Vertex MedGemma endpoint ID GOOGLE_CLIENT_ID No OAuth login (dev works without) GOOGLE_CLIENT_SECRET No OAuth login (dev works without) MLFLOW_TRACKING_URI No Defaults to http://localhost:5001 3. Start the Dev Stack \u00b6 make run-dev This starts: PostgreSQL on port 5432 (Docker) MLflow on port 5001 (local) API service on port 8000 (uvicorn with hot-reload) UI on port 3000 (Vite dev server) Alternatively, start components individually: make run-infra # DB + MLflow only make run-api # API service only make run-ui # UI only 4. Verify \u00b6 Open http://localhost:3000 \u2014 you should see the HITL UI Check http://localhost:8000/health \u2014 should return {\"status\": \"healthy\"} Check http://localhost:5001 \u2014 MLflow dashboard 5. Upload a Protocol \u00b6 Click Upload Protocol in the UI Drop a clinical trial protocol PDF (max 50 MB) The pipeline runs automatically: extract \u2192 parse \u2192 ground \u2192 structure Review extracted criteria in the HITL review page Project Structure \u00b6 medgemma-hackathon/ \u251c\u2500\u2500 apps/hitl-ui/ # React + Vite frontend \u251c\u2500\u2500 services/ \u2502 \u251c\u2500\u2500 api-service/ # FastAPI backend (REST + outbox) \u2502 \u2514\u2500\u2500 protocol-processor-service/ # LangGraph pipeline \u251c\u2500\u2500 libs/ \u2502 \u251c\u2500\u2500 shared/ # SQLModel data models \u2502 \u251c\u2500\u2500 events-py/ # Outbox processor + event contracts \u2502 \u251c\u2500\u2500 inference/ # Model inference utilities \u2502 \u251c\u2500\u2500 evaluation/ # Quality evaluation \u2502 \u251c\u2500\u2500 data-pipeline/ # Data loading utilities \u2502 \u2514\u2500\u2500 model-training/ # Fine-tuning scripts \u251c\u2500\u2500 infra/ # Docker Compose, OMOP vocab \u251c\u2500\u2500 scripts/ # Dev tooling, migrations \u251c\u2500\u2500 docs/ # This documentation \u251c\u2500\u2500 Makefile # All dev commands \u2514\u2500\u2500 mkdocs.yml # Docs site config Common Commands \u00b6 make help # Show all available commands make check # Run linters + type checkers + tests make lint-fix # Auto-fix lint issues make test # Run all tests make docs-build # Build documentation site make docs-serve # Serve docs at http://localhost:8000","title":"Onboarding"},{"location":"onboarding/#onboarding","text":"Get ElixirTrials running locally in under 10 minutes.","title":"Onboarding"},{"location":"onboarding/#prerequisites","text":"Tool Version Purpose Python 3.12+ Backend services uv latest Python package manager (replaces pip/venv) Node.js 18+ Frontend UI Docker latest PostgreSQL database gcloud CLI latest GCP authentication (optional for local dev)","title":"Prerequisites"},{"location":"onboarding/#1-clone-and-install","text":"git clone https://github.com/noahdolevelixir/medgemma-hackathon.git cd medgemma-hackathon uv sync # Install all Python dependencies cd apps/hitl-ui && npm install && cd ../..","title":"1. Clone and Install"},{"location":"onboarding/#2-environment-configuration","text":"cp .env.example .env Edit .env with your credentials: Variable Required Source GOOGLE_API_KEY Yes Google AI Studio UMLS_API_KEY Yes UMLS Sign Up GEMINI_MODEL_NAME No Defaults to gemini-2.5-flash MODEL_BACKEND No vertex for Vertex AI, omit for Gemini Developer API GCP_PROJECT_ID If Vertex Your GCP project ID GCP_REGION If Vertex e.g., europe-west4 VERTEX_ENDPOINT_ID If Vertex MedGemma endpoint ID GOOGLE_CLIENT_ID No OAuth login (dev works without) GOOGLE_CLIENT_SECRET No OAuth login (dev works without) MLFLOW_TRACKING_URI No Defaults to http://localhost:5001","title":"2. Environment Configuration"},{"location":"onboarding/#3-start-the-dev-stack","text":"make run-dev This starts: PostgreSQL on port 5432 (Docker) MLflow on port 5001 (local) API service on port 8000 (uvicorn with hot-reload) UI on port 3000 (Vite dev server) Alternatively, start components individually: make run-infra # DB + MLflow only make run-api # API service only make run-ui # UI only","title":"3. Start the Dev Stack"},{"location":"onboarding/#4-verify","text":"Open http://localhost:3000 \u2014 you should see the HITL UI Check http://localhost:8000/health \u2014 should return {\"status\": \"healthy\"} Check http://localhost:5001 \u2014 MLflow dashboard","title":"4. Verify"},{"location":"onboarding/#5-upload-a-protocol","text":"Click Upload Protocol in the UI Drop a clinical trial protocol PDF (max 50 MB) The pipeline runs automatically: extract \u2192 parse \u2192 ground \u2192 structure Review extracted criteria in the HITL review page","title":"5. Upload a Protocol"},{"location":"onboarding/#project-structure","text":"medgemma-hackathon/ \u251c\u2500\u2500 apps/hitl-ui/ # React + Vite frontend \u251c\u2500\u2500 services/ \u2502 \u251c\u2500\u2500 api-service/ # FastAPI backend (REST + outbox) \u2502 \u2514\u2500\u2500 protocol-processor-service/ # LangGraph pipeline \u251c\u2500\u2500 libs/ \u2502 \u251c\u2500\u2500 shared/ # SQLModel data models \u2502 \u251c\u2500\u2500 events-py/ # Outbox processor + event contracts \u2502 \u251c\u2500\u2500 inference/ # Model inference utilities \u2502 \u251c\u2500\u2500 evaluation/ # Quality evaluation \u2502 \u251c\u2500\u2500 data-pipeline/ # Data loading utilities \u2502 \u2514\u2500\u2500 model-training/ # Fine-tuning scripts \u251c\u2500\u2500 infra/ # Docker Compose, OMOP vocab \u251c\u2500\u2500 scripts/ # Dev tooling, migrations \u251c\u2500\u2500 docs/ # This documentation \u251c\u2500\u2500 Makefile # All dev commands \u2514\u2500\u2500 mkdocs.yml # Docs site config","title":"Project Structure"},{"location":"onboarding/#common-commands","text":"make help # Show all available commands make check # Run linters + type checkers + tests make lint-fix # Auto-fix lint issues make test # Run all tests make docs-build # Build documentation site make docs-serve # Serve docs at http://localhost:8000","title":"Common Commands"},{"location":"testing-guide/","text":"Testing Guide \u00b6 Running Tests \u00b6 # All tests (parallel by default with pytest-xdist) make test # API service tests only uv run pytest services/api-service/tests -q # Specific test file uv run pytest services/api-service/tests/test_exports.py -q # With coverage uv run pytest --cov services/api-service/tests # Disable parallel execution (useful for debugging) uv run pytest services/api-service/tests --override-ini = \"addopts=\" -q Test Structure \u00b6 services/api-service/tests/ \u251c\u2500\u2500 conftest.py # Shared fixtures (in-memory DB, test client) \u251c\u2500\u2500 test_auth_required.py # Authentication middleware \u251c\u2500\u2500 test_exports.py # CIRCE, FHIR, SQL export builders (36 tests) \u251c\u2500\u2500 test_integrity.py # Data integrity checks \u251c\u2500\u2500 test_models.py # SQLModel validation \u251c\u2500\u2500 test_protocol_api.py # Protocol upload/list/detail endpoints \u251c\u2500\u2500 test_quality.py # PDF quality scoring \u251c\u2500\u2500 test_review_api.py # Review workflow endpoints \u251c\u2500\u2500 test_schemas.py # Pydantic schema validation \u2514\u2500\u2500 test_umls_clients.py # UMLS API client mocking libs/events-py/tests/ \u2514\u2500\u2500 test_outbox.py # Outbox processor tests Test Fixtures \u00b6 The conftest.py provides: In-memory SQLite database \u2014 tests don't need PostgreSQL running SQLModel metadata creation \u2014 all tables created per test session FastAPI TestClient \u2014 for HTTP endpoint testing Mock authentication \u2014 bypasses OAuth for test requests Writing New Tests \u00b6 API Endpoint Test \u00b6 def test_list_protocols ( client , db_session ): \"\"\"Test paginated protocol listing.\"\"\" # Arrange: seed a protocol protocol = Protocol ( title = \"Test\" , file_uri = \"local://test.pdf\" , status = \"uploaded\" ) db_session . add ( protocol ) db_session . commit () # Act response = client . get ( \"/protocols\" ) # Assert assert response . status_code == 200 data = response . json () assert data [ \"total\" ] == 1 Export Builder Test \u00b6 def test_circe_includes_age_filter ( make_export_data , make_atomic ): \"\"\"Test that demographics criteria produce CIRCE age filters.\"\"\" atomic = make_atomic ( entity_domain = \"demographics\" , relation_operator = \">=\" , value_numeric = 18 ) data = make_export_data ( atomics = [ atomic ]) result = build_circe_export ( data ) assert any ( \"Age\" in str ( g ) for g in result . get ( \"InclusionRules\" , [])) Code Quality \u00b6 make check # Run all: lint + typecheck + test make lint # ruff (Python) + Biome (TypeScript) make lint-fix # Auto-fix lint issues make typecheck # mypy (Python) + tsc (TypeScript) Linting Configuration \u00b6 Python : ruff (configured in pyproject.toml ) TypeScript : Biome (configured in apps/hitl-ui/biome.json ) Type checking : mypy for Python, tsc for TypeScript Pre-existing mypy Issues \u00b6 The following mypy errors are pre-existing and non-critical: libs/events-py import stubs (3 errors) These do not affect runtime behavior","title":"Testing Guide"},{"location":"testing-guide/#testing-guide","text":"","title":"Testing Guide"},{"location":"testing-guide/#running-tests","text":"# All tests (parallel by default with pytest-xdist) make test # API service tests only uv run pytest services/api-service/tests -q # Specific test file uv run pytest services/api-service/tests/test_exports.py -q # With coverage uv run pytest --cov services/api-service/tests # Disable parallel execution (useful for debugging) uv run pytest services/api-service/tests --override-ini = \"addopts=\" -q","title":"Running Tests"},{"location":"testing-guide/#test-structure","text":"services/api-service/tests/ \u251c\u2500\u2500 conftest.py # Shared fixtures (in-memory DB, test client) \u251c\u2500\u2500 test_auth_required.py # Authentication middleware \u251c\u2500\u2500 test_exports.py # CIRCE, FHIR, SQL export builders (36 tests) \u251c\u2500\u2500 test_integrity.py # Data integrity checks \u251c\u2500\u2500 test_models.py # SQLModel validation \u251c\u2500\u2500 test_protocol_api.py # Protocol upload/list/detail endpoints \u251c\u2500\u2500 test_quality.py # PDF quality scoring \u251c\u2500\u2500 test_review_api.py # Review workflow endpoints \u251c\u2500\u2500 test_schemas.py # Pydantic schema validation \u2514\u2500\u2500 test_umls_clients.py # UMLS API client mocking libs/events-py/tests/ \u2514\u2500\u2500 test_outbox.py # Outbox processor tests","title":"Test Structure"},{"location":"testing-guide/#test-fixtures","text":"The conftest.py provides: In-memory SQLite database \u2014 tests don't need PostgreSQL running SQLModel metadata creation \u2014 all tables created per test session FastAPI TestClient \u2014 for HTTP endpoint testing Mock authentication \u2014 bypasses OAuth for test requests","title":"Test Fixtures"},{"location":"testing-guide/#writing-new-tests","text":"","title":"Writing New Tests"},{"location":"testing-guide/#api-endpoint-test","text":"def test_list_protocols ( client , db_session ): \"\"\"Test paginated protocol listing.\"\"\" # Arrange: seed a protocol protocol = Protocol ( title = \"Test\" , file_uri = \"local://test.pdf\" , status = \"uploaded\" ) db_session . add ( protocol ) db_session . commit () # Act response = client . get ( \"/protocols\" ) # Assert assert response . status_code == 200 data = response . json () assert data [ \"total\" ] == 1","title":"API Endpoint Test"},{"location":"testing-guide/#export-builder-test","text":"def test_circe_includes_age_filter ( make_export_data , make_atomic ): \"\"\"Test that demographics criteria produce CIRCE age filters.\"\"\" atomic = make_atomic ( entity_domain = \"demographics\" , relation_operator = \">=\" , value_numeric = 18 ) data = make_export_data ( atomics = [ atomic ]) result = build_circe_export ( data ) assert any ( \"Age\" in str ( g ) for g in result . get ( \"InclusionRules\" , []))","title":"Export Builder Test"},{"location":"testing-guide/#code-quality","text":"make check # Run all: lint + typecheck + test make lint # ruff (Python) + Biome (TypeScript) make lint-fix # Auto-fix lint issues make typecheck # mypy (Python) + tsc (TypeScript)","title":"Code Quality"},{"location":"testing-guide/#linting-configuration","text":"Python : ruff (configured in pyproject.toml ) TypeScript : Biome (configured in apps/hitl-ui/biome.json ) Type checking : mypy for Python, tsc for TypeScript","title":"Linting Configuration"},{"location":"testing-guide/#pre-existing-mypy-issues","text":"The following mypy errors are pre-existing and non-critical: libs/events-py import stubs (3 errors) These do not affect runtime behavior","title":"Pre-existing mypy Issues"},{"location":"api-service/api/","text":"API Reference for api-service \u00b6","title":"API Reference"},{"location":"api-service/api/#api-reference-for-api-service","text":"","title":"API Reference for api-service"},{"location":"architecture/","text":"Architecture Overview \u00b6 ElixirTrials is a monorepo containing three runtime components and several shared libraries, all orchestrated through an event-driven pipeline. Core Principles \u00b6 Consolidated pipeline \u2014 All processing (extraction, grounding, structuring) runs in a single protocol-processor-service via LangGraph. The earlier two-service split ( extraction-service + grounding-service ) is legacy. Event-driven trigger \u2014 Processing starts from a protocol_uploaded outbox event. There is no criteria_extracted cross-service event. Error accumulation \u2014 Individual entity/criterion failures don't crash the pipeline. Partial results are preserved alongside errors. Human-in-the-loop \u2014 All AI outputs go through a review step before being considered final. Runtime Components \u00b6 Component Tech Port Role hitl-ui React + Vite + Radix 3000 Upload PDFs, review criteria, manage protocols api-service FastAPI + SQLModel 8000 REST API, outbox processor, export endpoints protocol-processor-service LangGraph + Gemini (in-process) 7-node extraction/grounding/structuring pipeline Shared Libraries \u00b6 Library Purpose libs/shared SQLModel domain models ( Protocol , Criteria , Entity , etc.) libs/events-py Transactional outbox processor and domain event contracts libs/inference Model inference utilities libs/evaluation Quality evaluation framework libs/data-pipeline Data loading helpers libs/model-training Fine-tuning scripts Infrastructure \u00b6 Service Tech Purpose PostgreSQL 16-alpine Application database + LangGraph checkpoints MLflow v3.9.0 Experiment tracking, trace observability OMOP Vocab DB PostgreSQL Optional OMOP vocabulary for concept resolution Pub/Sub Emulator GCP emulator Not actively used (outbox pattern preferred) See System Architecture for diagrams and Data Models for the schema.","title":"Overview"},{"location":"architecture/#architecture-overview","text":"ElixirTrials is a monorepo containing three runtime components and several shared libraries, all orchestrated through an event-driven pipeline.","title":"Architecture Overview"},{"location":"architecture/#core-principles","text":"Consolidated pipeline \u2014 All processing (extraction, grounding, structuring) runs in a single protocol-processor-service via LangGraph. The earlier two-service split ( extraction-service + grounding-service ) is legacy. Event-driven trigger \u2014 Processing starts from a protocol_uploaded outbox event. There is no criteria_extracted cross-service event. Error accumulation \u2014 Individual entity/criterion failures don't crash the pipeline. Partial results are preserved alongside errors. Human-in-the-loop \u2014 All AI outputs go through a review step before being considered final.","title":"Core Principles"},{"location":"architecture/#runtime-components","text":"Component Tech Port Role hitl-ui React + Vite + Radix 3000 Upload PDFs, review criteria, manage protocols api-service FastAPI + SQLModel 8000 REST API, outbox processor, export endpoints protocol-processor-service LangGraph + Gemini (in-process) 7-node extraction/grounding/structuring pipeline","title":"Runtime Components"},{"location":"architecture/#shared-libraries","text":"Library Purpose libs/shared SQLModel domain models ( Protocol , Criteria , Entity , etc.) libs/events-py Transactional outbox processor and domain event contracts libs/inference Model inference utilities libs/evaluation Quality evaluation framework libs/data-pipeline Data loading helpers libs/model-training Fine-tuning scripts","title":"Shared Libraries"},{"location":"architecture/#infrastructure","text":"Service Tech Purpose PostgreSQL 16-alpine Application database + LangGraph checkpoints MLflow v3.9.0 Experiment tracking, trace observability OMOP Vocab DB PostgreSQL Optional OMOP vocabulary for concept resolution Pub/Sub Emulator GCP emulator Not actively used (outbox pattern preferred) See System Architecture for diagrams and Data Models for the schema.","title":"Infrastructure"},{"location":"architecture/data-models/","text":"Data Models \u00b6 All domain models live in libs/shared/src/shared/models.py using SQLModel (SQLAlchemy + Pydantic). Entity-Relationship Overview \u00b6 erDiagram Protocol ||--o{ CriteriaBatch : \"has batches\" CriteriaBatch ||--o{ Criteria : \"contains\" Criteria ||--o{ Entity : \"has entities\" Criteria ||--o{ AtomicCriterion : \"decomposes to\" Criteria ||--o{ CompositeCriterion : \"groups via\" CompositeCriterion ||--o{ CriterionRelationship : \"parent\" Protocol ||--o{ AtomicCriterion : \"belongs to\" Protocol ||--o{ CompositeCriterion : \"belongs to\" Review }o--|| Criteria : \"reviews\" Review }o--|| Entity : \"reviews\" OutboxEvent }o--|| Protocol : \"triggers for\" Core Tables \u00b6 Protocol \u00b6 Uploaded clinical trial protocol PDF. Central aggregate root. Column Type Description id UUID (str) Primary key title str From filename or user input file_uri str GCS URI ( gs://... ) or local path ( local://... ) status str Current processing state (see lifecycle below) page_count int? PDF page count from quality analysis quality_score float? 0-1 score from PDF quality check error_reason str? Human-readable failure message metadata_ JSON Quality details, pipeline_thread_id, error info File : libs/shared/src/shared/models.py:59 Protocol Status Lifecycle \u00b6 stateDiagram-v2 [*] --> uploaded: POST /upload uploaded --> extracting: confirm-upload (outbox) extracting --> extraction_failed: Fatal error extracting --> grounding: Pipeline progress grounding --> grounding_failed: All entities fail grounding --> pending_review: Pipeline success pending_review --> complete: All criteria reviewed extraction_failed --> extracting: retry / re-extract grounding_failed --> extracting: retry / re-extract extraction_failed --> dead_letter: Outbox exhausted (3 retries) dead_letter --> archived: Lazy archival (7+ days) extraction_failed --> archived: Manual archive grounding_failed --> archived: Manual archive dead_letter --> extracting: retry CriteriaBatch \u00b6 A batch of criteria extracted from a protocol. Re-extraction creates a new batch and archives old ones. Column Type Description id UUID (str) Primary key protocol_id FK \u2192 Protocol Parent protocol status str Batch review status extraction_model str? Model used for extraction is_archived bool Hidden from dashboard when True File : libs/shared/src/shared/models.py:74 Criteria \u00b6 Individual inclusion/exclusion criterion extracted from a protocol. Column Type Description id UUID (str) Primary key batch_id FK \u2192 CriteriaBatch Parent batch criteria_type str inclusion or exclusion text text Original criterion text structured_criterion JSON? Expression tree (Phase 2) conditions JSON? Contains field_mappings from grounding confidence float Extraction confidence (0-1) review_status str? Human review decision File : libs/shared/src/shared/models.py:86 Entity \u00b6 Medical entity extracted from a criterion (e.g., \"Type 2 diabetes\", \"HbA1c\"). Column Type Description id UUID (str) Primary key criteria_id FK \u2192 Criteria Parent criterion entity_type str condition , measurement , drug , etc. text str Entity surface text snomed_code str? SNOMED CT code rxnorm_code str? RxNorm code loinc_code str? LOINC code icd10_code str? ICD-10 code omop_concept_id str? OMOP CDM concept ID File : libs/shared/src/shared/models.py:114 AtomicCriterion \u00b6 Leaf node in an expression tree. Represents a single testable condition (e.g., \"HbA1c >= 7%\"). Column Type Description id UUID (str) Primary key criterion_id FK \u2192 Criteria Parent criterion protocol_id FK \u2192 Protocol Owning protocol inclusion_exclusion str inclusion or exclusion entity_domain str? condition , measurement , drug , demographics , etc. omop_concept_id str? OMOP concept ID for CDM joins entity_concept_id str? Source terminology code entity_concept_system str? snomed , loinc , rxnorm relation_operator str? >= , <= , = , > , < , != value_numeric float? Numeric threshold value_text str? Text value (e.g., \"Class III-IV\") unit_text str? Unit label unit_concept_id int? OMOP unit concept ID negation bool True for negated conditions File : libs/shared/src/shared/models.py:184 Table : atomic_criteria CompositeCriterion \u00b6 Branch node in an expression tree. Combines children with AND/OR/NOT logic. Column Type Description id UUID (str) Primary key criterion_id FK \u2192 Criteria Parent criterion protocol_id FK \u2192 Protocol Owning protocol logic_operator str AND , OR , NOT File : libs/shared/src/shared/models.py:235 Table : composite_criteria CriterionRelationship \u00b6 Edge in the expression tree. Links parent composite to child (atomic or composite). Column Type Description parent_criterion_id FK \u2192 CompositeCriterion Parent (PK) child_criterion_id str Child ID (PK) child_type str atomic or composite child_sequence int Operand ordering File : libs/shared/src/shared/models.py:263 Table : criterion_relationships Supporting Tables \u00b6 Table Purpose File line review Human review actions with before/after JSON models.py:144 auditlog Immutable log of all system events models.py:158 user Authenticated user accounts (Google OAuth) models.py:170 outboxevent Transactional outbox for event publishing models.py:285 Pipeline State \u00b6 The LangGraph pipeline passes state between nodes as a PipelineState TypedDict. File : services/protocol-processor-service/src/protocol_processor/state.py Field Type Set by protocol_id str Input (always present) file_uri str Input title str Input batch_id str? parse node pdf_bytes bytes? ingest node (cleared after extract) extraction_json str? extract node entities_json str? parse node grounded_entities_json str? ground node archived_reviewed_criteria list? Input (re-extraction only) ordinal_proposals_json str? ordinal_resolve node status str Each node error str? Fatal errors \u2192 routes to END errors list[str] Accumulated non-fatal errors State is serialized as JSON strings (not dicts) between nodes to minimize LangGraph checkpoint size.","title":"Data Models"},{"location":"architecture/data-models/#data-models","text":"All domain models live in libs/shared/src/shared/models.py using SQLModel (SQLAlchemy + Pydantic).","title":"Data Models"},{"location":"architecture/data-models/#entity-relationship-overview","text":"erDiagram Protocol ||--o{ CriteriaBatch : \"has batches\" CriteriaBatch ||--o{ Criteria : \"contains\" Criteria ||--o{ Entity : \"has entities\" Criteria ||--o{ AtomicCriterion : \"decomposes to\" Criteria ||--o{ CompositeCriterion : \"groups via\" CompositeCriterion ||--o{ CriterionRelationship : \"parent\" Protocol ||--o{ AtomicCriterion : \"belongs to\" Protocol ||--o{ CompositeCriterion : \"belongs to\" Review }o--|| Criteria : \"reviews\" Review }o--|| Entity : \"reviews\" OutboxEvent }o--|| Protocol : \"triggers for\"","title":"Entity-Relationship Overview"},{"location":"architecture/data-models/#core-tables","text":"","title":"Core Tables"},{"location":"architecture/data-models/#protocol","text":"Uploaded clinical trial protocol PDF. Central aggregate root. Column Type Description id UUID (str) Primary key title str From filename or user input file_uri str GCS URI ( gs://... ) or local path ( local://... ) status str Current processing state (see lifecycle below) page_count int? PDF page count from quality analysis quality_score float? 0-1 score from PDF quality check error_reason str? Human-readable failure message metadata_ JSON Quality details, pipeline_thread_id, error info File : libs/shared/src/shared/models.py:59","title":"Protocol"},{"location":"architecture/data-models/#protocol-status-lifecycle","text":"stateDiagram-v2 [*] --> uploaded: POST /upload uploaded --> extracting: confirm-upload (outbox) extracting --> extraction_failed: Fatal error extracting --> grounding: Pipeline progress grounding --> grounding_failed: All entities fail grounding --> pending_review: Pipeline success pending_review --> complete: All criteria reviewed extraction_failed --> extracting: retry / re-extract grounding_failed --> extracting: retry / re-extract extraction_failed --> dead_letter: Outbox exhausted (3 retries) dead_letter --> archived: Lazy archival (7+ days) extraction_failed --> archived: Manual archive grounding_failed --> archived: Manual archive dead_letter --> extracting: retry","title":"Protocol Status Lifecycle"},{"location":"architecture/data-models/#criteriabatch","text":"A batch of criteria extracted from a protocol. Re-extraction creates a new batch and archives old ones. Column Type Description id UUID (str) Primary key protocol_id FK \u2192 Protocol Parent protocol status str Batch review status extraction_model str? Model used for extraction is_archived bool Hidden from dashboard when True File : libs/shared/src/shared/models.py:74","title":"CriteriaBatch"},{"location":"architecture/data-models/#criteria","text":"Individual inclusion/exclusion criterion extracted from a protocol. Column Type Description id UUID (str) Primary key batch_id FK \u2192 CriteriaBatch Parent batch criteria_type str inclusion or exclusion text text Original criterion text structured_criterion JSON? Expression tree (Phase 2) conditions JSON? Contains field_mappings from grounding confidence float Extraction confidence (0-1) review_status str? Human review decision File : libs/shared/src/shared/models.py:86","title":"Criteria"},{"location":"architecture/data-models/#entity","text":"Medical entity extracted from a criterion (e.g., \"Type 2 diabetes\", \"HbA1c\"). Column Type Description id UUID (str) Primary key criteria_id FK \u2192 Criteria Parent criterion entity_type str condition , measurement , drug , etc. text str Entity surface text snomed_code str? SNOMED CT code rxnorm_code str? RxNorm code loinc_code str? LOINC code icd10_code str? ICD-10 code omop_concept_id str? OMOP CDM concept ID File : libs/shared/src/shared/models.py:114","title":"Entity"},{"location":"architecture/data-models/#atomiccriterion","text":"Leaf node in an expression tree. Represents a single testable condition (e.g., \"HbA1c >= 7%\"). Column Type Description id UUID (str) Primary key criterion_id FK \u2192 Criteria Parent criterion protocol_id FK \u2192 Protocol Owning protocol inclusion_exclusion str inclusion or exclusion entity_domain str? condition , measurement , drug , demographics , etc. omop_concept_id str? OMOP concept ID for CDM joins entity_concept_id str? Source terminology code entity_concept_system str? snomed , loinc , rxnorm relation_operator str? >= , <= , = , > , < , != value_numeric float? Numeric threshold value_text str? Text value (e.g., \"Class III-IV\") unit_text str? Unit label unit_concept_id int? OMOP unit concept ID negation bool True for negated conditions File : libs/shared/src/shared/models.py:184 Table : atomic_criteria","title":"AtomicCriterion"},{"location":"architecture/data-models/#compositecriterion","text":"Branch node in an expression tree. Combines children with AND/OR/NOT logic. Column Type Description id UUID (str) Primary key criterion_id FK \u2192 Criteria Parent criterion protocol_id FK \u2192 Protocol Owning protocol logic_operator str AND , OR , NOT File : libs/shared/src/shared/models.py:235 Table : composite_criteria","title":"CompositeCriterion"},{"location":"architecture/data-models/#criterionrelationship","text":"Edge in the expression tree. Links parent composite to child (atomic or composite). Column Type Description parent_criterion_id FK \u2192 CompositeCriterion Parent (PK) child_criterion_id str Child ID (PK) child_type str atomic or composite child_sequence int Operand ordering File : libs/shared/src/shared/models.py:263 Table : criterion_relationships","title":"CriterionRelationship"},{"location":"architecture/data-models/#supporting-tables","text":"Table Purpose File line review Human review actions with before/after JSON models.py:144 auditlog Immutable log of all system events models.py:158 user Authenticated user accounts (Google OAuth) models.py:170 outboxevent Transactional outbox for event publishing models.py:285","title":"Supporting Tables"},{"location":"architecture/data-models/#pipeline-state","text":"The LangGraph pipeline passes state between nodes as a PipelineState TypedDict. File : services/protocol-processor-service/src/protocol_processor/state.py Field Type Set by protocol_id str Input (always present) file_uri str Input title str Input batch_id str? parse node pdf_bytes bytes? ingest node (cleared after extract) extraction_json str? extract node entities_json str? parse node grounded_entities_json str? ground node archived_reviewed_criteria list? Input (re-extraction only) ordinal_proposals_json str? ordinal_resolve node status str Each node error str? Fatal errors \u2192 routes to END errors list[str] Accumulated non-fatal errors State is serialized as JSON strings (not dicts) between nodes to minimize LangGraph checkpoint size.","title":"Pipeline State"},{"location":"architecture/system-architecture/","text":"System Architecture \u00b6 Container Diagram \u00b6 graph TB subgraph \"Browser\" UI[\"hitl-ui<br/>(React + Vite + Radix UI)\"] end subgraph \"Backend\" API[\"api-service<br/>(FastAPI + SQLModel)\"] OBX[\"OutboxProcessor<br/>(events-py)\"] PP[\"protocol-processor-service<br/>(LangGraph StateGraph)\"] end subgraph \"Infrastructure\" DB[(PostgreSQL 16)] MLF[\"MLflow v3.9\"] OMOP[(OMOP Vocab DB<br/>optional)] end subgraph \"External Services\" GEMINI[\"Gemini 2.5 Flash<br/>(Google AI / Vertex AI)\"] UMLS[\"UMLS API<br/>(NLM)\"] GCS[\"GCS / Local Storage\"] end UI -->|\"HTTP REST\"| API API --> DB API --> OBX OBX -->|\"protocol_uploaded\"| PP PP --> DB PP -->|\"PDF extraction\"| GEMINI PP -->|\"Entity grounding\"| UMLS PP -->|\"Concept resolution\"| OMOP PP -->|\"PDF fetch\"| GCS API -->|\"Traces\"| MLF PP -->|\"Traces\"| MLF Communication Paths \u00b6 Synchronous HTTP (UI to API) \u00b6 All UI interactions flow through the FastAPI REST API: Path Description Key file POST /protocols/upload Generate signed upload URL services/api-service/src/api_service/protocols.py POST /protocols/{id}/confirm-upload Confirm upload, trigger pipeline protocols.py GET /protocols List protocols (paginated) protocols.py GET /reviews/batches/{id}/criteria Fetch criteria for review reviews.py POST /reviews/criteria/{id}/action Submit review decision reviews.py GET /exports/{id}/circe Export as CIRCE JSON exports.py GET /exports/{id}/fhir-group Export as FHIR R4 Group exports.py GET /exports/{id}/evaluation-sql Export as OMOP SQL exports.py Asynchronous Event Processing (Outbox to Pipeline) \u00b6 The pipeline is triggered via the transactional outbox pattern: confirm-upload endpoint calls persist_with_outbox() \u2014 writes Protocol record + OutboxEvent in a single DB transaction OutboxProcessor polls the outboxevent table every 2 seconds When it finds a protocol_uploaded event, it calls handle_protocol_uploaded() in a thread executor The handler invokes the 7-node LangGraph pipeline via asyncio.run() sequenceDiagram participant UI as hitl-ui participant API as api-service participant DB as PostgreSQL participant OBX as OutboxProcessor participant PP as protocol-processor UI->>API: POST /protocols/{id}/confirm-upload API->>DB: INSERT Protocol + OutboxEvent (single tx) API-->>UI: 200 OK loop Every 2s OBX->>DB: SELECT FROM outboxevent WHERE status='pending' end OBX->>PP: handle_protocol_uploaded(payload) PP->>DB: UPDATE protocol.status = 'extracting' PP->>PP: Run 7-node LangGraph pipeline PP->>DB: INSERT criteria, entities, atomics PP->>DB: UPDATE protocol.status = 'pending_review' Storage Paths \u00b6 Local dev : Files stored in ./uploads/ directory, served by api-service at /local-files/{path} Production : Files uploaded to GCS via signed URL, fetched by pipeline from gs:// URI Observability \u00b6 MLflow : Traces emitted by both api-service (request middleware) and pipeline nodes (per-node spans) Tracking URI : http://localhost:5001 (local), http://mlflow:5000 (Docker) Experiment : protocol-processing Orphan cleanup : trigger.py closes stale IN_PROGRESS traces at startup Authentication \u00b6 Google OAuth 2.0 via api-service/auth.py Session middleware with JWT tokens All API routes (except /auth/* and /health ) require authentication Dev mode works without OAuth configured Error Handling \u00b6 Error type Behavior Status Fatal (PDF corrupt, auth fail) Pipeline stops at failed node extraction_failed Partial (some entities fail grounding) Pipeline continues, errors accumulated pending_review with errors[] Total grounding failure All entities failed grounding_failed Outbox exhaustion (3 retries) Event marked dead_letter dead_letter Dead letter aging (7+ days) Lazy archival on next access archived","title":"System Architecture"},{"location":"architecture/system-architecture/#system-architecture","text":"","title":"System Architecture"},{"location":"architecture/system-architecture/#container-diagram","text":"graph TB subgraph \"Browser\" UI[\"hitl-ui<br/>(React + Vite + Radix UI)\"] end subgraph \"Backend\" API[\"api-service<br/>(FastAPI + SQLModel)\"] OBX[\"OutboxProcessor<br/>(events-py)\"] PP[\"protocol-processor-service<br/>(LangGraph StateGraph)\"] end subgraph \"Infrastructure\" DB[(PostgreSQL 16)] MLF[\"MLflow v3.9\"] OMOP[(OMOP Vocab DB<br/>optional)] end subgraph \"External Services\" GEMINI[\"Gemini 2.5 Flash<br/>(Google AI / Vertex AI)\"] UMLS[\"UMLS API<br/>(NLM)\"] GCS[\"GCS / Local Storage\"] end UI -->|\"HTTP REST\"| API API --> DB API --> OBX OBX -->|\"protocol_uploaded\"| PP PP --> DB PP -->|\"PDF extraction\"| GEMINI PP -->|\"Entity grounding\"| UMLS PP -->|\"Concept resolution\"| OMOP PP -->|\"PDF fetch\"| GCS API -->|\"Traces\"| MLF PP -->|\"Traces\"| MLF","title":"Container Diagram"},{"location":"architecture/system-architecture/#communication-paths","text":"","title":"Communication Paths"},{"location":"architecture/system-architecture/#synchronous-http-ui-to-api","text":"All UI interactions flow through the FastAPI REST API: Path Description Key file POST /protocols/upload Generate signed upload URL services/api-service/src/api_service/protocols.py POST /protocols/{id}/confirm-upload Confirm upload, trigger pipeline protocols.py GET /protocols List protocols (paginated) protocols.py GET /reviews/batches/{id}/criteria Fetch criteria for review reviews.py POST /reviews/criteria/{id}/action Submit review decision reviews.py GET /exports/{id}/circe Export as CIRCE JSON exports.py GET /exports/{id}/fhir-group Export as FHIR R4 Group exports.py GET /exports/{id}/evaluation-sql Export as OMOP SQL exports.py","title":"Synchronous HTTP (UI to API)"},{"location":"architecture/system-architecture/#asynchronous-event-processing-outbox-to-pipeline","text":"The pipeline is triggered via the transactional outbox pattern: confirm-upload endpoint calls persist_with_outbox() \u2014 writes Protocol record + OutboxEvent in a single DB transaction OutboxProcessor polls the outboxevent table every 2 seconds When it finds a protocol_uploaded event, it calls handle_protocol_uploaded() in a thread executor The handler invokes the 7-node LangGraph pipeline via asyncio.run() sequenceDiagram participant UI as hitl-ui participant API as api-service participant DB as PostgreSQL participant OBX as OutboxProcessor participant PP as protocol-processor UI->>API: POST /protocols/{id}/confirm-upload API->>DB: INSERT Protocol + OutboxEvent (single tx) API-->>UI: 200 OK loop Every 2s OBX->>DB: SELECT FROM outboxevent WHERE status='pending' end OBX->>PP: handle_protocol_uploaded(payload) PP->>DB: UPDATE protocol.status = 'extracting' PP->>PP: Run 7-node LangGraph pipeline PP->>DB: INSERT criteria, entities, atomics PP->>DB: UPDATE protocol.status = 'pending_review'","title":"Asynchronous Event Processing (Outbox to Pipeline)"},{"location":"architecture/system-architecture/#storage-paths","text":"Local dev : Files stored in ./uploads/ directory, served by api-service at /local-files/{path} Production : Files uploaded to GCS via signed URL, fetched by pipeline from gs:// URI","title":"Storage Paths"},{"location":"architecture/system-architecture/#observability","text":"MLflow : Traces emitted by both api-service (request middleware) and pipeline nodes (per-node spans) Tracking URI : http://localhost:5001 (local), http://mlflow:5000 (Docker) Experiment : protocol-processing Orphan cleanup : trigger.py closes stale IN_PROGRESS traces at startup","title":"Observability"},{"location":"architecture/system-architecture/#authentication","text":"Google OAuth 2.0 via api-service/auth.py Session middleware with JWT tokens All API routes (except /auth/* and /health ) require authentication Dev mode works without OAuth configured","title":"Authentication"},{"location":"architecture/system-architecture/#error-handling","text":"Error type Behavior Status Fatal (PDF corrupt, auth fail) Pipeline stops at failed node extraction_failed Partial (some entities fail grounding) Pipeline continues, errors accumulated pending_review with errors[] Total grounding failure All entities failed grounding_failed Outbox exhaustion (3 retries) Event marked dead_letter dead_letter Dead letter aging (7+ days) Lazy archival on next access archived","title":"Error Handling"},{"location":"code-tour/","text":"Code Tour \u00b6 A linear walkthrough of the key code paths in ElixirTrials, from upload to review. Each \"slide\" covers one critical module. Slide 1: Upload Dialog \u00b6 As a researcher , I want to upload a protocol PDF so the system can extract its eligibility criteria. File : apps/hitl-ui/src/components/ProtocolUploadDialog.tsx const uploadMutation = useUploadProtocol (); const handleUpload = useCallback ( async () => { if ( ! selectedFile ) return ; try { await uploadMutation . mutateAsync ({ file : selectedFile }); setSelectedFile ( null ); onOpenChange ( false ); } catch ( err ) { setError ( err instanceof Error ? err . message : 'Upload failed' ); } }, [ selectedFile , uploadMutation , onOpenChange ]); Why this matters : The upload dialog is the entry point for all protocol processing. It validates PDF type and 50 MB size limit client-side before hitting the API. The useUploadProtocol hook handles the two-step signed URL flow (upload \u2192 confirm). Slide 2: Protocol Upload API \u00b6 As the API , I create a protocol record and generate a signed URL for direct browser-to-storage upload. File : services/api-service/src/api_service/protocols.py:136 @router . post ( \"/upload\" , response_model = UploadResponse ) def upload_protocol ( body : UploadRequest , db : Session = Depends ( get_db )): signed_url , gcs_path = generate_upload_url ( filename = body . filename , content_type = body . content_type , ) protocol = Protocol ( title = title , file_uri = gcs_path , status = \"uploaded\" ) db . add ( protocol ) db . commit () return UploadResponse ( protocol_id = protocol . id , upload_url = signed_url , ... ) Why this matters : The upload is a two-phase process. Phase 1 creates the DB record and returns a signed URL. Phase 2 ( confirm-upload ) triggers processing via the outbox. This prevents processing a file that was never actually uploaded. Slide 3: Outbox Wiring \u00b6 As the API service , I connect the outbox processor to the pipeline trigger at startup. File : services/api-service/src/api_service/main.py:81-93 processor = OutboxProcessor ( engine = engine , handlers = { \"protocol_uploaded\" : [ handle_protocol_uploaded ], }, ) task = asyncio . create_task ( processor . start ()) Why this matters : This is the bridge between the synchronous API world and the async pipeline. The outbox pattern ensures at-least-once delivery \u2014 if the API crashes after writing the outbox event but before the handler runs, the event will be picked up on restart. Note: criteria_extracted was removed in v2.0; protocol_uploaded is the only event. Slide 4: Pipeline Trigger \u00b6 As the outbox processor , I dispatch events to the LangGraph pipeline. File : services/protocol-processor-service/src/protocol_processor/trigger.py:214 def handle_protocol_uploaded ( payload : dict [ str , Any ]) -> None : thread_id = f \" { protocol_id } : { uuid4 () } \" initial_state = { \"protocol_id\" : payload [ \"protocol_id\" ], \"file_uri\" : payload [ \"file_uri\" ], \"title\" : payload [ \"title\" ], \"status\" : \"processing\" , \"error\" : None , \"errors\" : [], ... } config = { \"configurable\" : { \"thread_id\" : thread_id }} asyncio . run ( _run_pipeline ( initial_state , config , payload )) Why this matters : The trigger bridges sync (outbox handler) to async (LangGraph) via asyncio.run() . Each run gets a unique thread_id (protocol_id + uuid4) to prevent checkpoint collision on re-extraction. The thread_id is stored in protocol metadata for retry support. Slide 5: Ingest Node \u00b6 As the first pipeline node , I fetch the PDF bytes from storage. File : services/protocol-processor-service/src/protocol_processor/nodes/ingest.py async def ingest_node ( state : PipelineState ) -> dict [ str , Any ]: pdf_bytes = await fetch_pdf_bytes ( state [ \"file_uri\" ]) # Update protocol status in DB with Session ( engine ) as session : protocol = session . get ( Protocol , state [ \"protocol_id\" ]) protocol . status = \"extracting\" session . commit () return { \"pdf_bytes\" : pdf_bytes , \"status\" : \"processing\" } Why this matters : Ingest is the first node with external I/O (storage fetch). If the file doesn't exist or storage is down, this is where it fails \u2014 with a clear error that routes the pipeline to END. Slide 6: Extract Node \u00b6 As the extract node , I send the PDF to Gemini and get structured criteria back. File : services/protocol-processor-service/src/protocol_processor/nodes/extract.py async def extract_node ( state : PipelineState ) -> dict [ str , Any ]: result = await extract_criteria_structured ( pdf_bytes = state [ \"pdf_bytes\" ], protocol_title = state [ \"title\" ], ) return { \"extraction_json\" : json . dumps ( result ), \"pdf_bytes\" : None , # Clear to reduce checkpoint size \"status\" : \"processing\" , } Why this matters : This is the core AI step \u2014 Gemini 2.5 Flash reads the PDF and returns structured JSON with inclusion/exclusion criteria, categories, and confidence scores. The pdf_bytes are explicitly cleared after extraction to keep LangGraph checkpoints small. Slide 7: Parse Node \u00b6 As the parse node , I create database records and decompose criteria into entities. File : services/protocol-processor-service/src/protocol_processor/nodes/parse.py # Phase A: Create batch and criteria records batch = CriteriaBatch ( protocol_id = protocol_id ) session . add ( batch ) for item in criteria_list : criterion = Criteria ( batch_id = batch . id , text = item [ \"text\" ], ... ) session . add ( criterion ) # Phase B: Async entity decomposition (outside DB session) tasks = [ decompose_entities_from_criterion ( c ) for c in criteria ] results = await asyncio . gather ( * tasks ) Why this matters : Parse separates DB persistence (fast, transactional) from LLM decomposition (slow, parallelized). Entity decomposition runs with a semaphore of 4 to limit concurrent Gemini calls. The DB session is closed before LLM calls to avoid holding connections during I/O. Slide 8: Ground Node \u00b6 As the ground node , I link entities to standard terminologies using dual grounding. File : services/protocol-processor-service/src/protocol_processor/nodes/ground.py async def _ground_entity_with_retry ( entity , semaphore ): async with semaphore : # Dual grounding: UMLS + OMOP in parallel umls_result , omop_result = await asyncio . gather ( terminology_router . search ( entity ), omop_mapper . resolve ( entity ), ) result = _reconcile_dual_grounding ( umls_result , omop_result ) # Agentic retry if confidence < 0.5 if result . confidence < 0.5 : for attempt in range ( 3 ): # MedGemma reasoning loop ... Why this matters : Grounding is the most complex node \u2014 dual sourcing, reconciliation, and agentic retries. Error accumulation means one failed entity doesn't block the others. This is where the system gets the SNOMED, LOINC, RxNorm, and OMOP codes needed for export. Slide 9: Structure Node \u00b6 As the structure node , I build expression trees from grounded criteria. File : services/protocol-processor-service/src/protocol_processor/nodes/structure.py tree = await build_expression_tree ( criterion_text = criterion . text , field_mappings = field_mappings , criterion_id = criterion . id , protocol_id = protocol_id , inclusion_exclusion = inclusion_exclusion , session = session , ) criterion . structured_criterion = tree . model_dump () Why this matters : Structure transforms flat criteria into queryable expression trees. Gemini detects AND/OR/NOT logic, then the builder creates AtomicCriterion , CompositeCriterion , and CriterionRelationship records. These trees power the CIRCE and FHIR exports. Slide 10: Review Page \u00b6 As a clinician , I review extracted criteria side-by-side with the source PDF. File : apps/hitl-ui/src/screens/ReviewPage.tsx < PanelGroup direction = \"horizontal\" > < Panel defaultSize = { 50 }> < PdfViewer url = { pdfData . url } targetPage = { activeCriterion ? . page_number ?? null } highlightText = { activeCriterion ? . text ?? null } /> </ Panel > < PanelResizeHandle /> < Panel defaultSize = { 50 }> { inclusionCriteria . map (( c ) => ( < CriterionCard criterion = { c } onAction = { handleAction } onCriterionClick = { handleCriterionClick } isActive = { activeCriterion ? . id === c . id } /> ))} </ Panel > </ PanelGroup > Why this matters : The review page is where human judgment meets AI output. Clicking a criterion scrolls the PDF to the source page and highlights the text. Criteria are grouped by inclusion/exclusion with pending items sorted first. The progress bar shows overall review completion.","title":"Overview"},{"location":"code-tour/#code-tour","text":"A linear walkthrough of the key code paths in ElixirTrials, from upload to review. Each \"slide\" covers one critical module.","title":"Code Tour"},{"location":"code-tour/#slide-1-upload-dialog","text":"As a researcher , I want to upload a protocol PDF so the system can extract its eligibility criteria. File : apps/hitl-ui/src/components/ProtocolUploadDialog.tsx const uploadMutation = useUploadProtocol (); const handleUpload = useCallback ( async () => { if ( ! selectedFile ) return ; try { await uploadMutation . mutateAsync ({ file : selectedFile }); setSelectedFile ( null ); onOpenChange ( false ); } catch ( err ) { setError ( err instanceof Error ? err . message : 'Upload failed' ); } }, [ selectedFile , uploadMutation , onOpenChange ]); Why this matters : The upload dialog is the entry point for all protocol processing. It validates PDF type and 50 MB size limit client-side before hitting the API. The useUploadProtocol hook handles the two-step signed URL flow (upload \u2192 confirm).","title":"Slide 1: Upload Dialog"},{"location":"code-tour/#slide-2-protocol-upload-api","text":"As the API , I create a protocol record and generate a signed URL for direct browser-to-storage upload. File : services/api-service/src/api_service/protocols.py:136 @router . post ( \"/upload\" , response_model = UploadResponse ) def upload_protocol ( body : UploadRequest , db : Session = Depends ( get_db )): signed_url , gcs_path = generate_upload_url ( filename = body . filename , content_type = body . content_type , ) protocol = Protocol ( title = title , file_uri = gcs_path , status = \"uploaded\" ) db . add ( protocol ) db . commit () return UploadResponse ( protocol_id = protocol . id , upload_url = signed_url , ... ) Why this matters : The upload is a two-phase process. Phase 1 creates the DB record and returns a signed URL. Phase 2 ( confirm-upload ) triggers processing via the outbox. This prevents processing a file that was never actually uploaded.","title":"Slide 2: Protocol Upload API"},{"location":"code-tour/#slide-3-outbox-wiring","text":"As the API service , I connect the outbox processor to the pipeline trigger at startup. File : services/api-service/src/api_service/main.py:81-93 processor = OutboxProcessor ( engine = engine , handlers = { \"protocol_uploaded\" : [ handle_protocol_uploaded ], }, ) task = asyncio . create_task ( processor . start ()) Why this matters : This is the bridge between the synchronous API world and the async pipeline. The outbox pattern ensures at-least-once delivery \u2014 if the API crashes after writing the outbox event but before the handler runs, the event will be picked up on restart. Note: criteria_extracted was removed in v2.0; protocol_uploaded is the only event.","title":"Slide 3: Outbox Wiring"},{"location":"code-tour/#slide-4-pipeline-trigger","text":"As the outbox processor , I dispatch events to the LangGraph pipeline. File : services/protocol-processor-service/src/protocol_processor/trigger.py:214 def handle_protocol_uploaded ( payload : dict [ str , Any ]) -> None : thread_id = f \" { protocol_id } : { uuid4 () } \" initial_state = { \"protocol_id\" : payload [ \"protocol_id\" ], \"file_uri\" : payload [ \"file_uri\" ], \"title\" : payload [ \"title\" ], \"status\" : \"processing\" , \"error\" : None , \"errors\" : [], ... } config = { \"configurable\" : { \"thread_id\" : thread_id }} asyncio . run ( _run_pipeline ( initial_state , config , payload )) Why this matters : The trigger bridges sync (outbox handler) to async (LangGraph) via asyncio.run() . Each run gets a unique thread_id (protocol_id + uuid4) to prevent checkpoint collision on re-extraction. The thread_id is stored in protocol metadata for retry support.","title":"Slide 4: Pipeline Trigger"},{"location":"code-tour/#slide-5-ingest-node","text":"As the first pipeline node , I fetch the PDF bytes from storage. File : services/protocol-processor-service/src/protocol_processor/nodes/ingest.py async def ingest_node ( state : PipelineState ) -> dict [ str , Any ]: pdf_bytes = await fetch_pdf_bytes ( state [ \"file_uri\" ]) # Update protocol status in DB with Session ( engine ) as session : protocol = session . get ( Protocol , state [ \"protocol_id\" ]) protocol . status = \"extracting\" session . commit () return { \"pdf_bytes\" : pdf_bytes , \"status\" : \"processing\" } Why this matters : Ingest is the first node with external I/O (storage fetch). If the file doesn't exist or storage is down, this is where it fails \u2014 with a clear error that routes the pipeline to END.","title":"Slide 5: Ingest Node"},{"location":"code-tour/#slide-6-extract-node","text":"As the extract node , I send the PDF to Gemini and get structured criteria back. File : services/protocol-processor-service/src/protocol_processor/nodes/extract.py async def extract_node ( state : PipelineState ) -> dict [ str , Any ]: result = await extract_criteria_structured ( pdf_bytes = state [ \"pdf_bytes\" ], protocol_title = state [ \"title\" ], ) return { \"extraction_json\" : json . dumps ( result ), \"pdf_bytes\" : None , # Clear to reduce checkpoint size \"status\" : \"processing\" , } Why this matters : This is the core AI step \u2014 Gemini 2.5 Flash reads the PDF and returns structured JSON with inclusion/exclusion criteria, categories, and confidence scores. The pdf_bytes are explicitly cleared after extraction to keep LangGraph checkpoints small.","title":"Slide 6: Extract Node"},{"location":"code-tour/#slide-7-parse-node","text":"As the parse node , I create database records and decompose criteria into entities. File : services/protocol-processor-service/src/protocol_processor/nodes/parse.py # Phase A: Create batch and criteria records batch = CriteriaBatch ( protocol_id = protocol_id ) session . add ( batch ) for item in criteria_list : criterion = Criteria ( batch_id = batch . id , text = item [ \"text\" ], ... ) session . add ( criterion ) # Phase B: Async entity decomposition (outside DB session) tasks = [ decompose_entities_from_criterion ( c ) for c in criteria ] results = await asyncio . gather ( * tasks ) Why this matters : Parse separates DB persistence (fast, transactional) from LLM decomposition (slow, parallelized). Entity decomposition runs with a semaphore of 4 to limit concurrent Gemini calls. The DB session is closed before LLM calls to avoid holding connections during I/O.","title":"Slide 7: Parse Node"},{"location":"code-tour/#slide-8-ground-node","text":"As the ground node , I link entities to standard terminologies using dual grounding. File : services/protocol-processor-service/src/protocol_processor/nodes/ground.py async def _ground_entity_with_retry ( entity , semaphore ): async with semaphore : # Dual grounding: UMLS + OMOP in parallel umls_result , omop_result = await asyncio . gather ( terminology_router . search ( entity ), omop_mapper . resolve ( entity ), ) result = _reconcile_dual_grounding ( umls_result , omop_result ) # Agentic retry if confidence < 0.5 if result . confidence < 0.5 : for attempt in range ( 3 ): # MedGemma reasoning loop ... Why this matters : Grounding is the most complex node \u2014 dual sourcing, reconciliation, and agentic retries. Error accumulation means one failed entity doesn't block the others. This is where the system gets the SNOMED, LOINC, RxNorm, and OMOP codes needed for export.","title":"Slide 8: Ground Node"},{"location":"code-tour/#slide-9-structure-node","text":"As the structure node , I build expression trees from grounded criteria. File : services/protocol-processor-service/src/protocol_processor/nodes/structure.py tree = await build_expression_tree ( criterion_text = criterion . text , field_mappings = field_mappings , criterion_id = criterion . id , protocol_id = protocol_id , inclusion_exclusion = inclusion_exclusion , session = session , ) criterion . structured_criterion = tree . model_dump () Why this matters : Structure transforms flat criteria into queryable expression trees. Gemini detects AND/OR/NOT logic, then the builder creates AtomicCriterion , CompositeCriterion , and CriterionRelationship records. These trees power the CIRCE and FHIR exports.","title":"Slide 9: Structure Node"},{"location":"code-tour/#slide-10-review-page","text":"As a clinician , I review extracted criteria side-by-side with the source PDF. File : apps/hitl-ui/src/screens/ReviewPage.tsx < PanelGroup direction = \"horizontal\" > < Panel defaultSize = { 50 }> < PdfViewer url = { pdfData . url } targetPage = { activeCriterion ? . page_number ?? null } highlightText = { activeCriterion ? . text ?? null } /> </ Panel > < PanelResizeHandle /> < Panel defaultSize = { 50 }> { inclusionCriteria . map (( c ) => ( < CriterionCard criterion = { c } onAction = { handleAction } onCriterionClick = { handleCriterionClick } isActive = { activeCriterion ? . id === c . id } /> ))} </ Panel > </ PanelGroup > Why this matters : The review page is where human judgment meets AI output. Clicking a criterion scrolls the PDF to the source page and highlights the text. Criteria are grouped by inclusion/exclusion with pending items sorted first. The progress bar shows overall review completion.","title":"Slide 10: Review Page"},{"location":"components/","text":"Components Overview \u00b6 Auto-generated list of workspace components. Services \u00b6 Component Has Docs pyproject.toml services/api-service yes yes services/protocol-processor-service yes yes Libraries \u00b6 Component Has Docs pyproject.toml libs/data-pipeline yes yes libs/evaluation yes yes libs/events-py yes yes libs/events-ts yes yes libs/inference yes yes libs/model-training yes yes libs/shared yes yes libs/shared-ts yes yes Applications \u00b6 Component Has Docs pyproject.toml apps/hitl-ui yes yes","title":"Overview"},{"location":"components/#components-overview","text":"Auto-generated list of workspace components.","title":"Components Overview"},{"location":"components/#services","text":"Component Has Docs pyproject.toml services/api-service yes yes services/protocol-processor-service yes yes","title":"Services"},{"location":"components/#libraries","text":"Component Has Docs pyproject.toml libs/data-pipeline yes yes libs/evaluation yes yes libs/events-py yes yes libs/events-ts yes yes libs/inference yes yes libs/model-training yes yes libs/shared yes yes libs/shared-ts yes yes","title":"Libraries"},{"location":"components/#applications","text":"Component Has Docs pyproject.toml apps/hitl-ui yes yes","title":"Applications"},{"location":"data-pipeline/api/","text":"API Reference for data-pipeline \u00b6","title":"API Reference"},{"location":"data-pipeline/api/#api-reference-for-data-pipeline","text":"","title":"API Reference for data-pipeline"},{"location":"development/gemini-vertex-auth/","text":"Gemini & Vertex AI Authentication \u00b6 ElixirTrials supports two backends for Gemini model access: the Gemini Developer API (default) and Vertex AI (for MedGemma). Gemini Developer API (Default) \u00b6 Used for criteria extraction, entity decomposition, logic detection, and ordinal resolution. Setup \u00b6 Get an API key from Google AI Studio Add to .env : GOOGLE_API_KEY = your-key-here GEMINI_MODEL_NAME = gemini-2.5-flash # default Verify \u00b6 make verify-gemini This runs scripts/verify_gemini_access.py which makes a test call to the Gemini API. Vertex AI (MedGemma) \u00b6 Used for agentic grounding retry loop (MedGemma reasoning). Setup \u00b6 Create a GCP project with Vertex AI enabled Deploy MedGemma to a Vertex AI endpoint Configure Application Default Credentials: gcloud auth application-default login make setup-adc # Sets quota project from .env Add to .env : MODEL_BACKEND = vertex GCP_PROJECT_ID = your-project-id GCP_REGION = europe-west4 VERTEX_ENDPOINT_ID = your-endpoint-id ADC Setup Script \u00b6 The make setup-adc command runs scripts/setup-gcloud-adc.sh , which: Reads GCP_PROJECT_ID or GOOGLE_CLOUD_QUOTA_PROJECT from .env Sets the quota project on your Application Default Credentials Required when using user-level ADC (not service accounts) for Vertex AI Docker Compose \u00b6 For Docker deployment, ADC credentials are mounted as a read-only volume: volumes : - ${GOOGLE_ADC_PATH:-~/.config/gcloud/application_default_credentials.json}:/tmp/keys/application_default_credentials.json:ro environment : - GOOGLE_APPLICATION_CREDENTIALS=/tmp/keys/application_default_credentials.json Google OAuth (UI Login) \u00b6 For the HITL UI authentication: Create OAuth 2.0 credentials at GCP Console \u2192 APIs & Services \u2192 Credentials Set redirect URI to http://localhost:8000/auth/callback Add to .env : GOOGLE_CLIENT_ID = your-client-id GOOGLE_CLIENT_SECRET = your-client-secret OAuth is optional for local development \u2014 the API works without it configured. Environment Variable Reference \u00b6 Variable Required for Default GOOGLE_API_KEY Gemini Developer API \u2014 GEMINI_MODEL_NAME Gemini model selection gemini-2.5-flash MODEL_BACKEND Backend selection (Gemini Developer API) GCP_PROJECT_ID Vertex AI \u2014 GCP_REGION Vertex AI europe-west4 VERTEX_ENDPOINT_ID MedGemma endpoint \u2014 GOOGLE_CLIENT_ID OAuth login \u2014 GOOGLE_CLIENT_SECRET OAuth login \u2014 GCLOUD_PROFILE gcloud config profile \u2014 MLFLOW_TRACKING_URI Experiment tracking http://localhost:5001 UMLS_API_KEY UMLS grounding \u2014","title":"Gemini & Vertex Auth"},{"location":"development/gemini-vertex-auth/#gemini-vertex-ai-authentication","text":"ElixirTrials supports two backends for Gemini model access: the Gemini Developer API (default) and Vertex AI (for MedGemma).","title":"Gemini &amp; Vertex AI Authentication"},{"location":"development/gemini-vertex-auth/#gemini-developer-api-default","text":"Used for criteria extraction, entity decomposition, logic detection, and ordinal resolution.","title":"Gemini Developer API (Default)"},{"location":"development/gemini-vertex-auth/#setup","text":"Get an API key from Google AI Studio Add to .env : GOOGLE_API_KEY = your-key-here GEMINI_MODEL_NAME = gemini-2.5-flash # default","title":"Setup"},{"location":"development/gemini-vertex-auth/#verify","text":"make verify-gemini This runs scripts/verify_gemini_access.py which makes a test call to the Gemini API.","title":"Verify"},{"location":"development/gemini-vertex-auth/#vertex-ai-medgemma","text":"Used for agentic grounding retry loop (MedGemma reasoning).","title":"Vertex AI (MedGemma)"},{"location":"development/gemini-vertex-auth/#setup_1","text":"Create a GCP project with Vertex AI enabled Deploy MedGemma to a Vertex AI endpoint Configure Application Default Credentials: gcloud auth application-default login make setup-adc # Sets quota project from .env Add to .env : MODEL_BACKEND = vertex GCP_PROJECT_ID = your-project-id GCP_REGION = europe-west4 VERTEX_ENDPOINT_ID = your-endpoint-id","title":"Setup"},{"location":"development/gemini-vertex-auth/#adc-setup-script","text":"The make setup-adc command runs scripts/setup-gcloud-adc.sh , which: Reads GCP_PROJECT_ID or GOOGLE_CLOUD_QUOTA_PROJECT from .env Sets the quota project on your Application Default Credentials Required when using user-level ADC (not service accounts) for Vertex AI","title":"ADC Setup Script"},{"location":"development/gemini-vertex-auth/#docker-compose","text":"For Docker deployment, ADC credentials are mounted as a read-only volume: volumes : - ${GOOGLE_ADC_PATH:-~/.config/gcloud/application_default_credentials.json}:/tmp/keys/application_default_credentials.json:ro environment : - GOOGLE_APPLICATION_CREDENTIALS=/tmp/keys/application_default_credentials.json","title":"Docker Compose"},{"location":"development/gemini-vertex-auth/#google-oauth-ui-login","text":"For the HITL UI authentication: Create OAuth 2.0 credentials at GCP Console \u2192 APIs & Services \u2192 Credentials Set redirect URI to http://localhost:8000/auth/callback Add to .env : GOOGLE_CLIENT_ID = your-client-id GOOGLE_CLIENT_SECRET = your-client-secret OAuth is optional for local development \u2014 the API works without it configured.","title":"Google OAuth (UI Login)"},{"location":"development/gemini-vertex-auth/#environment-variable-reference","text":"Variable Required for Default GOOGLE_API_KEY Gemini Developer API \u2014 GEMINI_MODEL_NAME Gemini model selection gemini-2.5-flash MODEL_BACKEND Backend selection (Gemini Developer API) GCP_PROJECT_ID Vertex AI \u2014 GCP_REGION Vertex AI europe-west4 VERTEX_ENDPOINT_ID MedGemma endpoint \u2014 GOOGLE_CLIENT_ID OAuth login \u2014 GOOGLE_CLIENT_SECRET OAuth login \u2014 GCLOUD_PROFILE gcloud config profile \u2014 MLFLOW_TRACKING_URI Experiment tracking http://localhost:5001 UMLS_API_KEY UMLS grounding \u2014","title":"Environment Variable Reference"},{"location":"diagrams/agent-flow/","text":"Agent Flow \u00b6 Agentic grounding pattern used when initial entity grounding has low confidence. flowchart TB ENT[Entity from parse node] --> DUAL[Dual grounding] subgraph Dual[\"Parallel Grounding\"] DUAL --> TR[TerminologyRouter<br/>UMLS search] DUAL --> OM[OMOP Mapper<br/>concept lookup] end TR --> REC[Reconcile results] OM --> REC REC --> CONF{Confidence >= 0.5?} CONF -->|Yes| DONE[Accept grounding] CONF -->|No| AGENT[Agentic retry loop] subgraph Retry[\"MedGemma Reasoning (max 3)\"] AGENT --> Q1[Is entity valid<br/>for coding?] Q1 -->|Yes| Q2[Derive broader<br/>concept?] Q1 -->|No| SKIP[Skip entity] Q2 --> Q3[Rephrase for<br/>better match?] Q3 --> RETRY[Retry grounding<br/>with improved input] end RETRY --> CONF2{Improved?} CONF2 -->|Yes| DONE CONF2 -->|No, attempts left| AGENT CONF2 -->|No, exhausted| EXPERT[Route to<br/>expert review] style DONE fill:#22c55e,color:#fff style EXPERT fill:#f59e0b,color:#fff style SKIP fill:#ef4444,color:#fff Grounding Pipeline Details \u00b6 TerminologyRouter \u00b6 Routes entities to the best UMLS source vocabulary based on entity type: Entity type Primary vocabulary Fallback condition SNOMED CT ICD-10 measurement LOINC SNOMED CT drug RxNorm SNOMED CT procedure SNOMED CT CPT OMOP Mapper \u00b6 Resolves entities to OMOP CDM concept IDs using the OMOP vocabulary tables. Enables direct joins against CDM data warehouses. Reconciliation \u00b6 _reconcile_dual_grounding() merges results from both paths: Prefers OMOP concept ID when available (needed for exports) Falls back to source terminology codes Combines confidence scores from both paths File : services/protocol-processor-service/src/protocol_processor/nodes/ground.py","title":"Agent Flow"},{"location":"diagrams/agent-flow/#agent-flow","text":"Agentic grounding pattern used when initial entity grounding has low confidence. flowchart TB ENT[Entity from parse node] --> DUAL[Dual grounding] subgraph Dual[\"Parallel Grounding\"] DUAL --> TR[TerminologyRouter<br/>UMLS search] DUAL --> OM[OMOP Mapper<br/>concept lookup] end TR --> REC[Reconcile results] OM --> REC REC --> CONF{Confidence >= 0.5?} CONF -->|Yes| DONE[Accept grounding] CONF -->|No| AGENT[Agentic retry loop] subgraph Retry[\"MedGemma Reasoning (max 3)\"] AGENT --> Q1[Is entity valid<br/>for coding?] Q1 -->|Yes| Q2[Derive broader<br/>concept?] Q1 -->|No| SKIP[Skip entity] Q2 --> Q3[Rephrase for<br/>better match?] Q3 --> RETRY[Retry grounding<br/>with improved input] end RETRY --> CONF2{Improved?} CONF2 -->|Yes| DONE CONF2 -->|No, attempts left| AGENT CONF2 -->|No, exhausted| EXPERT[Route to<br/>expert review] style DONE fill:#22c55e,color:#fff style EXPERT fill:#f59e0b,color:#fff style SKIP fill:#ef4444,color:#fff","title":"Agent Flow"},{"location":"diagrams/agent-flow/#grounding-pipeline-details","text":"","title":"Grounding Pipeline Details"},{"location":"diagrams/agent-flow/#terminologyrouter","text":"Routes entities to the best UMLS source vocabulary based on entity type: Entity type Primary vocabulary Fallback condition SNOMED CT ICD-10 measurement LOINC SNOMED CT drug RxNorm SNOMED CT procedure SNOMED CT CPT","title":"TerminologyRouter"},{"location":"diagrams/agent-flow/#omop-mapper","text":"Resolves entities to OMOP CDM concept IDs using the OMOP vocabulary tables. Enables direct joins against CDM data warehouses.","title":"OMOP Mapper"},{"location":"diagrams/agent-flow/#reconciliation","text":"_reconcile_dual_grounding() merges results from both paths: Prefers OMOP concept ID when available (needed for exports) Falls back to source terminology codes Combines confidence scores from both paths File : services/protocol-processor-service/src/protocol_processor/nodes/ground.py","title":"Reconciliation"},{"location":"diagrams/hitl-flow/","text":"HITL Flow \u00b6 Human-in-the-loop review workflow from criteria extraction to final approval. flowchart TB subgraph Pipeline[\"Automated Pipeline\"] EX[Extract criteria<br/>from PDF] --> PA[Parse into<br/>DB records] PA --> GR[Ground entities<br/>to terminologies] GR --> PE[Persist to DB] PE --> ST[Build expression<br/>trees] ST --> OR[Resolve ordinal<br/>scales] end OR --> PR{Protocol status?} PR -->|pending_review| RQ[Review Queue] PR -->|grounding_failed| FAIL[Error state] FAIL -->|retry| EX subgraph Review[\"HITL Review\"] RQ --> REV[Clinician reviews<br/>each criterion] REV -->|Approve| APP[Mark approved] REV -->|Reject| REJ[Mark rejected] REV -->|Modify| MOD[Edit + approve] end APP --> DONE{All reviewed?} REJ --> DONE MOD --> DONE DONE -->|Yes| COMP[Protocol complete] DONE -->|No| RQ COMP --> EXPORT[Export] EXPORT --> CIRCE[CIRCE JSON] EXPORT --> FHIR[FHIR R4 Group] EXPORT --> SQL[OMOP SQL] Review States \u00b6 State Meaning null (pending) Not yet reviewed approved Clinician confirmed AI output rejected Clinician marked as incorrect modified Clinician provided corrections Audit Trail \u00b6 Every review action creates a Review record with: before_value \u2014 state before the action after_value \u2014 state after the action reviewer_id \u2014 who performed the action comment \u2014 optional notes System-level events (grounding, status transitions) are logged to AuditLog .","title":"HITL Flow"},{"location":"diagrams/hitl-flow/#hitl-flow","text":"Human-in-the-loop review workflow from criteria extraction to final approval. flowchart TB subgraph Pipeline[\"Automated Pipeline\"] EX[Extract criteria<br/>from PDF] --> PA[Parse into<br/>DB records] PA --> GR[Ground entities<br/>to terminologies] GR --> PE[Persist to DB] PE --> ST[Build expression<br/>trees] ST --> OR[Resolve ordinal<br/>scales] end OR --> PR{Protocol status?} PR -->|pending_review| RQ[Review Queue] PR -->|grounding_failed| FAIL[Error state] FAIL -->|retry| EX subgraph Review[\"HITL Review\"] RQ --> REV[Clinician reviews<br/>each criterion] REV -->|Approve| APP[Mark approved] REV -->|Reject| REJ[Mark rejected] REV -->|Modify| MOD[Edit + approve] end APP --> DONE{All reviewed?} REJ --> DONE MOD --> DONE DONE -->|Yes| COMP[Protocol complete] DONE -->|No| RQ COMP --> EXPORT[Export] EXPORT --> CIRCE[CIRCE JSON] EXPORT --> FHIR[FHIR R4 Group] EXPORT --> SQL[OMOP SQL]","title":"HITL Flow"},{"location":"diagrams/hitl-flow/#review-states","text":"State Meaning null (pending) Not yet reviewed approved Clinician confirmed AI output rejected Clinician marked as incorrect modified Clinician provided corrections","title":"Review States"},{"location":"diagrams/hitl-flow/#audit-trail","text":"Every review action creates a Review record with: before_value \u2014 state before the action after_value \u2014 state after the action reviewer_id \u2014 who performed the action comment \u2014 optional notes System-level events (grounding, status transitions) are logged to AuditLog .","title":"Audit Trail"},{"location":"diagrams/langgraph-architecture/","text":"LangGraph Pipeline Architecture \u00b6 The protocol processing pipeline is implemented as a 7-node LangGraph StateGraph. Pipeline Graph \u00b6 graph LR START((START)) --> ingest ingest -->|ok| extract ingest -->|error| END1((END)) extract -->|ok| parse extract -->|error| END2((END)) parse -->|ok| ground parse -->|error| END3((END)) ground --> persist persist --> structure structure --> ordinal_resolve ordinal_resolve --> END4((END)) style ingest fill:#3b82f6,color:#fff style extract fill:#8b5cf6,color:#fff style parse fill:#06b6d4,color:#fff style ground fill:#10b981,color:#fff style persist fill:#f59e0b,color:#fff style structure fill:#ef4444,color:#fff style ordinal_resolve fill:#ec4899,color:#fff Node Responsibilities \u00b6 Node Purpose Key tool Error strategy ingest Fetch PDF bytes from storage fetch_pdf_bytes() Fatal \u2192 END extract LLM extraction of criteria from PDF Gemini structured output Fatal \u2192 END parse Create DB records, decompose entities decompose_entities_from_criterion() Fatal \u2192 END ground Link entities to SNOMED/LOINC/RxNorm/OMOP TerminologyRouter + OMOP Mapper Accumulate errors persist Write Entity records, update field_mappings Direct DB writes Accumulate errors structure Build expression trees from field_mappings Gemini logic detection Accumulate errors ordinal_resolve Identify ordinal scales, set unit_concept_id Gemini ordinal detection Accumulate errors Error Routing \u00b6 The first three nodes (ingest, extract, parse) use conditional edges \u2014 a fatal error routes to END immediately: workflow . add_conditional_edges ( source , should_continue , # checks state[\"error\"] { \"continue\" : target , \"error\" : END }, ) The remaining nodes (ground, persist, structure, ordinal_resolve) use error accumulation \u2014 individual item failures are appended to state[\"errors\"] but the pipeline continues. Checkpointing \u00b6 The graph is compiled with an AsyncPostgresSaver checkpointer that persists state after each node: Initial runs : thread_id = {protocol_id}:{uuid4} (unique per run to prevent collision on re-extraction) Retry : retry_from_checkpoint(protocol_id) looks up pipeline_thread_id from protocol metadata and resumes with ainvoke(None, config) \u2014 LangGraph replays from the last successful node File : services/protocol-processor-service/src/protocol_processor/graph.py State Shape \u00b6 See Data Models \u2014 Pipeline State for the full PipelineState TypedDict. Key design decisions: JSON strings over dicts for extraction/entity data \u2014 reduces checkpoint serialization size pdf_bytes cleared after extract \u2014 prevents storing megabytes in every checkpoint errors as list[str] \u2014 simple accumulation, no structured error objects","title":"LangGraph Pipeline"},{"location":"diagrams/langgraph-architecture/#langgraph-pipeline-architecture","text":"The protocol processing pipeline is implemented as a 7-node LangGraph StateGraph.","title":"LangGraph Pipeline Architecture"},{"location":"diagrams/langgraph-architecture/#pipeline-graph","text":"graph LR START((START)) --> ingest ingest -->|ok| extract ingest -->|error| END1((END)) extract -->|ok| parse extract -->|error| END2((END)) parse -->|ok| ground parse -->|error| END3((END)) ground --> persist persist --> structure structure --> ordinal_resolve ordinal_resolve --> END4((END)) style ingest fill:#3b82f6,color:#fff style extract fill:#8b5cf6,color:#fff style parse fill:#06b6d4,color:#fff style ground fill:#10b981,color:#fff style persist fill:#f59e0b,color:#fff style structure fill:#ef4444,color:#fff style ordinal_resolve fill:#ec4899,color:#fff","title":"Pipeline Graph"},{"location":"diagrams/langgraph-architecture/#node-responsibilities","text":"Node Purpose Key tool Error strategy ingest Fetch PDF bytes from storage fetch_pdf_bytes() Fatal \u2192 END extract LLM extraction of criteria from PDF Gemini structured output Fatal \u2192 END parse Create DB records, decompose entities decompose_entities_from_criterion() Fatal \u2192 END ground Link entities to SNOMED/LOINC/RxNorm/OMOP TerminologyRouter + OMOP Mapper Accumulate errors persist Write Entity records, update field_mappings Direct DB writes Accumulate errors structure Build expression trees from field_mappings Gemini logic detection Accumulate errors ordinal_resolve Identify ordinal scales, set unit_concept_id Gemini ordinal detection Accumulate errors","title":"Node Responsibilities"},{"location":"diagrams/langgraph-architecture/#error-routing","text":"The first three nodes (ingest, extract, parse) use conditional edges \u2014 a fatal error routes to END immediately: workflow . add_conditional_edges ( source , should_continue , # checks state[\"error\"] { \"continue\" : target , \"error\" : END }, ) The remaining nodes (ground, persist, structure, ordinal_resolve) use error accumulation \u2014 individual item failures are appended to state[\"errors\"] but the pipeline continues.","title":"Error Routing"},{"location":"diagrams/langgraph-architecture/#checkpointing","text":"The graph is compiled with an AsyncPostgresSaver checkpointer that persists state after each node: Initial runs : thread_id = {protocol_id}:{uuid4} (unique per run to prevent collision on re-extraction) Retry : retry_from_checkpoint(protocol_id) looks up pipeline_thread_id from protocol metadata and resumes with ainvoke(None, config) \u2014 LangGraph replays from the last successful node File : services/protocol-processor-service/src/protocol_processor/graph.py","title":"Checkpointing"},{"location":"diagrams/langgraph-architecture/#state-shape","text":"See Data Models \u2014 Pipeline State for the full PipelineState TypedDict. Key design decisions: JSON strings over dicts for extraction/entity data \u2014 reduces checkpoint serialization size pdf_bytes cleared after extract \u2014 prevents storing megabytes in every checkpoint errors as list[str] \u2014 simple accumulation, no structured error objects","title":"State Shape"},{"location":"evaluation/api/","text":"API Reference for evaluation \u00b6","title":"API Reference"},{"location":"evaluation/api/#api-reference-for-evaluation","text":"","title":"API Reference for evaluation"},{"location":"events-py/api/","text":"events-py API Reference \u00b6 events_py \u00b6 Event types and helpers for Python services (e.g. Pub/Sub). Classes \u00b6 EventEnvelope \u00b6 Bases: TypedDict Base event envelope for Pub/Sub or internal events. EventKind \u00b6 Bases: str , Enum Supported event kinds. Functions \u00b6 create_event \u00b6 create_event ( kind : EventKind , payload : Any , event_id : str | None = None , ) -> EventEnvelope Create an event envelope. Parameters: Name Type Description Default kind EventKind Event kind (created, updated, deleted). required payload Any Event payload (must be JSON-serializable). required event_id str | None Optional ID; if omitted, a UUID is generated. None Returns: Type Description EventEnvelope Event envelope dict. Example from events_py import EventKind, create_event ev = create_event(EventKind.CREATED, {\"name\": \"foo\"}) ev[\"kind\"] 'created' Source code in libs/events-py/src/events_py/models.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def create_event ( kind : EventKind , payload : Any , event_id : str | None = None , ) -> EventEnvelope : \"\"\"Create an event envelope. Args: kind: Event kind (created, updated, deleted). payload: Event payload (must be JSON-serializable). event_id: Optional ID; if omitted, a UUID is generated. Returns: Event envelope dict. Example: >>> from events_py import EventKind, create_event >>> ev = create_event(EventKind.CREATED, {\"name\": \"foo\"}) >>> ev[\"kind\"] 'created' \"\"\" return { \"id\" : event_id or str ( uuid4 ()), \"kind\" : kind . value , \"payload\" : payload , \"timestamp\" : datetime . now ( timezone . utc ) . isoformat (), } Modules \u00b6 models \u00b6 Event envelope and kind definitions. Classes \u00b6 EventKind \u00b6 Bases: str , Enum Supported event kinds. DomainEventKind \u00b6 Bases: str , Enum Domain-specific event types for the clinical trial pipeline. EventEnvelope \u00b6 Bases: TypedDict Base event envelope for Pub/Sub or internal events. DomainEventEnvelope \u00b6 Bases: TypedDict Domain event envelope for transactional outbox pattern. Functions \u00b6 create_event \u00b6 create_event ( kind : EventKind , payload : Any , event_id : str | None = None , ) -> EventEnvelope Create an event envelope. Parameters: Name Type Description Default kind EventKind Event kind (created, updated, deleted). required payload Any Event payload (must be JSON-serializable). required event_id str | None Optional ID; if omitted, a UUID is generated. None Returns: Type Description EventEnvelope Event envelope dict. Example from events_py import EventKind, create_event ev = create_event(EventKind.CREATED, {\"name\": \"foo\"}) ev[\"kind\"] 'created' Source code in libs/events-py/src/events_py/models.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def create_event ( kind : EventKind , payload : Any , event_id : str | None = None , ) -> EventEnvelope : \"\"\"Create an event envelope. Args: kind: Event kind (created, updated, deleted). payload: Event payload (must be JSON-serializable). event_id: Optional ID; if omitted, a UUID is generated. Returns: Event envelope dict. Example: >>> from events_py import EventKind, create_event >>> ev = create_event(EventKind.CREATED, {\"name\": \"foo\"}) >>> ev[\"kind\"] 'created' \"\"\" return { \"id\" : event_id or str ( uuid4 ()), \"kind\" : kind . value , \"payload\" : payload , \"timestamp\" : datetime . now ( timezone . utc ) . isoformat (), } create_domain_event \u00b6 create_domain_event ( event_type : DomainEventKind , aggregate_type : str , aggregate_id : str , payload : Any , idempotency_key : str | None = None , event_id : str | None = None , ) -> DomainEventEnvelope Create a domain event envelope for the transactional outbox. Parameters: Name Type Description Default event_type DomainEventKind Domain event kind. required aggregate_type str Type of aggregate (e.g. \"protocol\"). required aggregate_id str ID of the affected aggregate. required payload Any Event payload (must be JSON-serializable). required idempotency_key str | None Optional dedup key; auto-generated if omitted. None event_id str | None Optional event ID; auto-generated if omitted. None Returns: Type Description DomainEventEnvelope Domain event envelope dict. Source code in libs/events-py/src/events_py/models.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def create_domain_event ( event_type : DomainEventKind , aggregate_type : str , aggregate_id : str , payload : Any , idempotency_key : str | None = None , event_id : str | None = None , ) -> DomainEventEnvelope : \"\"\"Create a domain event envelope for the transactional outbox. Args: event_type: Domain event kind. aggregate_type: Type of aggregate (e.g. \"protocol\"). aggregate_id: ID of the affected aggregate. payload: Event payload (must be JSON-serializable). idempotency_key: Optional dedup key; auto-generated if omitted. event_id: Optional event ID; auto-generated if omitted. Returns: Domain event envelope dict. \"\"\" eid = event_id or str ( uuid4 ()) ikey = idempotency_key or ( f \" { event_type . value } : { aggregate_id } : { uuid4 () } \" ) return { \"id\" : eid , \"event_type\" : event_type . value , \"aggregate_type\" : aggregate_type , \"aggregate_id\" : aggregate_id , \"payload\" : payload , \"idempotency_key\" : ikey , \"timestamp\" : datetime . now ( timezone . utc ) . isoformat (), } outbox \u00b6 Outbox processor for publishing pending domain events. Implements the transactional outbox pattern with at-least-once delivery. The OutboxProcessor polls the outbox table for pending events and dispatches them to registered handlers. The persist_with_outbox helper enables atomic entity + event writes in a single database transaction. Classes \u00b6 OutboxProcessor \u00b6 OutboxProcessor ( engine : Engine , handlers : dict [ str , list [ Callable [ ... , Any ]]] | None = None , poll_interval : float = 1.0 , batch_size : int = 100 , ) Processes pending outbox events with at-least-once delivery. Usage processor = OutboxProcessor(engine, handlers={ \"protocol_uploaded\": [handle_protocol_uploaded], }) await processor.start() # Runs polling loop await processor.stop() # Graceful shutdown Initialize the outbox processor. Parameters: Name Type Description Default engine Engine SQLAlchemy database engine. required handlers dict [ str , list [ Callable [..., Any ]]] | None Maps event_type string to handler callbacks. None poll_interval float Seconds between polls. 1.0 batch_size int Max events per poll cycle. 100 Source code in libs/events-py/src/events_py/outbox.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , engine : Engine , handlers : dict [ str , list [ Callable [ ... , Any ]]] | None = None , poll_interval : float = 1.0 , batch_size : int = 100 , ) -> None : \"\"\"Initialize the outbox processor. Args: engine: SQLAlchemy database engine. handlers: Maps event_type string to handler callbacks. poll_interval: Seconds between polls. batch_size: Max events per poll cycle. \"\"\" self . engine = engine self . handlers : dict [ str , list [ Callable [ ... , Any ]]] = handlers or {} self . poll_interval = poll_interval self . batch_size = batch_size # Dedicated thread pool so pipeline handlers don't starve # FastAPI's default executor (used for sync endpoints). self . _executor = ThreadPoolExecutor ( max_workers = 2 , thread_name_prefix = \"outbox\" ) self . _shutdown_event = asyncio . Event () Functions \u00b6 poll_and_process \u00b6 poll_and_process () -> int Poll for pending events and process them. Returns: Type Description int Number of events processed in this poll cycle. Source code in libs/events-py/src/events_py/outbox.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def poll_and_process ( self ) -> int : \"\"\"Poll for pending events and process them. Returns: Number of events processed in this poll cycle. \"\"\" processed = 0 with Session ( self . engine ) as session : statement = ( select ( OutboxEvent ) . where ( col ( OutboxEvent . status ) . in_ ([ \"pending\" , \"failed\" ])) . order_by ( OutboxEvent . created_at . asc ()) # type: ignore[attr-defined] . limit ( self . batch_size ) ) # Use FOR UPDATE SKIP LOCKED on PostgreSQL # for concurrent processor safety db_url = str ( self . engine . url ) if \"postgresql\" in db_url : statement = statement . with_for_update ( skip_locked = True ) events = session . exec ( statement ) . all () for event in events : event_handlers = self . handlers . get ( event . event_type , []) if not event_handlers : logger . debug ( \"No handlers for event type: %s \" , event . event_type , ) try : for handler in event_handlers : handler ( event . payload ) event . status = \"published\" event . published_at = datetime . now ( timezone . utc ) session . add ( event ) processed += 1 logger . info ( \"Published event %s (type= %s )\" , event . id , event . event_type , ) except Exception : event . retry_count += 1 if event . retry_count >= MAX_RETRIES : event . status = \"dead_letter\" logger . warning ( \"Event %s exhausted retries ( %d ), marking as dead_letter\" , event . id , event . retry_count , ) # Update protocol status to dead_letter if protocol event if event . aggregate_type == \"protocol\" : from shared.models import Protocol protocol = session . get ( Protocol , event . aggregate_id ) if protocol : protocol . status = \"dead_letter\" protocol . error_reason = \"Maximum retries exceeded\" protocol . metadata_ = { ** protocol . metadata_ , \"error\" : { \"category\" : \"pipeline_failed\" , \"reason\" : \"Maximum retries exceeded\" , \"retry_count\" : event . retry_count , }, } session . add ( protocol ) else : event . status = \"failed\" session . add ( event ) logger . exception ( \"Failed to process event %s (type= %s , retry_count= %d )\" , event . id , event . event_type , event . retry_count , ) session . commit () return processed start async \u00b6 start () -> None Run the polling loop until stop() is called. Source code in libs/events-py/src/events_py/outbox.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 async def start ( self ) -> None : \"\"\"Run the polling loop until stop() is called.\"\"\" logger . info ( \"Outbox processor started (interval= %.1f s, batch_size= %d )\" , self . poll_interval , self . batch_size , ) while not self . _shutdown_event . is_set (): try : # Copy context so MLflow ContextVars propagate to the # worker thread, avoiding \"created in a different Context\" # warnings when autologging resets tokens. loop = asyncio . get_event_loop () ctx = contextvars . copy_context () count = await loop . run_in_executor ( self . _executor , ctx . run , self . poll_and_process ) if count > 0 : logger . info ( \"Processed %d outbox events\" , count ) except Exception : logger . exception ( \"Error in outbox processor poll cycle\" ) try : await asyncio . wait_for ( self . _shutdown_event . wait (), timeout = self . poll_interval , ) except asyncio . TimeoutError : pass logger . info ( \"Outbox processor stopped\" ) stop async \u00b6 stop () -> None Signal the processor to stop after current poll. Source code in libs/events-py/src/events_py/outbox.py 182 183 184 185 186 async def stop ( self ) -> None : \"\"\"Signal the processor to stop after current poll.\"\"\" logger . info ( \"Outbox processor shutdown requested\" ) self . _shutdown_event . set () self . _executor . shutdown ( wait = True ) Functions \u00b6 persist_with_outbox \u00b6 persist_with_outbox ( session : Session , entity : Any , event_type : DomainEventKind , aggregate_type : str , aggregate_id : str , payload : dict [ str , Any ], idempotency_key : str | None = None , ) -> OutboxEvent Persist an entity and its outbox event in the same transaction. The caller is responsible for committing the session. Parameters: Name Type Description Default session Session Active SQLModel session (not yet committed). required entity Any The SQLModel entity to persist. required event_type DomainEventKind Domain event kind. required aggregate_type str Type of aggregate (e.g. \"protocol\"). required aggregate_id str ID of the affected aggregate. required payload dict [ str , Any ] Event payload dict. required idempotency_key str | None Dedup key; auto-generated if omitted. None Returns: Type Description OutboxEvent The created OutboxEvent (uncommitted). Source code in libs/events-py/src/events_py/outbox.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def persist_with_outbox ( session : Session , entity : Any , event_type : DomainEventKind , aggregate_type : str , aggregate_id : str , payload : dict [ str , Any ], idempotency_key : str | None = None , ) -> OutboxEvent : \"\"\"Persist an entity and its outbox event in the same transaction. The caller is responsible for committing the session. Args: session: Active SQLModel session (not yet committed). entity: The SQLModel entity to persist. event_type: Domain event kind. aggregate_type: Type of aggregate (e.g. \"protocol\"). aggregate_id: ID of the affected aggregate. payload: Event payload dict. idempotency_key: Dedup key; auto-generated if omitted. Returns: The created OutboxEvent (uncommitted). \"\"\" ikey = idempotency_key or ( f \" { event_type . value } : { aggregate_id } : { uuid4 () } \" ) outbox_event = OutboxEvent ( event_type = event_type . value , aggregate_type = aggregate_type , aggregate_id = aggregate_id , payload = payload , idempotency_key = ikey , status = \"pending\" , ) session . add ( entity ) session . add ( outbox_event ) return outbox_event","title":"API Reference"},{"location":"events-py/api/#events-py-api-reference","text":"","title":"events-py API Reference"},{"location":"events-py/api/#events_py","text":"Event types and helpers for Python services (e.g. Pub/Sub).","title":"events_py"},{"location":"events-py/api/#events_py-classes","text":"","title":"Classes"},{"location":"events-py/api/#events_py.EventEnvelope","text":"Bases: TypedDict Base event envelope for Pub/Sub or internal events.","title":"EventEnvelope"},{"location":"events-py/api/#events_py.EventKind","text":"Bases: str , Enum Supported event kinds.","title":"EventKind"},{"location":"events-py/api/#events_py-functions","text":"","title":"Functions"},{"location":"events-py/api/#events_py.create_event","text":"create_event ( kind : EventKind , payload : Any , event_id : str | None = None , ) -> EventEnvelope Create an event envelope. Parameters: Name Type Description Default kind EventKind Event kind (created, updated, deleted). required payload Any Event payload (must be JSON-serializable). required event_id str | None Optional ID; if omitted, a UUID is generated. None Returns: Type Description EventEnvelope Event envelope dict. Example from events_py import EventKind, create_event ev = create_event(EventKind.CREATED, {\"name\": \"foo\"}) ev[\"kind\"] 'created' Source code in libs/events-py/src/events_py/models.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def create_event ( kind : EventKind , payload : Any , event_id : str | None = None , ) -> EventEnvelope : \"\"\"Create an event envelope. Args: kind: Event kind (created, updated, deleted). payload: Event payload (must be JSON-serializable). event_id: Optional ID; if omitted, a UUID is generated. Returns: Event envelope dict. Example: >>> from events_py import EventKind, create_event >>> ev = create_event(EventKind.CREATED, {\"name\": \"foo\"}) >>> ev[\"kind\"] 'created' \"\"\" return { \"id\" : event_id or str ( uuid4 ()), \"kind\" : kind . value , \"payload\" : payload , \"timestamp\" : datetime . now ( timezone . utc ) . isoformat (), }","title":"create_event"},{"location":"events-py/api/#events_py-modules","text":"","title":"Modules"},{"location":"events-py/api/#events_py.models","text":"Event envelope and kind definitions.","title":"models"},{"location":"events-py/api/#events_py.models-classes","text":"","title":"Classes"},{"location":"events-py/api/#events_py.models.EventKind","text":"Bases: str , Enum Supported event kinds.","title":"EventKind"},{"location":"events-py/api/#events_py.models.DomainEventKind","text":"Bases: str , Enum Domain-specific event types for the clinical trial pipeline.","title":"DomainEventKind"},{"location":"events-py/api/#events_py.models.EventEnvelope","text":"Bases: TypedDict Base event envelope for Pub/Sub or internal events.","title":"EventEnvelope"},{"location":"events-py/api/#events_py.models.DomainEventEnvelope","text":"Bases: TypedDict Domain event envelope for transactional outbox pattern.","title":"DomainEventEnvelope"},{"location":"events-py/api/#events_py.models-functions","text":"","title":"Functions"},{"location":"events-py/api/#events_py.models.create_event","text":"create_event ( kind : EventKind , payload : Any , event_id : str | None = None , ) -> EventEnvelope Create an event envelope. Parameters: Name Type Description Default kind EventKind Event kind (created, updated, deleted). required payload Any Event payload (must be JSON-serializable). required event_id str | None Optional ID; if omitted, a UUID is generated. None Returns: Type Description EventEnvelope Event envelope dict. Example from events_py import EventKind, create_event ev = create_event(EventKind.CREATED, {\"name\": \"foo\"}) ev[\"kind\"] 'created' Source code in libs/events-py/src/events_py/models.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def create_event ( kind : EventKind , payload : Any , event_id : str | None = None , ) -> EventEnvelope : \"\"\"Create an event envelope. Args: kind: Event kind (created, updated, deleted). payload: Event payload (must be JSON-serializable). event_id: Optional ID; if omitted, a UUID is generated. Returns: Event envelope dict. Example: >>> from events_py import EventKind, create_event >>> ev = create_event(EventKind.CREATED, {\"name\": \"foo\"}) >>> ev[\"kind\"] 'created' \"\"\" return { \"id\" : event_id or str ( uuid4 ()), \"kind\" : kind . value , \"payload\" : payload , \"timestamp\" : datetime . now ( timezone . utc ) . isoformat (), }","title":"create_event"},{"location":"events-py/api/#events_py.models.create_domain_event","text":"create_domain_event ( event_type : DomainEventKind , aggregate_type : str , aggregate_id : str , payload : Any , idempotency_key : str | None = None , event_id : str | None = None , ) -> DomainEventEnvelope Create a domain event envelope for the transactional outbox. Parameters: Name Type Description Default event_type DomainEventKind Domain event kind. required aggregate_type str Type of aggregate (e.g. \"protocol\"). required aggregate_id str ID of the affected aggregate. required payload Any Event payload (must be JSON-serializable). required idempotency_key str | None Optional dedup key; auto-generated if omitted. None event_id str | None Optional event ID; auto-generated if omitted. None Returns: Type Description DomainEventEnvelope Domain event envelope dict. Source code in libs/events-py/src/events_py/models.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def create_domain_event ( event_type : DomainEventKind , aggregate_type : str , aggregate_id : str , payload : Any , idempotency_key : str | None = None , event_id : str | None = None , ) -> DomainEventEnvelope : \"\"\"Create a domain event envelope for the transactional outbox. Args: event_type: Domain event kind. aggregate_type: Type of aggregate (e.g. \"protocol\"). aggregate_id: ID of the affected aggregate. payload: Event payload (must be JSON-serializable). idempotency_key: Optional dedup key; auto-generated if omitted. event_id: Optional event ID; auto-generated if omitted. Returns: Domain event envelope dict. \"\"\" eid = event_id or str ( uuid4 ()) ikey = idempotency_key or ( f \" { event_type . value } : { aggregate_id } : { uuid4 () } \" ) return { \"id\" : eid , \"event_type\" : event_type . value , \"aggregate_type\" : aggregate_type , \"aggregate_id\" : aggregate_id , \"payload\" : payload , \"idempotency_key\" : ikey , \"timestamp\" : datetime . now ( timezone . utc ) . isoformat (), }","title":"create_domain_event"},{"location":"events-py/api/#events_py.outbox","text":"Outbox processor for publishing pending domain events. Implements the transactional outbox pattern with at-least-once delivery. The OutboxProcessor polls the outbox table for pending events and dispatches them to registered handlers. The persist_with_outbox helper enables atomic entity + event writes in a single database transaction.","title":"outbox"},{"location":"events-py/api/#events_py.outbox-classes","text":"","title":"Classes"},{"location":"events-py/api/#events_py.outbox.OutboxProcessor","text":"OutboxProcessor ( engine : Engine , handlers : dict [ str , list [ Callable [ ... , Any ]]] | None = None , poll_interval : float = 1.0 , batch_size : int = 100 , ) Processes pending outbox events with at-least-once delivery. Usage processor = OutboxProcessor(engine, handlers={ \"protocol_uploaded\": [handle_protocol_uploaded], }) await processor.start() # Runs polling loop await processor.stop() # Graceful shutdown Initialize the outbox processor. Parameters: Name Type Description Default engine Engine SQLAlchemy database engine. required handlers dict [ str , list [ Callable [..., Any ]]] | None Maps event_type string to handler callbacks. None poll_interval float Seconds between polls. 1.0 batch_size int Max events per poll cycle. 100 Source code in libs/events-py/src/events_py/outbox.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , engine : Engine , handlers : dict [ str , list [ Callable [ ... , Any ]]] | None = None , poll_interval : float = 1.0 , batch_size : int = 100 , ) -> None : \"\"\"Initialize the outbox processor. Args: engine: SQLAlchemy database engine. handlers: Maps event_type string to handler callbacks. poll_interval: Seconds between polls. batch_size: Max events per poll cycle. \"\"\" self . engine = engine self . handlers : dict [ str , list [ Callable [ ... , Any ]]] = handlers or {} self . poll_interval = poll_interval self . batch_size = batch_size # Dedicated thread pool so pipeline handlers don't starve # FastAPI's default executor (used for sync endpoints). self . _executor = ThreadPoolExecutor ( max_workers = 2 , thread_name_prefix = \"outbox\" ) self . _shutdown_event = asyncio . Event () Functions \u00b6 poll_and_process \u00b6 poll_and_process () -> int Poll for pending events and process them. Returns: Type Description int Number of events processed in this poll cycle. Source code in libs/events-py/src/events_py/outbox.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def poll_and_process ( self ) -> int : \"\"\"Poll for pending events and process them. Returns: Number of events processed in this poll cycle. \"\"\" processed = 0 with Session ( self . engine ) as session : statement = ( select ( OutboxEvent ) . where ( col ( OutboxEvent . status ) . in_ ([ \"pending\" , \"failed\" ])) . order_by ( OutboxEvent . created_at . asc ()) # type: ignore[attr-defined] . limit ( self . batch_size ) ) # Use FOR UPDATE SKIP LOCKED on PostgreSQL # for concurrent processor safety db_url = str ( self . engine . url ) if \"postgresql\" in db_url : statement = statement . with_for_update ( skip_locked = True ) events = session . exec ( statement ) . all () for event in events : event_handlers = self . handlers . get ( event . event_type , []) if not event_handlers : logger . debug ( \"No handlers for event type: %s \" , event . event_type , ) try : for handler in event_handlers : handler ( event . payload ) event . status = \"published\" event . published_at = datetime . now ( timezone . utc ) session . add ( event ) processed += 1 logger . info ( \"Published event %s (type= %s )\" , event . id , event . event_type , ) except Exception : event . retry_count += 1 if event . retry_count >= MAX_RETRIES : event . status = \"dead_letter\" logger . warning ( \"Event %s exhausted retries ( %d ), marking as dead_letter\" , event . id , event . retry_count , ) # Update protocol status to dead_letter if protocol event if event . aggregate_type == \"protocol\" : from shared.models import Protocol protocol = session . get ( Protocol , event . aggregate_id ) if protocol : protocol . status = \"dead_letter\" protocol . error_reason = \"Maximum retries exceeded\" protocol . metadata_ = { ** protocol . metadata_ , \"error\" : { \"category\" : \"pipeline_failed\" , \"reason\" : \"Maximum retries exceeded\" , \"retry_count\" : event . retry_count , }, } session . add ( protocol ) else : event . status = \"failed\" session . add ( event ) logger . exception ( \"Failed to process event %s (type= %s , retry_count= %d )\" , event . id , event . event_type , event . retry_count , ) session . commit () return processed start async \u00b6 start () -> None Run the polling loop until stop() is called. Source code in libs/events-py/src/events_py/outbox.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 async def start ( self ) -> None : \"\"\"Run the polling loop until stop() is called.\"\"\" logger . info ( \"Outbox processor started (interval= %.1f s, batch_size= %d )\" , self . poll_interval , self . batch_size , ) while not self . _shutdown_event . is_set (): try : # Copy context so MLflow ContextVars propagate to the # worker thread, avoiding \"created in a different Context\" # warnings when autologging resets tokens. loop = asyncio . get_event_loop () ctx = contextvars . copy_context () count = await loop . run_in_executor ( self . _executor , ctx . run , self . poll_and_process ) if count > 0 : logger . info ( \"Processed %d outbox events\" , count ) except Exception : logger . exception ( \"Error in outbox processor poll cycle\" ) try : await asyncio . wait_for ( self . _shutdown_event . wait (), timeout = self . poll_interval , ) except asyncio . TimeoutError : pass logger . info ( \"Outbox processor stopped\" ) stop async \u00b6 stop () -> None Signal the processor to stop after current poll. Source code in libs/events-py/src/events_py/outbox.py 182 183 184 185 186 async def stop ( self ) -> None : \"\"\"Signal the processor to stop after current poll.\"\"\" logger . info ( \"Outbox processor shutdown requested\" ) self . _shutdown_event . set () self . _executor . shutdown ( wait = True )","title":"OutboxProcessor"},{"location":"events-py/api/#events_py.outbox-functions","text":"","title":"Functions"},{"location":"events-py/api/#events_py.outbox.persist_with_outbox","text":"persist_with_outbox ( session : Session , entity : Any , event_type : DomainEventKind , aggregate_type : str , aggregate_id : str , payload : dict [ str , Any ], idempotency_key : str | None = None , ) -> OutboxEvent Persist an entity and its outbox event in the same transaction. The caller is responsible for committing the session. Parameters: Name Type Description Default session Session Active SQLModel session (not yet committed). required entity Any The SQLModel entity to persist. required event_type DomainEventKind Domain event kind. required aggregate_type str Type of aggregate (e.g. \"protocol\"). required aggregate_id str ID of the affected aggregate. required payload dict [ str , Any ] Event payload dict. required idempotency_key str | None Dedup key; auto-generated if omitted. None Returns: Type Description OutboxEvent The created OutboxEvent (uncommitted). Source code in libs/events-py/src/events_py/outbox.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def persist_with_outbox ( session : Session , entity : Any , event_type : DomainEventKind , aggregate_type : str , aggregate_id : str , payload : dict [ str , Any ], idempotency_key : str | None = None , ) -> OutboxEvent : \"\"\"Persist an entity and its outbox event in the same transaction. The caller is responsible for committing the session. Args: session: Active SQLModel session (not yet committed). entity: The SQLModel entity to persist. event_type: Domain event kind. aggregate_type: Type of aggregate (e.g. \"protocol\"). aggregate_id: ID of the affected aggregate. payload: Event payload dict. idempotency_key: Dedup key; auto-generated if omitted. Returns: The created OutboxEvent (uncommitted). \"\"\" ikey = idempotency_key or ( f \" { event_type . value } : { aggregate_id } : { uuid4 () } \" ) outbox_event = OutboxEvent ( event_type = event_type . value , aggregate_type = aggregate_type , aggregate_id = aggregate_id , payload = payload , idempotency_key = ikey , status = \"pending\" , ) session . add ( entity ) session . add ( outbox_event ) return outbox_event","title":"persist_with_outbox"},{"location":"hitl-ui/api/","text":"API Reference for hitl-ui \u00b6","title":"API Reference"},{"location":"hitl-ui/api/#api-reference-for-hitl-ui","text":"","title":"API Reference for hitl-ui"},{"location":"inference/api/","text":"API Reference for inference \u00b6","title":"API Reference"},{"location":"inference/api/#api-reference-for-inference","text":"","title":"API Reference for inference"},{"location":"journeys/","text":"User Journeys \u00b6 End-to-end workflows through the system, from protocol upload to structured export. Journey Map \u00b6 Journey Actors Outcome Upload & Extraction Researcher \u2192 System PDF becomes structured criteria in DB Grounding, Structuring & HITL Review System \u2192 Clinician Criteria get coded, structured, and reviewed The Big Picture \u00b6 A protocol PDF moves through these stages: Upload \u2192 Extract \u2192 Parse \u2192 Ground \u2192 Persist \u2192 Structure \u2192 Ordinal Resolve \u2192 Review \u2192 Export (UI) (Gemini) (DB) (UMLS) (DB) (Gemini) (Gemini) (HITL) (API) Each stage is a LangGraph node in protocol-processor-service , except Upload (API endpoint), Review (UI), and Export (API endpoint).","title":"Overview"},{"location":"journeys/#user-journeys","text":"End-to-end workflows through the system, from protocol upload to structured export.","title":"User Journeys"},{"location":"journeys/#journey-map","text":"Journey Actors Outcome Upload & Extraction Researcher \u2192 System PDF becomes structured criteria in DB Grounding, Structuring & HITL Review System \u2192 Clinician Criteria get coded, structured, and reviewed","title":"Journey Map"},{"location":"journeys/#the-big-picture","text":"A protocol PDF moves through these stages: Upload \u2192 Extract \u2192 Parse \u2192 Ground \u2192 Persist \u2192 Structure \u2192 Ordinal Resolve \u2192 Review \u2192 Export (UI) (Gemini) (DB) (UMLS) (DB) (Gemini) (Gemini) (HITL) (API) Each stage is a LangGraph node in protocol-processor-service , except Upload (API endpoint), Review (UI), and Export (API endpoint).","title":"The Big Picture"},{"location":"journeys/grounding-review/","text":"Grounding, Structuring & HITL Review \u00b6 How extracted entities get linked to standard terminologies, structured into expression trees, and reviewed by clinicians. Sequence Diagram \u00b6 sequenceDiagram participant PA as parse node participant GR as ground node participant UMLS as UMLS API participant OMOP as OMOP Mapper participant PE as persist node participant ST as structure node participant OR as ordinal_resolve node participant DB as PostgreSQL actor Clinician as Clinician participant UI as hitl-ui PA->>GR: state with entities_json loop For each entity (semaphore=4) GR->>UMLS: TerminologyRouter search UMLS-->>GR: SNOMED/LOINC/RxNorm codes GR->>OMOP: OMOP concept lookup OMOP-->>GR: concept_id + domain GR->>GR: reconcile_dual_grounding() opt Confidence < 0.5 GR->>GR: Agentic retry (MedGemma reasoning, max 3 attempts) end end GR->>PE: state with grounded_entities_json PE->>DB: INSERT Entity records PE->>DB: UPDATE Criteria.conditions.field_mappings PE->>DB: UPDATE protocol.status PE->>ST: state with batch_id ST->>DB: SELECT criteria with field_mappings loop For each criterion (semaphore=4) ST->>ST: Gemini detect_logic_structure() ST->>DB: INSERT AtomicCriterion + CompositeCriterion ST->>DB: INSERT CriterionRelationship edges ST->>DB: UPDATE Criteria.structured_criterion (JSON tree) end ST->>OR: state OR->>DB: SELECT atomics with null unit_concept_id OR->>OR: Gemini ordinal scale detection OR->>DB: UPDATE unit_concept_id = 8527 (ordinal) OR->>DB: INSERT AuditLog proposals Note over DB,UI: Protocol now in pending_review Clinician->>UI: Open Review Page UI->>DB: GET /reviews/batches/{id}/criteria UI-->>Clinician: Show criteria split by inclusion/exclusion loop For each criterion Clinician->>UI: Approve / Reject / Modify UI->>DB: POST /reviews/criteria/{id}/action end Grounding Phase (ground node) \u00b6 The ground node processes each entity through a dual-grounding pipeline: Dual Grounding \u00b6 TerminologyRouter \u2014 routes entities to the appropriate UMLS source vocabulary (SNOMED for conditions, LOINC for labs, RxNorm for drugs) OMOP Mapper \u2014 resolves entities to OMOP CDM concept IDs for data warehouse joins Both run in parallel per entity (asyncio.gather), results reconciled in _reconcile_dual_grounding() . Agentic Retry Loop \u00b6 When initial grounding confidence is below 0.5, the ground node enters an agentic reasoning loop (max 3 attempts): Asks MedGemma: \"Is this entity valid for coding?\" Asks: \"Can we derive a parent/broader concept?\" Asks: \"Can you rephrase for better matching?\" Retries grounding with improved input If all attempts fail, the entity is marked for expert review. File : services/protocol-processor-service/src/protocol_processor/nodes/ground.py Error Accumulation \u00b6 Individual entity failures are collected in state[\"errors\"] but don't stop the pipeline. The persist node decides the final status: All entities succeed \u2192 pending_review Some entities fail \u2192 pending_review (with errors logged) All entities fail \u2192 grounding_failed Persist Phase (persist node) \u00b6 Creates Entity records in the database and updates Criteria.conditions.field_mappings with grounding results. Also handles review inheritance during re-extraction. File : services/protocol-processor-service/src/protocol_processor/nodes/persist.py Structure Phase (structure node) \u00b6 Builds expression trees for criteria that have field_mappings in their conditions JSON: Queries criteria from the current batch For each qualifying criterion, calls Gemini to detect logic structure (AND/OR/NOT) Creates AtomicCriterion leaf records with OMOP concept IDs, operators, values Creates CompositeCriterion branch records with logic operators Creates CriterionRelationship edges linking parents to children Stores the expression tree as structured_criterion JSON on the Criteria record File : services/protocol-processor-service/src/protocol_processor/nodes/structure.py Expression Tree Example \u00b6 For the criterion \"HbA1c between 7.0% and 10.0%\": { \"type\" : \"AND\" , \"children\" : [ { \"type\" : \"ATOMIC\" , \"atomic_criterion_id\" : \"uuid-1\" , \"details\" : \"HbA1c >= 7.0%\" }, { \"type\" : \"ATOMIC\" , \"atomic_criterion_id\" : \"uuid-2\" , \"details\" : \"HbA1c <= 10.0%\" } ] } This maps to: 1 CompositeCriterion (operator=AND) 2 AtomicCriterion records (one for each bound) 2 CriterionRelationship edges Ordinal Resolution Phase (ordinal_resolve node) \u00b6 Identifies atomic criteria that represent ordinal scales (e.g., NYHA class, ECOG score) and sets their unit_concept_id to 8527 (OMOP concept for ordinal). File : services/protocol-processor-service/src/protocol_processor/nodes/ordinal_resolve.py HITL Review (ReviewPage.tsx) \u00b6 The clinician reviews extracted criteria in a split-pane view: Left panel : PDF viewer (highlights source text for selected criterion) Right panel : Criteria list grouped by inclusion/exclusion, with filters for status, type, and confidence For each criterion, the clinician can: Approve \u2014 confirms the AI extraction is correct Reject \u2014 marks as incorrect Modify \u2014 provides corrections Review decisions are stored as Review records with before/after JSON for audit trail. File : apps/hitl-ui/src/screens/ReviewPage.tsx Exports \u00b6 Once criteria are reviewed, they can be exported in three formats: Format Endpoint Use case CIRCE JSON GET /exports/{id}/circe OHDSI cohort definition tools (ATLAS) FHIR R4 Group GET /exports/{id}/fhir-group FHIR-based EHR systems OMOP SQL GET /exports/{id}/evaluation-sql Direct CDM database queries All export builders consume the same ProtocolExportData dataclass, which loads the expression tree (atomics, composites, relationships) from the database. Files : services/api-service/src/api_service/exports.py services/api-service/src/api_service/exporters/circe_builder.py services/api-service/src/api_service/exporters/fhir_group_builder.py services/api-service/src/api_service/exporters/evaluation_sql_builder.py","title":"Grounding, Structuring & HITL Review"},{"location":"journeys/grounding-review/#grounding-structuring-hitl-review","text":"How extracted entities get linked to standard terminologies, structured into expression trees, and reviewed by clinicians.","title":"Grounding, Structuring &amp; HITL Review"},{"location":"journeys/grounding-review/#sequence-diagram","text":"sequenceDiagram participant PA as parse node participant GR as ground node participant UMLS as UMLS API participant OMOP as OMOP Mapper participant PE as persist node participant ST as structure node participant OR as ordinal_resolve node participant DB as PostgreSQL actor Clinician as Clinician participant UI as hitl-ui PA->>GR: state with entities_json loop For each entity (semaphore=4) GR->>UMLS: TerminologyRouter search UMLS-->>GR: SNOMED/LOINC/RxNorm codes GR->>OMOP: OMOP concept lookup OMOP-->>GR: concept_id + domain GR->>GR: reconcile_dual_grounding() opt Confidence < 0.5 GR->>GR: Agentic retry (MedGemma reasoning, max 3 attempts) end end GR->>PE: state with grounded_entities_json PE->>DB: INSERT Entity records PE->>DB: UPDATE Criteria.conditions.field_mappings PE->>DB: UPDATE protocol.status PE->>ST: state with batch_id ST->>DB: SELECT criteria with field_mappings loop For each criterion (semaphore=4) ST->>ST: Gemini detect_logic_structure() ST->>DB: INSERT AtomicCriterion + CompositeCriterion ST->>DB: INSERT CriterionRelationship edges ST->>DB: UPDATE Criteria.structured_criterion (JSON tree) end ST->>OR: state OR->>DB: SELECT atomics with null unit_concept_id OR->>OR: Gemini ordinal scale detection OR->>DB: UPDATE unit_concept_id = 8527 (ordinal) OR->>DB: INSERT AuditLog proposals Note over DB,UI: Protocol now in pending_review Clinician->>UI: Open Review Page UI->>DB: GET /reviews/batches/{id}/criteria UI-->>Clinician: Show criteria split by inclusion/exclusion loop For each criterion Clinician->>UI: Approve / Reject / Modify UI->>DB: POST /reviews/criteria/{id}/action end","title":"Sequence Diagram"},{"location":"journeys/grounding-review/#grounding-phase-ground-node","text":"The ground node processes each entity through a dual-grounding pipeline:","title":"Grounding Phase (ground node)"},{"location":"journeys/grounding-review/#dual-grounding","text":"TerminologyRouter \u2014 routes entities to the appropriate UMLS source vocabulary (SNOMED for conditions, LOINC for labs, RxNorm for drugs) OMOP Mapper \u2014 resolves entities to OMOP CDM concept IDs for data warehouse joins Both run in parallel per entity (asyncio.gather), results reconciled in _reconcile_dual_grounding() .","title":"Dual Grounding"},{"location":"journeys/grounding-review/#agentic-retry-loop","text":"When initial grounding confidence is below 0.5, the ground node enters an agentic reasoning loop (max 3 attempts): Asks MedGemma: \"Is this entity valid for coding?\" Asks: \"Can we derive a parent/broader concept?\" Asks: \"Can you rephrase for better matching?\" Retries grounding with improved input If all attempts fail, the entity is marked for expert review. File : services/protocol-processor-service/src/protocol_processor/nodes/ground.py","title":"Agentic Retry Loop"},{"location":"journeys/grounding-review/#error-accumulation","text":"Individual entity failures are collected in state[\"errors\"] but don't stop the pipeline. The persist node decides the final status: All entities succeed \u2192 pending_review Some entities fail \u2192 pending_review (with errors logged) All entities fail \u2192 grounding_failed","title":"Error Accumulation"},{"location":"journeys/grounding-review/#persist-phase-persist-node","text":"Creates Entity records in the database and updates Criteria.conditions.field_mappings with grounding results. Also handles review inheritance during re-extraction. File : services/protocol-processor-service/src/protocol_processor/nodes/persist.py","title":"Persist Phase (persist node)"},{"location":"journeys/grounding-review/#structure-phase-structure-node","text":"Builds expression trees for criteria that have field_mappings in their conditions JSON: Queries criteria from the current batch For each qualifying criterion, calls Gemini to detect logic structure (AND/OR/NOT) Creates AtomicCriterion leaf records with OMOP concept IDs, operators, values Creates CompositeCriterion branch records with logic operators Creates CriterionRelationship edges linking parents to children Stores the expression tree as structured_criterion JSON on the Criteria record File : services/protocol-processor-service/src/protocol_processor/nodes/structure.py","title":"Structure Phase (structure node)"},{"location":"journeys/grounding-review/#expression-tree-example","text":"For the criterion \"HbA1c between 7.0% and 10.0%\": { \"type\" : \"AND\" , \"children\" : [ { \"type\" : \"ATOMIC\" , \"atomic_criterion_id\" : \"uuid-1\" , \"details\" : \"HbA1c >= 7.0%\" }, { \"type\" : \"ATOMIC\" , \"atomic_criterion_id\" : \"uuid-2\" , \"details\" : \"HbA1c <= 10.0%\" } ] } This maps to: 1 CompositeCriterion (operator=AND) 2 AtomicCriterion records (one for each bound) 2 CriterionRelationship edges","title":"Expression Tree Example"},{"location":"journeys/grounding-review/#ordinal-resolution-phase-ordinal_resolve-node","text":"Identifies atomic criteria that represent ordinal scales (e.g., NYHA class, ECOG score) and sets their unit_concept_id to 8527 (OMOP concept for ordinal). File : services/protocol-processor-service/src/protocol_processor/nodes/ordinal_resolve.py","title":"Ordinal Resolution Phase (ordinal_resolve node)"},{"location":"journeys/grounding-review/#hitl-review-reviewpagetsx","text":"The clinician reviews extracted criteria in a split-pane view: Left panel : PDF viewer (highlights source text for selected criterion) Right panel : Criteria list grouped by inclusion/exclusion, with filters for status, type, and confidence For each criterion, the clinician can: Approve \u2014 confirms the AI extraction is correct Reject \u2014 marks as incorrect Modify \u2014 provides corrections Review decisions are stored as Review records with before/after JSON for audit trail. File : apps/hitl-ui/src/screens/ReviewPage.tsx","title":"HITL Review (ReviewPage.tsx)"},{"location":"journeys/grounding-review/#exports","text":"Once criteria are reviewed, they can be exported in three formats: Format Endpoint Use case CIRCE JSON GET /exports/{id}/circe OHDSI cohort definition tools (ATLAS) FHIR R4 Group GET /exports/{id}/fhir-group FHIR-based EHR systems OMOP SQL GET /exports/{id}/evaluation-sql Direct CDM database queries All export builders consume the same ProtocolExportData dataclass, which loads the expression tree (atomics, composites, relationships) from the database. Files : services/api-service/src/api_service/exports.py services/api-service/src/api_service/exporters/circe_builder.py services/api-service/src/api_service/exporters/fhir_group_builder.py services/api-service/src/api_service/exporters/evaluation_sql_builder.py","title":"Exports"},{"location":"journeys/upload-extraction/","text":"Upload & Extraction Journey \u00b6 How a protocol PDF goes from the researcher's browser to structured criteria in the database. Sequence Diagram \u00b6 sequenceDiagram actor User as Researcher participant UI as hitl-ui participant API as api-service participant DB as PostgreSQL participant GCS as Storage (GCS/local) participant OBX as OutboxProcessor participant IG as ingest node participant EX as extract node participant PA as parse node User->>UI: Drag & drop protocol PDF UI->>API: POST /protocols/upload {filename, size} API->>DB: INSERT Protocol (status=uploaded) API->>GCS: Generate signed upload URL API-->>UI: {protocol_id, upload_url} UI->>GCS: PUT file to signed URL UI->>API: POST /protocols/{id}/confirm-upload {pdf_bytes_base64} API->>API: compute_quality_score(pdf_bytes) API->>DB: persist_with_outbox(protocol, PROTOCOL_UPLOADED) API-->>UI: ProtocolResponse Note over OBX,DB: OutboxProcessor polls every 2s OBX->>DB: SELECT pending outbox events OBX->>IG: handle_protocol_uploaded(payload) IG->>GCS: fetch_pdf_bytes(file_uri) GCS-->>IG: raw PDF bytes IG->>DB: UPDATE protocol.status = extracting IG->>EX: state with pdf_bytes EX->>EX: Gemini structured output extraction Note over EX: Sends PDF to Gemini 2.5 Flash<br/>with JSON schema for criteria EX->>PA: state with extraction_json PA->>DB: INSERT CriteriaBatch + Criteria records PA->>PA: decompose_entities_from_criterion() x N Note over PA: Async entity decomposition<br/>using Gemini (semaphore=4) PA->>PA: state with entities_json, batch_id Step-by-Step \u00b6 1. Upload Dialog ( ProtocolUploadDialog.tsx ) \u00b6 The user opens the upload dialog in the HITL UI. Client-side validation enforces: PDF content type only 50 MB maximum file size File : apps/hitl-ui/src/components/ProtocolUploadDialog.tsx 2. Signed URL Generation ( protocols.py ) \u00b6 The API creates a Protocol record (status: uploaded ) and returns a signed URL for direct browser-to-GCS upload. If the Gemini circuit breaker is open, the protocol is marked pending with a warning. File : services/api-service/src/api_service/protocols.py:136 3. Upload Confirmation ( protocols.py ) \u00b6 After the browser uploads the file, it calls confirm-upload . This endpoint: Optionally decodes pdf_bytes_base64 and computes a quality score (page count, text extractability) Stores quality metadata on the GCS blob Writes the PROTOCOL_UPLOADED outbox event in the same transaction as the protocol update File : services/api-service/src/api_service/protocols.py:205 4. Outbox Processing ( events-py/outbox.py ) \u00b6 The OutboxProcessor runs as a background task inside api-service . It polls for pending events and dispatches them to registered handlers in a thread executor (to avoid blocking FastAPI). File : libs/events-py/src/events_py/outbox.py 5. Ingest Node ( nodes/ingest.py ) \u00b6 First pipeline node. Fetches the raw PDF bytes from GCS (or local storage) and updates the protocol status to extracting . File : services/protocol-processor-service/src/protocol_processor/nodes/ingest.py 6. Extract Node ( nodes/extract.py ) \u00b6 Sends the PDF to Gemini 2.5 Flash with a structured output schema requesting: Criteria text (inclusion/exclusion) Category classification Confidence scores Returns a JSON string of extracted criteria. The PDF bytes are cleared from state after this step to reduce checkpoint size. File : services/protocol-processor-service/src/protocol_processor/nodes/extract.py 7. Parse Node ( nodes/parse.py ) \u00b6 Converts extraction JSON into database records: Phase A : Creates CriteriaBatch and Criteria records in the database Phase B : Runs decompose_entities_from_criterion() concurrently (asyncio.gather with semaphore=4) to identify medical entities within each criterion Produces entities_json in state for the grounding phase. File : services/protocol-processor-service/src/protocol_processor/nodes/parse.py Error Handling \u00b6 Failure Response Invalid file type 400 error at upload File too large (>50MB) 400 error at upload PDF quality too low Pipeline may fail at extract with extraction_failed Gemini API down Circuit breaker opens; protocol queued as pending Extraction LLM error extraction_failed status; retryable via /retry endpoint Re-Extraction \u00b6 The POST /protocols/{id}/re-extract endpoint allows re-running extraction without re-uploading: Archives all existing non-archived batches (preserves review history) Collects reviewed criteria from archived batches for fuzzy-match inheritance Triggers the same pipeline via a new PROTOCOL_UPLOADED outbox event New batch criteria inherit review decisions from archived criteria (>90% text match, same type) File : services/api-service/src/api_service/protocols.py:360","title":"Upload & Extraction"},{"location":"journeys/upload-extraction/#upload-extraction-journey","text":"How a protocol PDF goes from the researcher's browser to structured criteria in the database.","title":"Upload &amp; Extraction Journey"},{"location":"journeys/upload-extraction/#sequence-diagram","text":"sequenceDiagram actor User as Researcher participant UI as hitl-ui participant API as api-service participant DB as PostgreSQL participant GCS as Storage (GCS/local) participant OBX as OutboxProcessor participant IG as ingest node participant EX as extract node participant PA as parse node User->>UI: Drag & drop protocol PDF UI->>API: POST /protocols/upload {filename, size} API->>DB: INSERT Protocol (status=uploaded) API->>GCS: Generate signed upload URL API-->>UI: {protocol_id, upload_url} UI->>GCS: PUT file to signed URL UI->>API: POST /protocols/{id}/confirm-upload {pdf_bytes_base64} API->>API: compute_quality_score(pdf_bytes) API->>DB: persist_with_outbox(protocol, PROTOCOL_UPLOADED) API-->>UI: ProtocolResponse Note over OBX,DB: OutboxProcessor polls every 2s OBX->>DB: SELECT pending outbox events OBX->>IG: handle_protocol_uploaded(payload) IG->>GCS: fetch_pdf_bytes(file_uri) GCS-->>IG: raw PDF bytes IG->>DB: UPDATE protocol.status = extracting IG->>EX: state with pdf_bytes EX->>EX: Gemini structured output extraction Note over EX: Sends PDF to Gemini 2.5 Flash<br/>with JSON schema for criteria EX->>PA: state with extraction_json PA->>DB: INSERT CriteriaBatch + Criteria records PA->>PA: decompose_entities_from_criterion() x N Note over PA: Async entity decomposition<br/>using Gemini (semaphore=4) PA->>PA: state with entities_json, batch_id","title":"Sequence Diagram"},{"location":"journeys/upload-extraction/#step-by-step","text":"","title":"Step-by-Step"},{"location":"journeys/upload-extraction/#1-upload-dialog-protocoluploaddialogtsx","text":"The user opens the upload dialog in the HITL UI. Client-side validation enforces: PDF content type only 50 MB maximum file size File : apps/hitl-ui/src/components/ProtocolUploadDialog.tsx","title":"1. Upload Dialog (ProtocolUploadDialog.tsx)"},{"location":"journeys/upload-extraction/#2-signed-url-generation-protocolspy","text":"The API creates a Protocol record (status: uploaded ) and returns a signed URL for direct browser-to-GCS upload. If the Gemini circuit breaker is open, the protocol is marked pending with a warning. File : services/api-service/src/api_service/protocols.py:136","title":"2. Signed URL Generation (protocols.py)"},{"location":"journeys/upload-extraction/#3-upload-confirmation-protocolspy","text":"After the browser uploads the file, it calls confirm-upload . This endpoint: Optionally decodes pdf_bytes_base64 and computes a quality score (page count, text extractability) Stores quality metadata on the GCS blob Writes the PROTOCOL_UPLOADED outbox event in the same transaction as the protocol update File : services/api-service/src/api_service/protocols.py:205","title":"3. Upload Confirmation (protocols.py)"},{"location":"journeys/upload-extraction/#4-outbox-processing-events-pyoutboxpy","text":"The OutboxProcessor runs as a background task inside api-service . It polls for pending events and dispatches them to registered handlers in a thread executor (to avoid blocking FastAPI). File : libs/events-py/src/events_py/outbox.py","title":"4. Outbox Processing (events-py/outbox.py)"},{"location":"journeys/upload-extraction/#5-ingest-node-nodesingestpy","text":"First pipeline node. Fetches the raw PDF bytes from GCS (or local storage) and updates the protocol status to extracting . File : services/protocol-processor-service/src/protocol_processor/nodes/ingest.py","title":"5. Ingest Node (nodes/ingest.py)"},{"location":"journeys/upload-extraction/#6-extract-node-nodesextractpy","text":"Sends the PDF to Gemini 2.5 Flash with a structured output schema requesting: Criteria text (inclusion/exclusion) Category classification Confidence scores Returns a JSON string of extracted criteria. The PDF bytes are cleared from state after this step to reduce checkpoint size. File : services/protocol-processor-service/src/protocol_processor/nodes/extract.py","title":"6. Extract Node (nodes/extract.py)"},{"location":"journeys/upload-extraction/#7-parse-node-nodesparsepy","text":"Converts extraction JSON into database records: Phase A : Creates CriteriaBatch and Criteria records in the database Phase B : Runs decompose_entities_from_criterion() concurrently (asyncio.gather with semaphore=4) to identify medical entities within each criterion Produces entities_json in state for the grounding phase. File : services/protocol-processor-service/src/protocol_processor/nodes/parse.py","title":"7. Parse Node (nodes/parse.py)"},{"location":"journeys/upload-extraction/#error-handling","text":"Failure Response Invalid file type 400 error at upload File too large (>50MB) 400 error at upload PDF quality too low Pipeline may fail at extract with extraction_failed Gemini API down Circuit breaker opens; protocol queued as pending Extraction LLM error extraction_failed status; retryable via /retry endpoint","title":"Error Handling"},{"location":"journeys/upload-extraction/#re-extraction","text":"The POST /protocols/{id}/re-extract endpoint allows re-running extraction without re-uploading: Archives all existing non-archived batches (preserves review history) Collects reviewed criteria from archived batches for fuzzy-match inheritance Triggers the same pipeline via a new PROTOCOL_UPLOADED outbox event New batch criteria inherit review decisions from archived criteria (>90% text match, same type) File : services/api-service/src/api_service/protocols.py:360","title":"Re-Extraction"},{"location":"model-training/api/","text":"API Reference for model-training \u00b6","title":"API Reference"},{"location":"model-training/api/#api-reference-for-model-training","text":"","title":"API Reference for model-training"},{"location":"protocol-processor-service/api/","text":"API Reference for protocol-processor-service \u00b6","title":"API Reference"},{"location":"protocol-processor-service/api/#api-reference-for-protocol-processor-service","text":"","title":"API Reference for protocol-processor-service"},{"location":"shared/api/","text":"API Reference for shared \u00b6","title":"API Reference"},{"location":"shared/api/#api-reference-for-shared","text":"","title":"API Reference for shared"},{"location":"status/","text":"Implementation Status \u00b6 Current state of ElixirTrials components as of the feature/major-refactor-langgraph branch. Maturity Matrix \u00b6 Component Status Notes PDF upload + quality scoring Production-ready Signed URL flow, client-side validation, quality analysis Gemini criteria extraction Production-ready Structured output with Gemini 2.5 Flash Entity decomposition (parse) Production-ready Concurrent async decomposition UMLS grounding (TerminologyRouter) Production-ready Multi-vocab routing with caching OMOP concept mapping Production-ready Dual grounding with reconciliation Agentic retry loop Experimental MedGemma reasoning for low-confidence entities Expression tree structuring Production-ready Gemini logic detection, atomic/composite model Ordinal scale resolution Experimental Gemini-based detection, proposals for review HITL review UI Production-ready Split-pane PDF + criteria, filtering, audit trail Re-extraction with review inheritance Production-ready Fuzzy matching (>90% threshold) CIRCE export Production-ready Full expression tree walking FHIR R4 Group export Production-ready Full expression tree walking OMOP evaluation SQL export Production-ready (limited) Flat AND/OR model only (see known limitations) LangGraph checkpointing Production-ready AsyncPostgresSaver with retry support Outbox event processing Production-ready At-least-once delivery, dead letter handling Google OAuth Production-ready JWT session with protected routes MLflow observability Production-ready Per-node traces, orphan cleanup Circuit breaker (Gemini) Production-ready pybreaker-based with UI warning Known Limitations \u00b6 Evaluation SQL Builder (flat model) \u00b6 The OMOP CDM evaluation SQL builder uses a flat AND/OR model: all inclusion atomics are combined with AND, all exclusion atomics use NOT EXISTS. The expression tree's nested AND/OR/NOT structure is not respected by the SQL builder. For example, \"HbA1c >= 7% OR fasting glucose >= 126\" is evaluated as requiring both rather than either . The CIRCE and FHIR Group builders correctly walk the expression tree. File : services/api-service/src/api_service/exporters/evaluation_sql_builder.py:1-13 CompositeCriterion.parent_criterion_id \u00b6 The parent_criterion_id column on CompositeCriterion is intentionally unused by the automated pipeline. Tree parent-child relationships are stored in CriterionRelationship . This field is reserved for future HITL use (manual tree restructuring). No UI Tests \u00b6 The apps/hitl-ui directory has no test files. The React components are untested. Stale Workspace Members \u00b6 Several workspace members in pyproject.toml exist as directories but have minimal functionality: libs/data-pipeline \u2014 data loading helpers (limited) libs/evaluation \u2014 quality evaluation framework (limited) libs/inference \u2014 model inference utilities (limited) libs/model-training \u2014 fine-tuning scripts (limited) These are scaffolded for future use but not actively consumed by the pipeline. Test Coverage \u00b6 Area Test files Count Runner API endpoints services/api-service/tests/test_protocol_api.py ~15 tests pytest Review workflow services/api-service/tests/test_review_api.py ~10 tests pytest Auth services/api-service/tests/test_auth_required.py ~5 tests pytest Exports services/api-service/tests/test_exports.py 36 tests pytest Data integrity services/api-service/tests/test_integrity.py ~5 tests pytest Quality scoring services/api-service/tests/test_quality.py ~5 tests pytest Models/schemas services/api-service/tests/test_models.py , test_schemas.py ~10 tests pytest UMLS clients services/api-service/tests/test_umls_clients.py ~5 tests pytest Event contracts libs/events-py/tests/ ~5 tests pytest Pipeline nodes None 0 \u2014 UI components None 0 \u2014 Run all tests : make test Top 5 Risks for Maintainers \u00b6 1. Cross-Service Import Coupling \u00b6 Pipeline nodes ( protocol-processor-service ) directly import from api-service for database access ( from api_service.storage import engine ). This creates a tight runtime coupling \u2014 the processor cannot run without the api-service package on the Python path. Mitigation : The pyproject.toml pythonpath config handles this for dev. For production, both packages must be installed together. 2. No Pipeline Node Tests \u00b6 The 7 LangGraph nodes have zero test coverage. All testing is through the API layer. A change to a node could break the pipeline without any test failure. Mitigation : Add integration tests that invoke individual nodes with mock LLM responses. 3. Outbox Polling Latency \u00b6 The outbox processor polls every 2 seconds. Under load, this could create a backlog. There's no back-pressure mechanism. Mitigation : Monitor outbox table depth; consider reducing poll interval or switching to PostgreSQL LISTEN/NOTIFY. 4. Gemini API Rate Limits \u00b6 Multiple pipeline nodes call Gemini concurrently (semaphore=4 per node). A burst of protocol uploads could hit API rate limits. Mitigation : The circuit breaker ( pybreaker ) handles transient failures. Consider adding global rate limiting. 5. Expression Tree vs Flat SQL Mismatch \u00b6 The CIRCE and FHIR exporters walk the expression tree correctly, but the SQL exporter uses a flat model. Users may get inconsistent results across export formats. Mitigation : Document the limitation clearly (done). Future: implement tree-aware SQL generation using recursive CTE grouping.","title":"Overview"},{"location":"status/#implementation-status","text":"Current state of ElixirTrials components as of the feature/major-refactor-langgraph branch.","title":"Implementation Status"},{"location":"status/#maturity-matrix","text":"Component Status Notes PDF upload + quality scoring Production-ready Signed URL flow, client-side validation, quality analysis Gemini criteria extraction Production-ready Structured output with Gemini 2.5 Flash Entity decomposition (parse) Production-ready Concurrent async decomposition UMLS grounding (TerminologyRouter) Production-ready Multi-vocab routing with caching OMOP concept mapping Production-ready Dual grounding with reconciliation Agentic retry loop Experimental MedGemma reasoning for low-confidence entities Expression tree structuring Production-ready Gemini logic detection, atomic/composite model Ordinal scale resolution Experimental Gemini-based detection, proposals for review HITL review UI Production-ready Split-pane PDF + criteria, filtering, audit trail Re-extraction with review inheritance Production-ready Fuzzy matching (>90% threshold) CIRCE export Production-ready Full expression tree walking FHIR R4 Group export Production-ready Full expression tree walking OMOP evaluation SQL export Production-ready (limited) Flat AND/OR model only (see known limitations) LangGraph checkpointing Production-ready AsyncPostgresSaver with retry support Outbox event processing Production-ready At-least-once delivery, dead letter handling Google OAuth Production-ready JWT session with protected routes MLflow observability Production-ready Per-node traces, orphan cleanup Circuit breaker (Gemini) Production-ready pybreaker-based with UI warning","title":"Maturity Matrix"},{"location":"status/#known-limitations","text":"","title":"Known Limitations"},{"location":"status/#evaluation-sql-builder-flat-model","text":"The OMOP CDM evaluation SQL builder uses a flat AND/OR model: all inclusion atomics are combined with AND, all exclusion atomics use NOT EXISTS. The expression tree's nested AND/OR/NOT structure is not respected by the SQL builder. For example, \"HbA1c >= 7% OR fasting glucose >= 126\" is evaluated as requiring both rather than either . The CIRCE and FHIR Group builders correctly walk the expression tree. File : services/api-service/src/api_service/exporters/evaluation_sql_builder.py:1-13","title":"Evaluation SQL Builder (flat model)"},{"location":"status/#compositecriterionparent_criterion_id","text":"The parent_criterion_id column on CompositeCriterion is intentionally unused by the automated pipeline. Tree parent-child relationships are stored in CriterionRelationship . This field is reserved for future HITL use (manual tree restructuring).","title":"CompositeCriterion.parent_criterion_id"},{"location":"status/#no-ui-tests","text":"The apps/hitl-ui directory has no test files. The React components are untested.","title":"No UI Tests"},{"location":"status/#stale-workspace-members","text":"Several workspace members in pyproject.toml exist as directories but have minimal functionality: libs/data-pipeline \u2014 data loading helpers (limited) libs/evaluation \u2014 quality evaluation framework (limited) libs/inference \u2014 model inference utilities (limited) libs/model-training \u2014 fine-tuning scripts (limited) These are scaffolded for future use but not actively consumed by the pipeline.","title":"Stale Workspace Members"},{"location":"status/#test-coverage","text":"Area Test files Count Runner API endpoints services/api-service/tests/test_protocol_api.py ~15 tests pytest Review workflow services/api-service/tests/test_review_api.py ~10 tests pytest Auth services/api-service/tests/test_auth_required.py ~5 tests pytest Exports services/api-service/tests/test_exports.py 36 tests pytest Data integrity services/api-service/tests/test_integrity.py ~5 tests pytest Quality scoring services/api-service/tests/test_quality.py ~5 tests pytest Models/schemas services/api-service/tests/test_models.py , test_schemas.py ~10 tests pytest UMLS clients services/api-service/tests/test_umls_clients.py ~5 tests pytest Event contracts libs/events-py/tests/ ~5 tests pytest Pipeline nodes None 0 \u2014 UI components None 0 \u2014 Run all tests : make test","title":"Test Coverage"},{"location":"status/#top-5-risks-for-maintainers","text":"","title":"Top 5 Risks for Maintainers"},{"location":"status/#1-cross-service-import-coupling","text":"Pipeline nodes ( protocol-processor-service ) directly import from api-service for database access ( from api_service.storage import engine ). This creates a tight runtime coupling \u2014 the processor cannot run without the api-service package on the Python path. Mitigation : The pyproject.toml pythonpath config handles this for dev. For production, both packages must be installed together.","title":"1. Cross-Service Import Coupling"},{"location":"status/#2-no-pipeline-node-tests","text":"The 7 LangGraph nodes have zero test coverage. All testing is through the API layer. A change to a node could break the pipeline without any test failure. Mitigation : Add integration tests that invoke individual nodes with mock LLM responses.","title":"2. No Pipeline Node Tests"},{"location":"status/#3-outbox-polling-latency","text":"The outbox processor polls every 2 seconds. Under load, this could create a backlog. There's no back-pressure mechanism. Mitigation : Monitor outbox table depth; consider reducing poll interval or switching to PostgreSQL LISTEN/NOTIFY.","title":"3. Outbox Polling Latency"},{"location":"status/#4-gemini-api-rate-limits","text":"Multiple pipeline nodes call Gemini concurrently (semaphore=4 per node). A burst of protocol uploads could hit API rate limits. Mitigation : The circuit breaker ( pybreaker ) handles transient failures. Consider adding global rate limiting.","title":"4. Gemini API Rate Limits"},{"location":"status/#5-expression-tree-vs-flat-sql-mismatch","text":"The CIRCE and FHIR exporters walk the expression tree correctly, but the SQL exporter uses a flat model. Users may get inconsistent results across export formats. Mitigation : Document the limitation clearly (done). Future: implement tree-aware SQL generation using recursive CTE grouping.","title":"5. Expression Tree vs Flat SQL Mismatch"}]}